{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üí¨ Lecture 16: Efficient LLMs - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/16_efficient_llms/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- KV cache and its memory implications\n", "- Speculative decoding for faster inference\n", "- Continuous batching\n", "- LLM quantization (GPTQ, AWQ)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import time\n", "\n", "print('Ready for Efficient LLMs!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: The LLM Inference Challenge"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def llm_inference_analysis():\n", "    \"\"\"\n", "    Analyze LLM inference characteristics.\n", "    \"\"\"\n", "    print('üìä LLM INFERENCE CHARACTERISTICS')\n", "    print('=' * 60)\n", "    \n", "    print('\\nüîπ Prefill Phase (Process prompt):')\n", "    print('   - Process all prompt tokens in parallel')\n", "    print('   - Compute-bound (matrix multiplications)')\n", "    print('   - Good GPU utilization')\n", "    \n", "    print('\\nüîπ Decode Phase (Generate tokens):')\n", "    print('   - Generate ONE token at a time')\n", "    print('   - Memory-bound (load weights for single token)')\n", "    print('   - Poor GPU utilization (~1-5%)')\n", "    \n", "    # Calculate throughput\n", "    models = {\n", "        'GPT-2': {'params_b': 1.5, 'prefill_tok_s': 5000, 'decode_tok_s': 50},\n", "        'LLaMA-7B': {'params_b': 7, 'prefill_tok_s': 2000, 'decode_tok_s': 30},\n", "        'LLaMA-70B': {'params_b': 70, 'prefill_tok_s': 500, 'decode_tok_s': 10},\n", "    }\n", "    \n", "    print('\\nüìä INFERENCE SPEED (A100, FP16)')\n", "    print(f'{\"Model\":<15} {\"Params\":<10} {\"Prefill\":<15} {\"Decode\":<15} {\"Slowdown\":<10}')\n", "    print('-' * 65)\n", "    for name, info in models.items():\n", "        slowdown = info['prefill_tok_s'] / info['decode_tok_s']\n", "        print(f'{name:<15} {info[\"params_b\"]:<10.1f}B {info[\"prefill_tok_s\"]:>12} tok/s {info[\"decode_tok_s\"]:>10} tok/s {slowdown:>8.0f}x')\n", "    \n", "    print('\\n‚ö†Ô∏è Decode is 50-100x slower than prefill!')\n", "\n", "llm_inference_analysis()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: KV Cache"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kv_cache_memory(model_params_b, n_layers, n_heads, d_head, seq_len, dtype_bytes=2):\n", "    \"\"\"\n", "    Calculate KV cache memory.\n", "    \n", "    KV cache stores key and value tensors for all past tokens\n", "    to avoid recomputation during autoregressive generation.\n", "    \n", "    Memory = 2 (K+V) √ó n_layers √ó seq_len √ó n_heads √ó d_head √ó dtype_bytes\n", "    \"\"\"\n", "    cache_mem = 2 * n_layers * seq_len * n_heads * d_head * dtype_bytes\n", "    return cache_mem / 1e9  # GB\n", "\n", "# LLaMA architecture\n", "llama_configs = {\n", "    'LLaMA-7B': {'layers': 32, 'heads': 32, 'd_head': 128},\n", "    'LLaMA-13B': {'layers': 40, 'heads': 40, 'd_head': 128},\n", "    'LLaMA-70B': {'layers': 80, 'heads': 64, 'd_head': 128},\n", "}\n", "\n", "print('üìä KV CACHE MEMORY ANALYSIS')\n", "print('=' * 70)\n", "\n", "seq_lengths = [1024, 4096, 16384, 32768, 131072]\n", "\n", "print(f'{\"Model\":<12}', end='')\n", "for seq in seq_lengths:\n", "    print(f'{seq:>10}', end='')\n", "print('  (tokens)')\n", "print('-' * 70)\n", "\n", "for name, config in llama_configs.items():\n", "    print(f'{name:<12}', end='')\n", "    for seq in seq_lengths:\n", "        mem = kv_cache_memory(0, config['layers'], config['heads'], \n", "                              config['d_head'], seq)\n", "        print(f'{mem:>9.1f}GB', end='')\n", "    print()\n", "\n", "print('\\n‚ö†Ô∏è KV cache can exceed model size at long contexts!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize KV cache scaling\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Memory vs sequence length\n", "seq_range = np.linspace(1024, 131072, 100)\n", "for name, config in llama_configs.items():\n", "    mems = [kv_cache_memory(0, config['layers'], config['heads'], \n", "                           config['d_head'], int(s)) for s in seq_range]\n", "    axes[0].plot(seq_range/1000, mems, label=name, linewidth=2)\n", "\n", "axes[0].axhline(y=80, color='red', linestyle='--', label='A100 80GB')\n", "axes[0].set_xlabel('Sequence Length (K tokens)')\n", "axes[0].set_ylabel('KV Cache Memory (GB)')\n", "axes[0].set_title('KV Cache Memory vs Context Length')\n", "axes[0].legend()\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "# Batch size vs memory\n", "batch_sizes = [1, 2, 4, 8, 16, 32]\n", "seq_len = 4096\n", "\n", "for name, config in llama_configs.items():\n", "    mems = [kv_cache_memory(0, config['layers'], config['heads'], \n", "                           config['d_head'], seq_len) * bs for bs in batch_sizes]\n", "    axes[1].plot(batch_sizes, mems, 'o-', label=name, linewidth=2)\n", "\n", "axes[1].axhline(y=80, color='red', linestyle='--', label='A100 80GB')\n", "axes[1].set_xlabel('Batch Size')\n", "axes[1].set_ylabel('KV Cache Memory (GB)')\n", "axes[1].set_title('KV Cache Memory vs Batch Size (4K context)')\n", "axes[1].legend()\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Speculative Decoding"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def speculative_decoding_demo():\n", "    \"\"\"\n", "    Demonstrate speculative decoding concept.\n", "    \n", "    Key idea:\n", "    1. Use small draft model to generate K tokens quickly\n", "    2. Verify all K tokens with large model in ONE forward pass\n", "    3. Accept verified tokens, reject and regenerate if needed\n", "    \"\"\"\n", "    print('üìä SPECULATIVE DECODING')\n", "    print('=' * 60)\n", "    \n", "    print('\\nüîπ Standard Decoding (7B model):')\n", "    print('   Token 1: [Forward 7B] ‚Üí \"The\"')\n", "    print('   Token 2: [Forward 7B] ‚Üí \"quick\"')\n", "    print('   Token 3: [Forward 7B] ‚Üí \"brown\"')\n", "    print('   Token 4: [Forward 7B] ‚Üí \"fox\"')\n", "    print('   Total: 4 forward passes')\n", "    \n", "    print('\\nüîπ Speculative Decoding:')\n", "    print('   Draft (68M): [Quick] ‚Üí \"The quick brown fox\" (4 tokens)')\n", "    print('   Target (7B): [Verify] ‚Üí Accept \"The quick brown\" (3 accepted)')\n", "    print('   Total: 1 draft + 1 target forward pass = 3 tokens!')\n", "    \n", "    # Simulate speedup\n", "    target_time = 100  # ms per token\n", "    draft_time = 5     # ms per token\n", "    K = 4              # speculation depth\n", "    accept_rate = 0.8  # acceptance rate\n", "    \n", "    # Standard: K tokens √ó target_time\n", "    standard_time = K * target_time\n", "    \n", "    # Speculative: K √ó draft_time + 1 √ó target_time\n", "    # Expected accepted = K √ó accept_rate\n", "    spec_time = K * draft_time + target_time\n", "    expected_tokens = K * accept_rate + 1  # Plus at least 1 from target\n", "    \n", "    print(f'\\nüìä SPEEDUP ANALYSIS')\n", "    print(f'Standard: {K} tokens in {standard_time}ms ({standard_time/K:.0f}ms/token)')\n", "    print(f'Speculative: {expected_tokens:.1f} tokens in {spec_time}ms ({spec_time/expected_tokens:.0f}ms/token)')\n", "    print(f'Speedup: {(standard_time/K) / (spec_time/expected_tokens):.1f}x')\n", "\n", "speculative_decoding_demo()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simulate speculative decoding\n", "def simulate_speculative(n_tokens, accept_rate, K=4, target_time=100, draft_time=5):\n", "    \"\"\"\n", "    Simulate speculative decoding throughput.\n", "    \"\"\"\n", "    total_time = 0\n", "    generated = 0\n", "    \n", "    while generated < n_tokens:\n", "        # Draft K tokens\n", "        total_time += K * draft_time\n", "        \n", "        # Verify with target\n", "        total_time += target_time\n", "        \n", "        # Accept based on rate\n", "        n_accepted = int(K * accept_rate) + 1  # Plus bonus token from target\n", "        generated += min(n_accepted, n_tokens - generated)\n", "    \n", "    return total_time, generated\n", "\n", "# Compare methods\n", "n_tokens = 100\n", "accept_rates = [0.5, 0.6, 0.7, 0.8, 0.9]\n", "\n", "standard_time = n_tokens * 100  # 100ms per token\n", "\n", "print('üìä SPECULATIVE DECODING SPEEDUP')\n", "print('=' * 50)\n", "print(f'{\"Accept Rate\":<15} {\"Time (ms)\":<15} {\"Speedup\":<15}')\n", "print('-' * 45)\n", "\n", "for rate in accept_rates:\n", "    spec_time, _ = simulate_speculative(n_tokens, rate)\n", "    speedup = standard_time / spec_time\n", "    print(f'{rate:<15.0%} {spec_time:<15.0f} {speedup:<15.1f}x')\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "rates = np.linspace(0.3, 0.95, 20)\n", "speedups = [standard_time / simulate_speculative(n_tokens, r)[0] for r in rates]\n", "\n", "ax.plot(rates * 100, speedups, 'o-', color='#3b82f6', linewidth=2)\n", "ax.axhline(y=1, color='red', linestyle='--', label='Standard decoding')\n", "ax.set_xlabel('Acceptance Rate (%)')\n", "ax.set_ylabel('Speedup')\n", "ax.set_title('üìä Speculative Decoding Speedup vs Acceptance Rate')\n", "ax.grid(True, alpha=0.3)\n", "ax.legend()\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: LLM Quantization (GPTQ, AWQ)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def llm_quantization_comparison():\n", "    \"\"\"\n", "    Compare LLM quantization methods.\n", "    \"\"\"\n", "    methods = {\n", "        'FP16': {'bits': 16, 'perplexity_delta': 0, 'speed': 1.0},\n", "        'INT8 (LLM.int8)': {'bits': 8, 'perplexity_delta': 0.1, 'speed': 1.2},\n", "        'GPTQ 4-bit': {'bits': 4, 'perplexity_delta': 0.3, 'speed': 2.5},\n", "        'AWQ 4-bit': {'bits': 4, 'perplexity_delta': 0.2, 'speed': 2.8},\n", "        'GGML Q4_K_M': {'bits': 4.5, 'perplexity_delta': 0.25, 'speed': 3.0},\n", "    }\n", "    \n", "    print('üìä LLM QUANTIZATION METHODS')\n", "    print('=' * 70)\n", "    print(f'{\"Method\":<20} {\"Bits\":<10} {\"PPL Delta\":<15} {\"Speed vs FP16\":<15}')\n", "    print('-' * 70)\n", "    \n", "    for name, info in methods.items():\n", "        print(f'{name:<20} {info[\"bits\"]:<10.1f} {info[\"perplexity_delta\"]:>+12.2f}   {info[\"speed\"]:>12.1f}x')\n", "    \n", "    # Memory comparison for LLaMA-70B\n", "    print('\\nüìä LLaMA-70B MEMORY')\n", "    print('-' * 40)\n", "    base_mem = 70 * 2  # 70B √ó 2 bytes (FP16)\n", "    \n", "    for name, info in methods.items():\n", "        mem = 70 * info['bits'] / 8\n", "        print(f'{name:<20}: {mem:.0f} GB')\n", "\n", "llm_quantization_comparison()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize quantization trade-offs\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Memory comparison\n", "methods = ['FP16', 'INT8', '4-bit GPTQ', '4-bit AWQ', '3-bit']\n", "llama_70b_mem = [140, 70, 35, 35, 26.25]  # GB\n", "llama_7b_mem = [14, 7, 3.5, 3.5, 2.625]   # GB\n", "\n", "x = np.arange(len(methods))\n", "width = 0.35\n", "\n", "bars1 = axes[0].bar(x - width/2, llama_70b_mem, width, label='LLaMA-70B', color='#ef4444')\n", "bars2 = axes[0].bar(x + width/2, llama_7b_mem, width, label='LLaMA-7B', color='#3b82f6')\n", "\n", "axes[0].axhline(y=24, color='green', linestyle='--', label='RTX 4090 (24GB)')\n", "axes[0].axhline(y=80, color='orange', linestyle='--', label='A100 (80GB)')\n", "\n", "axes[0].set_ylabel('Memory (GB)')\n", "axes[0].set_title('Model Memory by Precision')\n", "axes[0].set_xticks(x)\n", "axes[0].set_xticklabels(methods, rotation=15)\n", "axes[0].legend()\n", "axes[0].grid(True, alpha=0.3, axis='y')\n", "\n", "# Quality vs Size trade-off\n", "bits = [16, 8, 4, 3, 2]\n", "ppl_delta = [0, 0.1, 0.3, 1.0, 3.0]  # Perplexity increase\n", "\n", "axes[1].plot(bits, ppl_delta, 'o-', color='#ef4444', linewidth=2, markersize=10)\n", "axes[1].set_xlabel('Bits per Weight')\n", "axes[1].set_ylabel('Perplexity Increase')\n", "axes[1].set_title('Quality Degradation vs Quantization')\n", "axes[1].grid(True, alpha=0.3)\n", "axes[1].invert_xaxis()\n", "\n", "# Annotate\n", "for b, p in zip(bits, ppl_delta):\n", "    quality = 'Good' if p < 0.5 else ('OK' if p < 1.5 else 'Poor')\n", "    axes[1].annotate(quality, (b, p), xytext=(10, 5), textcoords='offset points')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('üéØ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. LLM decode is memory-bound (50-100x slower than prefill)')\n", "print('\\n2. KV cache memory grows linearly with sequence length')\n", "print('\\n3. Speculative decoding: 2-3x faster with draft model')\n", "print('\\n4. 4-bit quantization: 4x less memory, minimal quality loss')\n", "print('\\n5. Combine techniques: quant + spec decode + paged attention')\n", "print('\\n6. vLLM, TensorRT-LLM implement all optimizations')\n", "print('\\n' + '=' * 60)\n", "print('\\nüìö Next: Efficient Diffusion Models!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
