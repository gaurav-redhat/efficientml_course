{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üöÄ Lecture 1: Introduction to Efficient ML - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/01_introduction/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Why ML efficiency matters (cost, latency, energy)\n", "- Model size comparison across different architectures\n", "- Memory and compute requirements\n", "- The efficiency gap between research and deployment"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "print('üñ•Ô∏è PyTorch version:', torch.__version__)\n", "print('GPU available:', torch.cuda.is_available())"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: The Model Size Explosion\n", "\n", "ML models have grown **1000x** in just a few years!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Model sizes over the years\n", "models = {\n", "    'AlexNet (2012)': 61,\n", "    'VGG-16 (2014)': 138,\n", "    'ResNet-152 (2015)': 60,\n", "    'BERT-base (2018)': 110,\n", "    'GPT-2 (2019)': 1500,\n", "    'GPT-3 (2020)': 175000,\n", "    'PaLM (2022)': 540000,\n", "    'GPT-4 (2023)': 1800000,\n", "}\n", "\n", "# Visualization\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "names = list(models.keys())\n", "sizes = list(models.values())\n", "colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(models)))\n", "\n", "bars = ax.barh(names, sizes, color=colors)\n", "ax.set_xlabel('Parameters (Millions)', fontsize=12)\n", "ax.set_title('üöÄ Model Size Growth Over Time', fontsize=14)\n", "ax.set_xscale('log')\n", "\n", "for bar, size in zip(bars, sizes):\n", "    label = f'{size/1000:.0f}B' if size >= 1000 else f'{size}M'\n", "    ax.text(bar.get_width() * 1.1, bar.get_y() + bar.get_height()/2, \n", "            label, va='center', fontsize=10)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print(f'\\nüìà GPT-4 is {models[\"GPT-4 (2023)\"]/models[\"AlexNet (2012)\"]:.0f}x larger than AlexNet!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Memory Requirements\n", "\n", "Let's calculate how much GPU memory different models need."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calculate_memory(params_millions, dtype='fp32', training=False):\n", "    \"\"\"\n", "    Calculate memory requirements.\n", "    \n", "    Training memory = Model + Gradients + Optimizer States + Activations\n", "    - FP32: 4 bytes per param\n", "    - FP16: 2 bytes per param\n", "    - Adam optimizer: 8 bytes per param (momentum + variance in FP32)\n", "    \"\"\"\n", "    bytes_per_param = 4 if dtype == 'fp32' else 2\n", "    model_memory = params_millions * 1e6 * bytes_per_param\n", "    \n", "    if training:\n", "        # Training: Model + Gradients + Optimizer (Adam)\n", "        gradient_memory = model_memory\n", "        optimizer_memory = params_millions * 1e6 * 8  # Adam states in FP32\n", "        # Activations depend on batch size and sequence length (estimate)\n", "        activation_memory = model_memory * 2  # Rough estimate\n", "        total = model_memory + gradient_memory + optimizer_memory + activation_memory\n", "    else:\n", "        total = model_memory\n", "    \n", "    return total / 1e9  # Return in GB\n", "\n", "print('üìä Memory Requirements Analysis')\n", "print('=' * 60)\n", "print(f'{\"Model\":<20} {\"Params\":<12} {\"Inference\":<12} {\"Training\":<12}')\n", "print('-' * 60)\n", "\n", "for name, params in models.items():\n", "    short_name = name.split(' ')[0]\n", "    inf_mem = calculate_memory(params, 'fp16', training=False)\n", "    train_mem = calculate_memory(params, 'fp16', training=True)\n", "    print(f'{short_name:<20} {params:<12,} {inf_mem:<12.1f} GB {train_mem:<12.1f} GB')\n", "\n", "print('\\nüí° Key Insight: Training needs 10-20x more memory than inference!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: The Cost of Training Large Models"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training costs\n", "training_costs = {\n", "    'BERT-base': 0.5,       # ~$500\n", "    'GPT-2': 50,            # ~$50K\n", "    'GPT-3': 4600,          # ~$4.6M\n", "    'PaLM': 8000,           # ~$8M (estimated)\n", "    'GPT-4': 100000,        # ~$100M (estimated)\n", "}\n", "\n", "# CO2 emissions (tons)\n", "co2_emissions = {\n", "    'BERT-base': 0.6,\n", "    'GPT-2': 5,\n", "    'GPT-3': 500,\n", "    'PaLM': 600,\n", "    'GPT-4': 5000,  # Estimated\n", "}\n", "\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Cost chart\n", "names = list(training_costs.keys())\n", "costs = list(training_costs.values())\n", "axes[0].bar(names, costs, color='#22c55e')\n", "axes[0].set_ylabel('Training Cost ($K)', fontsize=12)\n", "axes[0].set_title('üí∞ Training Costs', fontsize=14)\n", "axes[0].set_yscale('log')\n", "for i, (n, c) in enumerate(zip(names, costs)):\n", "    label = f'${c/1000:.0f}M' if c >= 1000 else f'${c:.0f}K'\n", "    axes[0].text(i, c * 1.2, label, ha='center')\n", "\n", "# CO2 chart\n", "co2 = list(co2_emissions.values())\n", "axes[1].bar(names, co2, color='#ef4444')\n", "axes[1].set_ylabel('CO2 Emissions (tons)', fontsize=12)\n", "axes[1].set_title('üåç Environmental Impact', fontsize=14)\n", "axes[1].set_yscale('log')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüåç For reference:')\n", "print('   - Average car: 4.6 tons CO2/year')\n", "print('   - GPT-3 training: 500 tons CO2 = 100+ car-years!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Latency Requirements in Real Applications"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Application latency requirements\n", "apps = {\n", "    'Voice Assistant': {'required': 100, 'typical_llm': 500},\n", "    'Auto-complete': {'required': 50, 'typical_llm': 200},\n", "    'Chatbot': {'required': 500, 'typical_llm': 1000},\n", "    'Image Generation': {'required': 5000, 'typical_llm': 30000},\n", "    'Self-driving Car': {'required': 10, 'typical_llm': 100},\n", "}\n", "\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "\n", "x = np.arange(len(apps))\n", "width = 0.35\n", "\n", "required = [v['required'] for v in apps.values()]\n", "typical = [v['typical_llm'] for v in apps.values()]\n", "\n", "bars1 = ax.bar(x - width/2, required, width, label='Required Latency', color='#22c55e')\n", "bars2 = ax.bar(x + width/2, typical, width, label='Typical LLM Latency', color='#ef4444')\n", "\n", "ax.set_ylabel('Latency (ms)', fontsize=12)\n", "ax.set_title('‚è±Ô∏è Latency Gap: Requirements vs Reality', fontsize=14)\n", "ax.set_xticks(x)\n", "ax.set_xticklabels(apps.keys(), rotation=15)\n", "ax.legend()\n", "ax.set_yscale('log')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\n‚ö†Ô∏è The Gap: Most LLMs are 2-10x slower than required!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Efficiency Techniques Overview"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Summary of efficiency techniques\n", "techniques = [\n", "    ('Pruning', '10x smaller', 'Remove unimportant weights'),\n", "    ('Quantization', '4x smaller', 'FP32 ‚Üí INT8/INT4'),\n", "    ('Knowledge Distillation', '3-10x smaller', 'Train small model from large'),\n", "    ('Neural Architecture Search', '2-5x efficient', 'Auto-design efficient nets'),\n", "    ('FlashAttention', '2-4x faster', 'Memory-efficient attention'),\n", "    ('Speculative Decoding', '2-3x faster', 'Use draft model to speed up'),\n", "]\n", "\n", "print('üõ†Ô∏è EFFICIENCY TECHNIQUES COVERED IN THIS COURSE')\n", "print('=' * 70)\n", "for tech, benefit, description in techniques:\n", "    print(f'\\nüìå {tech}')\n", "    print(f'   Benefit: {benefit}')\n", "    print(f'   How: {description}')\n", "\n", "print('\\n' + '=' * 70)\n", "print('\\nüéØ KEY TAKEAWAYS')\n", "print('-' * 70)\n", "print('1. Model sizes have grown 1000x in 5 years')\n", "print('2. Training costs millions of dollars and tons of CO2')\n", "print('3. Real applications need 10x lower latency than current models')\n", "print('4. Efficiency techniques can reduce size/cost/latency by 10x+')\n", "print('5. This course teaches you how to make ML practical!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Final visualization: The efficiency opportunity\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "categories = ['Model Size', 'Memory', 'Latency', 'Cost', 'Energy']\n", "before = [100, 100, 100, 100, 100]\n", "after = [10, 15, 25, 10, 15]\n", "\n", "x = np.arange(len(categories))\n", "width = 0.35\n", "\n", "bars1 = ax.bar(x - width/2, before, width, label='Before Optimization', color='#ef4444')\n", "bars2 = ax.bar(x + width/2, after, width, label='After Optimization', color='#22c55e')\n", "\n", "ax.set_ylabel('Relative Value (%)', fontsize=12)\n", "ax.set_title('üéØ The Efficiency Opportunity', fontsize=14)\n", "ax.set_xticks(x)\n", "ax.set_xticklabels(categories)\n", "ax.legend()\n", "ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5)\n", "\n", "for bar in bars2:\n", "    height = bar.get_height()\n", "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n", "            f'{100-height:.0f}% ‚Üì', ha='center', va='bottom', fontsize=10, color='green')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüöÄ Ready to make ML efficient? Let\\'s go!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
