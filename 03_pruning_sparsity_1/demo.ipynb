{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî¨ Lecture 3: Pruning & Sparsity - Complete Demo\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/03_pruning_sparsity_1/demo.ipynb)\n",
        "\n",
        "## What You'll Learn\n",
        "- Why neural networks are over-parameterized\n",
        "- How magnitude pruning works step-by-step\n",
        "- Implementing pruning from scratch\n",
        "- Visualizing sparse weight matrices\n",
        "- Measuring accuracy before/after pruning\n",
        "- Iterative pruning for better results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "!pip install torch torchvision matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è Using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Create and Train a Model\n",
        "\n",
        "We'll use a simple MLP on MNIST to demonstrate pruning concepts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a simple MLP for MNIST\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleMLP().to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"üìä Model Architecture:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n",
        "print(f\"\\n   Layer sizes:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        print(f\"   {name}: {param.shape} = {param.numel():,} params\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000)\n",
        "\n",
        "print(f\"üì¶ Dataset loaded:\")\n",
        "print(f\"   Training samples: {len(train_dataset):,}\")\n",
        "print(f\"   Test samples: {len(test_dataset):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += (pred == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "# Train the model\n",
        "print(\"üèãÔ∏è Training the model...\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    loss = train_epoch(model, train_loader, optimizer)\n",
        "    acc = evaluate(model, test_loader)\n",
        "    print(f\"   Epoch {epoch}: Loss={loss:.4f}, Accuracy={acc:.2f}%\")\n",
        "\n",
        "original_accuracy = evaluate(model, test_loader)\n",
        "print(f\"\\n‚úÖ Original Model Accuracy: {original_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Analyzing Weight Distribution\n",
        "\n",
        "**Key Insight**: Most weights in trained neural networks are very small!\n",
        "\n",
        "This is why pruning works - we can remove small weights without hurting accuracy much.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize weight distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "layers = [('fc1', model.fc1), ('fc2', model.fc2), \n",
        "          ('fc3', model.fc3), ('fc4', model.fc4)]\n",
        "\n",
        "for ax, (name, layer) in zip(axes.flat, layers):\n",
        "    weights = layer.weight.data.cpu().numpy().flatten()\n",
        "    \n",
        "    ax.hist(weights, bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "    ax.set_title(f'{name}: {len(weights):,} weights', fontsize=12)\n",
        "    ax.set_xlabel('Weight Value')\n",
        "    ax.set_ylabel('Count')\n",
        "    \n",
        "    # Calculate % of small weights\n",
        "    small_weights = np.abs(weights) < 0.1\n",
        "    ax.text(0.95, 0.95, f'{small_weights.sum()/len(weights)*100:.1f}% are tiny\\n(|w| < 0.1)', \n",
        "            transform=ax.transAxes, ha='right', va='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
        "\n",
        "plt.suptitle('üìä Weight Distribution by Layer\\n(Notice: Most weights are clustered near zero!)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Implementing Magnitude Pruning\n",
        "\n",
        "**Algorithm**: Remove weights with the smallest absolute values\n",
        "\n",
        "```\n",
        "1. Collect all weights from the model\n",
        "2. Sort by absolute value\n",
        "3. Find threshold at desired sparsity (e.g., 90th percentile)\n",
        "4. Set weights below threshold to zero\n",
        "5. Fine-tune to recover accuracy\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement magnitude pruning from scratch\n",
        "def magnitude_prune(model, sparsity):\n",
        "    \"\"\"\n",
        "    Prune weights by magnitude (global pruning).\n",
        "    \n",
        "    Args:\n",
        "        model: Neural network\n",
        "        sparsity: Fraction to remove (0.9 = remove 90%)\n",
        "    \n",
        "    Returns:\n",
        "        masks: Dictionary of binary masks\n",
        "    \"\"\"\n",
        "    # Step 1: Collect all weights\n",
        "    all_weights = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            all_weights.append(param.data.abs().flatten())\n",
        "    all_weights = torch.cat(all_weights)\n",
        "    \n",
        "    # Step 2: Find threshold\n",
        "    threshold = torch.quantile(all_weights, sparsity)\n",
        "    print(f\"   Threshold for {sparsity*100:.0f}% sparsity: {threshold:.6f}\")\n",
        "    \n",
        "    # Step 3: Create masks and apply\n",
        "    masks = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            mask = (param.data.abs() > threshold).float()\n",
        "            masks[name] = mask\n",
        "            param.data *= mask  # Zero out pruned weights\n",
        "    \n",
        "    return masks\n",
        "\n",
        "def count_sparsity(model):\n",
        "    \"\"\"Calculate model sparsity percentage.\"\"\"\n",
        "    total = zeros = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            total += param.numel()\n",
        "            zeros += (param == 0).sum().item()\n",
        "    return zeros / total * 100\n",
        "\n",
        "# Create a copy for pruning\n",
        "pruned_model = copy.deepcopy(model)\n",
        "\n",
        "print(\"üî™ Applying 90% magnitude pruning...\")\n",
        "masks = magnitude_prune(pruned_model, sparsity=0.9)\n",
        "\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"   Sparsity: {count_sparsity(pruned_model):.1f}%\")\n",
        "print(f\"   Original accuracy: {original_accuracy:.2f}%\")\n",
        "pruned_acc = evaluate(pruned_model, test_loader)\n",
        "print(f\"   Pruned accuracy (before fine-tuning): {pruned_acc:.2f}%\")\n",
        "print(f\"   Accuracy drop: {original_accuracy - pruned_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Fine-tuning to Recover Accuracy\n",
        "\n",
        "After pruning, we fine-tune while **keeping pruned weights at zero**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune while keeping pruned weights at zero\n",
        "def fine_tune_pruned(model, train_loader, test_loader, masks, epochs=3):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Zero out gradients for pruned weights\n",
        "            for name, param in model.named_parameters():\n",
        "                if name in masks:\n",
        "                    param.grad.data *= masks[name]\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            # Re-apply masks (keep zeros at zero)\n",
        "            for name, param in model.named_parameters():\n",
        "                if name in masks:\n",
        "                    param.data *= masks[name]\n",
        "        \n",
        "        acc = evaluate(model, test_loader)\n",
        "        print(f\"   Fine-tune epoch {epoch+1}: Accuracy = {acc:.2f}%\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"üîß Fine-tuning pruned model...\")\n",
        "pruned_model = fine_tune_pruned(pruned_model, train_loader, test_loader, masks, epochs=3)\n",
        "\n",
        "final_accuracy = evaluate(pruned_model, test_loader)\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(f\"üìä FINAL RESULTS\")\n",
        "print(f\"=\"*50)\n",
        "print(f\"   Original accuracy:     {original_accuracy:.2f}%\")\n",
        "print(f\"   Pruned + fine-tuned:   {final_accuracy:.2f}%\")\n",
        "print(f\"   Accuracy drop:         {original_accuracy - final_accuracy:.2f}%\")\n",
        "print(f\"   Weights removed:       90%\")\n",
        "print(f\"   Compression ratio:     10x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Visualizing Sparse Weight Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sparse weight matrices\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "orig_layers = [model.fc1, model.fc2, model.fc3, model.fc4]\n",
        "prune_layers = [pruned_model.fc1, pruned_model.fc2, pruned_model.fc3, pruned_model.fc4]\n",
        "names = ['fc1', 'fc2', 'fc3', 'fc4']\n",
        "\n",
        "for i, (orig, pruned, name) in enumerate(zip(orig_layers, prune_layers, names)):\n",
        "    # Original (top row)\n",
        "    w = orig.weight.data.cpu().numpy()[:64, :64]\n",
        "    axes[0, i].imshow(w != 0, cmap='Blues', aspect='auto')\n",
        "    axes[0, i].set_title(f'Original {name}')\n",
        "    axes[0, i].axis('off')\n",
        "    \n",
        "    # Pruned (bottom row)\n",
        "    w = pruned.weight.data.cpu().numpy()[:64, :64]\n",
        "    sparsity = (w == 0).sum() / w.size * 100\n",
        "    axes[1, i].imshow(w != 0, cmap='Reds', aspect='auto')\n",
        "    axes[1, i].set_title(f'Pruned {name}\\n({sparsity:.0f}% sparse)')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.suptitle('üîç Weight Matrices: Colored = Non-zero, White = Pruned', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéØ KEY TAKEAWAYS\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚Ä¢ Neural networks are over-parameterized\")\n",
        "print(\"‚Ä¢ 90% of weights can be removed with minimal accuracy loss\")\n",
        "print(\"‚Ä¢ Magnitude pruning: remove smallest |weights|\")\n",
        "print(\"‚Ä¢ Fine-tuning is essential to recover accuracy\")\n",
        "print(\"‚Ä¢ Sparse models can be 10x smaller!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
