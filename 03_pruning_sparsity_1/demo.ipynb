{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# âœ‚ï¸ Lecture 3: Pruning & Sparsity (Part 1) - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/03_pruning_sparsity_1/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Weight pruning fundamentals\n", "- Magnitude-based pruning implementation\n", "- Structured vs unstructured pruning\n", "- Sparsity patterns and their hardware implications"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch torchvision matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.utils.prune as prune\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "print('Ready for pruning!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Why Pruning Works - Weight Distribution Analysis"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a simple trained model\n", "class SimpleNet(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = nn.Linear(784, 512)\n", "        self.fc2 = nn.Linear(512, 256)\n", "        self.fc3 = nn.Linear(256, 10)\n", "        self.relu = nn.ReLU()\n", "    \n", "    def forward(self, x):\n", "        x = self.relu(self.fc1(x))\n", "        x = self.relu(self.fc2(x))\n", "        return self.fc3(x)\n", "\n", "model = SimpleNet()\n", "\n", "# Simulate trained weights (trained models have near-zero weights)\n", "# Real trained networks have many small weights!\n", "with torch.no_grad():\n", "    for param in model.parameters():\n", "        # Many weights become small after training\n", "        mask = torch.rand_like(param) > 0.3\n", "        param.data = param.data * mask.float() * 0.1 + param.data * (~mask).float()\n", "\n", "# Visualize weight distribution\n", "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n", "\n", "for idx, (name, param) in enumerate(model.named_parameters()):\n", "    if 'weight' in name:\n", "        weights = param.data.flatten().numpy()\n", "        axes[idx].hist(weights, bins=100, color='#3b82f6', alpha=0.7, edgecolor='black')\n", "        axes[idx].axvline(x=0, color='red', linestyle='--', linewidth=2)\n", "        axes[idx].set_title(f'{name}\\nMean: {np.mean(weights):.4f}, Std: {np.std(weights):.4f}')\n", "        axes[idx].set_xlabel('Weight Value')\n", "        axes[idx].set_ylabel('Count')\n", "        \n", "        # Calculate near-zero weights\n", "        threshold = 0.05\n", "        near_zero = np.sum(np.abs(weights) < threshold) / len(weights) * 100\n", "        axes[idx].text(0.95, 0.95, f'{near_zero:.1f}% near-zero', \n", "                       transform=axes[idx].transAxes, ha='right', va='top')\n", "\n", "plt.suptitle('ðŸ“Š Weight Distribution in Neural Networks', fontsize=14)\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ Key Insight: Many weights are near-zero and can be removed!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Implementing Magnitude Pruning from Scratch"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def magnitude_prune(tensor, sparsity):\n", "    \"\"\"\n", "    Prune weights with smallest magnitude.\n", "    \n", "    Args:\n", "        tensor: Weight tensor to prune\n", "        sparsity: Fraction of weights to remove (0.9 = remove 90%)\n", "    \n", "    Returns:\n", "        Pruned tensor and binary mask\n", "    \"\"\"\n", "    # Get absolute values\n", "    abs_weights = torch.abs(tensor)\n", "    \n", "    # Find threshold\n", "    num_weights = tensor.numel()\n", "    num_to_prune = int(num_weights * sparsity)\n", "    \n", "    # Get threshold value\n", "    threshold = torch.kthvalue(abs_weights.flatten(), num_to_prune).values\n", "    \n", "    # Create mask\n", "    mask = abs_weights > threshold\n", "    \n", "    # Apply mask\n", "    pruned_tensor = tensor * mask.float()\n", "    \n", "    return pruned_tensor, mask\n", "\n", "# Test our implementation\n", "original_weights = model.fc1.weight.data.clone()\n", "pruned_weights, mask = magnitude_prune(original_weights, sparsity=0.9)\n", "\n", "print('ðŸ“Š MAGNITUDE PRUNING RESULTS')\n", "print('=' * 50)\n", "print(f'Original weights: {original_weights.numel():,}')\n", "print(f'Non-zero after pruning: {mask.sum().item():,}')\n", "print(f'Actual sparsity: {100 - mask.sum().item()/mask.numel()*100:.1f}%')\n", "\n", "# Visualize\n", "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n", "\n", "# Original weights heatmap\n", "im0 = axes[0].imshow(original_weights[:50, :50].numpy(), cmap='RdBu', aspect='auto')\n", "axes[0].set_title('Original Weights')\n", "plt.colorbar(im0, ax=axes[0])\n", "\n", "# Mask\n", "im1 = axes[1].imshow(mask[:50, :50].numpy(), cmap='binary', aspect='auto')\n", "axes[1].set_title('Pruning Mask (white=keep)')\n", "\n", "# Pruned weights\n", "im2 = axes[2].imshow(pruned_weights[:50, :50].numpy(), cmap='RdBu', aspect='auto')\n", "axes[2].set_title('Pruned Weights (90% sparse)')\n", "plt.colorbar(im2, ax=axes[2])\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Using PyTorch's Built-in Pruning"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create fresh model\n", "model = SimpleNet()\n", "\n", "print('ðŸ”§ PYTORCH PRUNING API')\n", "print('=' * 50)\n", "\n", "# Before pruning\n", "print('\\nBefore pruning:')\n", "print(f'  fc1.weight shape: {model.fc1.weight.shape}')\n", "print(f'  Parameters: {list(model.fc1.named_parameters())}')\n", "\n", "# Apply L1 unstructured pruning\n", "prune.l1_unstructured(model.fc1, name='weight', amount=0.9)\n", "\n", "print('\\nAfter pruning:')\n", "print(f'  Parameters: {list(model.fc1.named_parameters())}')\n", "print(f'  Buffers: {list(model.fc1.named_buffers())}')\n", "\n", "# Check sparsity\n", "sparsity = 100 * float(torch.sum(model.fc1.weight == 0)) / float(model.fc1.weight.nelement())\n", "print(f'\\nðŸ“Š Sparsity achieved: {sparsity:.1f}%')\n", "\n", "# Make pruning permanent\n", "prune.remove(model.fc1, 'weight')\n", "print('\\nAfter removing pruning reparametrization:')\n", "print(f'  Parameters: {list(model.fc1.named_parameters())}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Structured vs Unstructured Pruning"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize_pruning_types(weight_matrix):\n", "    \"\"\"\n", "    Compare different pruning patterns.\n", "    \"\"\"\n", "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n", "    \n", "    # Original\n", "    axes[0, 0].imshow(weight_matrix, cmap='RdBu', aspect='auto')\n", "    axes[0, 0].set_title('Original Weights', fontsize=12)\n", "    \n", "    # Unstructured (random)\n", "    unstructured_mask = torch.rand_like(weight_matrix) > 0.7\n", "    axes[0, 1].imshow(weight_matrix * unstructured_mask.float(), cmap='RdBu', aspect='auto')\n", "    axes[0, 1].set_title('Unstructured Pruning (70%)\\n(Random pattern)', fontsize=12)\n", "    \n", "    # Row pruning (structured - output channels)\n", "    row_mask = torch.ones_like(weight_matrix)\n", "    rows_to_remove = torch.randperm(weight_matrix.shape[0])[:int(weight_matrix.shape[0]*0.7)]\n", "    row_mask[rows_to_remove] = 0\n", "    axes[0, 2].imshow(weight_matrix * row_mask, cmap='RdBu', aspect='auto')\n", "    axes[0, 2].set_title('Row/Filter Pruning (70%)\\n(Remove entire filters)', fontsize=12)\n", "    \n", "    # Column pruning (structured - input channels)\n", "    col_mask = torch.ones_like(weight_matrix)\n", "    cols_to_remove = torch.randperm(weight_matrix.shape[1])[:int(weight_matrix.shape[1]*0.7)]\n", "    col_mask[:, cols_to_remove] = 0\n", "    axes[1, 0].imshow(weight_matrix * col_mask, cmap='RdBu', aspect='auto')\n", "    axes[1, 0].set_title('Column Pruning (70%)\\n(Remove input channels)', fontsize=12)\n", "    \n", "    # N:M Sparsity (2:4)\n", "    nm_mask = torch.zeros_like(weight_matrix)\n", "    for i in range(weight_matrix.shape[0]):\n", "        for j in range(0, weight_matrix.shape[1], 4):\n", "            if j + 4 <= weight_matrix.shape[1]:\n", "                vals = weight_matrix[i, j:j+4]\n", "                _, top2_idx = vals.abs().topk(2)\n", "                nm_mask[i, j + top2_idx[0]] = 1\n", "                nm_mask[i, j + top2_idx[1]] = 1\n", "    axes[1, 1].imshow(weight_matrix * nm_mask, cmap='RdBu', aspect='auto')\n", "    axes[1, 1].set_title('2:4 Structured Sparsity (50%)\\n(Hardware-friendly)', fontsize=12)\n", "    \n", "    # Block sparsity\n", "    block_mask = torch.ones_like(weight_matrix)\n", "    block_size = 8\n", "    for i in range(0, weight_matrix.shape[0], block_size):\n", "        for j in range(0, weight_matrix.shape[1], block_size):\n", "            if torch.rand(1) > 0.3:  # 70% of blocks removed\n", "                block_mask[i:i+block_size, j:j+block_size] = 0\n", "    axes[1, 2].imshow(weight_matrix * block_mask, cmap='RdBu', aspect='auto')\n", "    axes[1, 2].set_title('Block Sparsity (70%)\\n(GPU-friendly)', fontsize=12)\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "# Create example weight matrix\n", "test_weights = torch.randn(64, 128)\n", "visualize_pruning_types(test_weights)\n", "\n", "print('\\nðŸ“Š PRUNING TYPES COMPARISON')\n", "print('=' * 60)\n", "print(f'{\"Type\":<25} {\"Speedup\":<15} {\"Hardware Support\":<20}')\n", "print('-' * 60)\n", "print(f'{\"Unstructured\":<25} {\"Minimal\":<15} {\"Sparse BLAS\":<20}')\n", "print(f'{\"Row/Column (Structured)\":<25} {\"High\":<15} {\"All GPUs\":<20}')\n", "print(f'{\"2:4 Sparsity\":<25} {\"2x\":<15} {\"NVIDIA Ampere+\":<20}')\n", "print(f'{\"Block Sparsity\":<25} {\"Good\":<15} {\"GPU Tensor Cores\":<20}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Pruning with Fine-tuning Pipeline"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Complete pruning pipeline\n", "from torch.utils.data import DataLoader, TensorDataset\n", "\n", "# Create synthetic data\n", "X_train = torch.randn(1000, 784)\n", "y_train = torch.randint(0, 10, (1000,))\n", "X_test = torch.randn(200, 784)\n", "y_test = torch.randint(0, 10, (200,))\n", "\n", "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32)\n", "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)\n", "\n", "def evaluate(model, loader):\n", "    model.eval()\n", "    correct = 0\n", "    total = 0\n", "    with torch.no_grad():\n", "        for x, y in loader:\n", "            outputs = model(x)\n", "            _, predicted = outputs.max(1)\n", "            total += y.size(0)\n", "            correct += predicted.eq(y).sum().item()\n", "    return 100 * correct / total\n", "\n", "def count_parameters(model):\n", "    total = sum(p.numel() for p in model.parameters())\n", "    nonzero = sum((p != 0).sum().item() for p in model.parameters())\n", "    return total, nonzero\n", "\n", "# Train original model\n", "model = SimpleNet()\n", "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "print('ðŸ“š TRAINING ORIGINAL MODEL')\n", "for epoch in range(5):\n", "    model.train()\n", "    for x, y in train_loader:\n", "        optimizer.zero_grad()\n", "        loss = criterion(model(x), y)\n", "        loss.backward()\n", "        optimizer.step()\n", "\n", "acc_before = evaluate(model, test_loader)\n", "total, nonzero = count_parameters(model)\n", "print(f'Original accuracy: {acc_before:.1f}%')\n", "print(f'Parameters: {total:,} (100% non-zero)')\n", "\n", "# Pruning at different sparsity levels\n", "print('\\nðŸ“Š PRUNING AT DIFFERENT SPARSITY LEVELS')\n", "print('=' * 60)\n", "\n", "sparsity_levels = [0.5, 0.7, 0.9, 0.95, 0.99]\n", "results = []\n", "\n", "for sparsity in sparsity_levels:\n", "    # Create fresh model and copy weights\n", "    pruned_model = SimpleNet()\n", "    pruned_model.load_state_dict(model.state_dict())\n", "    \n", "    # Apply pruning to all linear layers\n", "    for name, module in pruned_model.named_modules():\n", "        if isinstance(module, nn.Linear):\n", "            prune.l1_unstructured(module, name='weight', amount=sparsity)\n", "    \n", "    # Accuracy before fine-tuning\n", "    acc_no_ft = evaluate(pruned_model, test_loader)\n", "    \n", "    # Fine-tune for 3 epochs\n", "    optimizer = torch.optim.Adam(pruned_model.parameters(), lr=0.0001)\n", "    for epoch in range(3):\n", "        pruned_model.train()\n", "        for x, y in train_loader:\n", "            optimizer.zero_grad()\n", "            loss = criterion(pruned_model(x), y)\n", "            loss.backward()\n", "            # Zero out gradients of pruned weights\n", "            for name, module in pruned_model.named_modules():\n", "                if isinstance(module, nn.Linear):\n", "                    module.weight.grad.data *= (module.weight.data != 0).float()\n", "            optimizer.step()\n", "    \n", "    acc_with_ft = evaluate(pruned_model, test_loader)\n", "    results.append((sparsity, acc_no_ft, acc_with_ft))\n", "    print(f'Sparsity {sparsity*100:.0f}%: Before FT={acc_no_ft:.1f}%, After FT={acc_with_ft:.1f}%')\n", "\n", "# Visualize results\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "sparsities = [r[0]*100 for r in results]\n", "acc_no_ft = [r[1] for r in results]\n", "acc_with_ft = [r[2] for r in results]\n", "\n", "ax.plot(sparsities, acc_no_ft, 'o-', label='Without Fine-tuning', color='#ef4444', linewidth=2)\n", "ax.plot(sparsities, acc_with_ft, 's-', label='With Fine-tuning', color='#22c55e', linewidth=2)\n", "ax.axhline(y=acc_before, color='#3b82f6', linestyle='--', label=f'Original ({acc_before:.1f}%)')\n", "\n", "ax.set_xlabel('Sparsity (%)', fontsize=12)\n", "ax.set_ylabel('Accuracy (%)', fontsize=12)\n", "ax.set_title('ðŸ“ˆ Accuracy vs Sparsity', fontsize=14)\n", "ax.legend()\n", "ax.grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ Key Insight: Fine-tuning recovers most accuracy loss from pruning!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('ðŸŽ¯ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. Many weights in trained networks are near-zero')\n", "print('\\n2. Magnitude pruning: Remove smallest absolute values')\n", "print('\\n3. Unstructured pruning: Higher sparsity, but harder to accelerate')\n", "print('\\n4. Structured pruning: Lower sparsity, but real speedups')\n", "print('\\n5. 2:4 Sparsity: Best of both - 2x speedup on NVIDIA GPUs')\n", "print('\\n6. Fine-tuning is crucial to recover accuracy')\n", "print('\\n' + '=' * 60)\n", "print('\\nðŸ“š Next: Lottery Ticket Hypothesis and Advanced Pruning!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
