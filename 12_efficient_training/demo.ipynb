{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üèãÔ∏è Lecture 12: Efficient Training - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/12_efficient_training/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Gradient checkpointing for memory savings\n", "- Mixed precision training (FP16/BF16)\n", "- Gradient accumulation\n", "- Memory-efficient optimizers"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.checkpoint import checkpoint\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "print('Ready for Efficient Training!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Training Memory Breakdown\n", "\n", "Training memory = Model + Gradients + Optimizer States + Activations"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def training_memory_breakdown(params_millions, seq_len=512, batch_size=8, dtype='fp32'):\n", "    \"\"\"\n", "    Calculate memory breakdown for training.\n", "    \"\"\"\n", "    params = params_millions * 1e6\n", "    bytes_per_param = 4 if dtype == 'fp32' else 2\n", "    \n", "    # Model weights\n", "    model_mem = params * bytes_per_param\n", "    \n", "    # Gradients (same size as model)\n", "    grad_mem = params * bytes_per_param\n", "    \n", "    # Optimizer states (Adam: momentum + variance, always FP32)\n", "    opt_mem = params * 4 * 2  # Two FP32 states\n", "    \n", "    # Activations (rough estimate: ~10x model size for typical transformers)\n", "    act_mem = model_mem * 10 * (batch_size / 8)\n", "    \n", "    total = model_mem + grad_mem + opt_mem + act_mem\n", "    \n", "    return {\n", "        'model': model_mem / 1e9,\n", "        'gradients': grad_mem / 1e9,\n", "        'optimizer': opt_mem / 1e9,\n", "        'activations': act_mem / 1e9,\n", "        'total': total / 1e9\n", "    }\n", "\n", "# Analyze different model sizes\n", "print('üìä TRAINING MEMORY BREAKDOWN (FP32)')\n", "print('=' * 70)\n", "print(f'{\"Model\":<15} {\"Weights\":<12} {\"Grads\":<12} {\"Optimizer\":<12} {\"Acts\":<12} {\"Total\":<12}')\n", "print('-' * 70)\n", "\n", "models = {\n", "    'BERT-base': 110,\n", "    'GPT-2': 1500,\n", "    'LLaMA-7B': 7000,\n", "    'LLaMA-70B': 70000,\n", "}\n", "\n", "for name, params in models.items():\n", "    mem = training_memory_breakdown(params)\n", "    print(f'{name:<15} {mem[\"model\"]:>10.1f}GB {mem[\"gradients\"]:>10.1f}GB {mem[\"optimizer\"]:>10.1f}GB {mem[\"activations\"]:>10.1f}GB {mem[\"total\"]:>10.1f}GB')\n", "\n", "print('\\n‚ö†Ô∏è Activations often dominate training memory!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize memory breakdown\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Breakdown for LLaMA-7B\n", "mem = training_memory_breakdown(7000)\n", "labels = ['Model', 'Gradients', 'Optimizer', 'Activations']\n", "sizes = [mem['model'], mem['gradients'], mem['optimizer'], mem['activations']]\n", "colors = ['#3b82f6', '#22c55e', '#f59e0b', '#ef4444']\n", "\n", "axes[0].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n", "axes[0].set_title(f'LLaMA-7B Training Memory\\nTotal: {mem[\"total\"]:.0f} GB')\n", "\n", "# Comparison across models\n", "model_names = list(models.keys())\n", "totals = [training_memory_breakdown(p)['total'] for p in models.values()]\n", "\n", "bars = axes[1].bar(model_names, totals, color='#3b82f6')\n", "axes[1].set_ylabel('Total Memory (GB)')\n", "axes[1].set_title('Training Memory by Model Size')\n", "axes[1].set_yscale('log')\n", "\n", "# Add GPU reference lines\n", "axes[1].axhline(y=24, color='green', linestyle='--', label='RTX 4090 (24GB)')\n", "axes[1].axhline(y=80, color='orange', linestyle='--', label='A100 (80GB)')\n", "axes[1].legend()\n", "\n", "for bar, total in zip(bars, totals):\n", "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,\n", "                 f'{total:.0f}GB', ha='center')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Gradient Checkpointing"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TransformerBlock(nn.Module):\n", "    \"\"\"Standard transformer block.\"\"\"\n", "    def __init__(self, d_model=512, n_heads=8):\n", "        super().__init__()\n", "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n", "        self.ffn = nn.Sequential(\n", "            nn.Linear(d_model, d_model * 4),\n", "            nn.GELU(),\n", "            nn.Linear(d_model * 4, d_model)\n", "        )\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "    \n", "    def forward(self, x):\n", "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n", "        x = x + self.ffn(self.norm2(x))\n", "        return x\n", "\n", "class TransformerWithCheckpointing(nn.Module):\n", "    \"\"\"Transformer that optionally uses gradient checkpointing.\"\"\"\n", "    def __init__(self, n_layers=12, d_model=512, use_checkpointing=False):\n", "        super().__init__()\n", "        self.use_checkpointing = use_checkpointing\n", "        self.layers = nn.ModuleList([TransformerBlock(d_model) for _ in range(n_layers)])\n", "        self.head = nn.Linear(d_model, 1000)\n", "    \n", "    def forward(self, x):\n", "        for layer in self.layers:\n", "            if self.use_checkpointing and self.training:\n", "                # Checkpoint: Don't store activations, recompute in backward\n", "                x = checkpoint(layer, x, use_reentrant=False)\n", "            else:\n", "                x = layer(x)\n", "        return self.head(x.mean(dim=1))\n", "\n", "def measure_memory(model, input_shape, backward=True):\n", "    \"\"\"Measure peak memory during forward/backward.\"\"\"\n", "    torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n", "    \n", "    x = torch.randn(input_shape)\n", "    if torch.cuda.is_available():\n", "        x = x.cuda()\n", "        model = model.cuda()\n", "    \n", "    # Forward\n", "    model.train()\n", "    out = model(x)\n", "    \n", "    if backward:\n", "        loss = out.sum()\n", "        loss.backward()\n", "    \n", "    if torch.cuda.is_available():\n", "        return torch.cuda.max_memory_allocated() / 1e9\n", "    return 0\n", "\n", "print('üìä GRADIENT CHECKPOINTING CONCEPT')\n", "print('=' * 60)\n", "print('\\nWithout checkpointing:')\n", "print('  - Store ALL intermediate activations')\n", "print('  - Memory: O(n_layers √ó batch √ó seq √ó d)')\n", "print('\\nWith checkpointing:')\n", "print('  - Store only layer inputs')\n", "print('  - Recompute activations during backward')\n", "print('  - Memory: O(‚àön_layers √ó batch √ó seq √ó d)')\n", "print('  - Time: +30% (recomputation cost)')\n", "\n", "# Demo models\n", "model_standard = TransformerWithCheckpointing(n_layers=8, use_checkpointing=False)\n", "model_checkpointed = TransformerWithCheckpointing(n_layers=8, use_checkpointing=True)\n", "\n", "# Count parameters\n", "params = sum(p.numel() for p in model_standard.parameters())\n", "print(f'\\nModel parameters: {params/1e6:.1f}M')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simulate memory savings\n", "def simulate_checkpoint_savings(n_layers, batch_size=8, seq_len=512, d_model=512):\n", "    \"\"\"\n", "    Simulate memory savings from checkpointing.\n", "    \"\"\"\n", "    # Activation memory per layer (simplified)\n", "    act_per_layer = batch_size * seq_len * d_model * 4 / 1e9  # GB, FP32\n", "    \n", "    # Without checkpointing: store all\n", "    no_ckpt = n_layers * act_per_layer\n", "    \n", "    # With checkpointing: store sqrt(n) checkpoints\n", "    n_checkpoints = int(np.sqrt(n_layers))\n", "    with_ckpt = n_checkpoints * act_per_layer\n", "    \n", "    return no_ckpt, with_ckpt\n", "\n", "# Compare across different layer counts\n", "layer_counts = [6, 12, 24, 48, 96]\n", "\n", "print('üìä CHECKPOINTING MEMORY SAVINGS')\n", "print('=' * 50)\n", "print(f'{\"Layers\":<10} {\"No Ckpt (GB)\":<15} {\"With Ckpt (GB)\":<15} {\"Savings\":<10}')\n", "print('-' * 50)\n", "\n", "no_ckpt_mems = []\n", "ckpt_mems = []\n", "\n", "for n in layer_counts:\n", "    no_ckpt, with_ckpt = simulate_checkpoint_savings(n)\n", "    savings = (no_ckpt - with_ckpt) / no_ckpt * 100\n", "    no_ckpt_mems.append(no_ckpt)\n", "    ckpt_mems.append(with_ckpt)\n", "    print(f'{n:<10} {no_ckpt:<15.2f} {with_ckpt:<15.2f} {savings:<10.0f}%')\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "x = np.arange(len(layer_counts))\n", "width = 0.35\n", "\n", "bars1 = ax.bar(x - width/2, no_ckpt_mems, width, label='Standard', color='#ef4444')\n", "bars2 = ax.bar(x + width/2, ckpt_mems, width, label='Checkpointed', color='#22c55e')\n", "\n", "ax.set_xlabel('Number of Layers')\n", "ax.set_ylabel('Activation Memory (GB)')\n", "ax.set_title('üìä Gradient Checkpointing Memory Savings')\n", "ax.set_xticks(x)\n", "ax.set_xticklabels(layer_counts)\n", "ax.legend()\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Mixed Precision Training"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mixed_precision_demo():\n", "    \"\"\"\n", "    Demonstrate mixed precision training concepts.\n", "    \"\"\"\n", "    print('üìä MIXED PRECISION TRAINING')\n", "    print('=' * 60)\n", "    print('\\nKey components:')\n", "    print('  1. Forward pass: FP16 (half memory, faster compute)')\n", "    print('  2. Loss scaling: Prevent underflow in FP16 gradients')\n", "    print('  3. Master weights: FP32 copy for accumulation')\n", "    print('  4. Gradient unscaling: Before optimizer step')\n", "    \n", "    # Compare data types\n", "    dtypes = {\n", "        'FP32': {'bytes': 4, 'range': '¬±3.4e38', 'precision': '~7 digits'},\n", "        'FP16': {'bytes': 2, 'range': '¬±65504', 'precision': '~3 digits'},\n", "        'BF16': {'bytes': 2, 'range': '¬±3.4e38', 'precision': '~2 digits'},\n", "    }\n", "    \n", "    print(f'\\n{\"Type\":<8} {\"Bytes\":<8} {\"Range\":<15} {\"Precision\":<15}')\n", "    print('-' * 50)\n", "    for name, info in dtypes.items():\n", "        print(f'{name:<8} {info[\"bytes\"]:<8} {info[\"range\"]:<15} {info[\"precision\"]:<15}')\n", "    \n", "    # Memory savings\n", "    print('\\nüìä MEMORY SAVINGS')\n", "    params_m = 1000  # 1B parameters\n", "    \n", "    fp32_mem = params_m * 4 / 1000  # GB\n", "    mixed_mem = params_m * 2 / 1000 + params_m * 4 / 1000  # FP16 model + FP32 master\n", "    \n", "    print(f'1B param model:')\n", "    print(f'  FP32: {fp32_mem:.1f} GB (model only)')\n", "    print(f'  Mixed: {mixed_mem:.1f} GB (FP16 + FP32 master)')\n", "    print(f'  Activations: 2x savings in FP16!')\n", "\n", "mixed_precision_demo()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simulate loss scaling\n", "def demonstrate_loss_scaling():\n", "    \"\"\"\n", "    Show why loss scaling is needed for FP16 training.\n", "    \"\"\"\n", "    # FP16 smallest positive normal: ~6e-5\n", "    # Gradients can be smaller than this!\n", "    \n", "    # Simulate gradient distribution\n", "    np.random.seed(42)\n", "    gradients = np.abs(np.random.randn(10000)) * 1e-5  # Small gradients\n", "    \n", "    fp16_min = 6e-5  # Approximate FP16 minimum\n", "    \n", "    # Without scaling: many gradients underflow\n", "    underflow_mask = gradients < fp16_min\n", "    underflow_pct = underflow_mask.sum() / len(gradients) * 100\n", "    \n", "    # With scaling (scale = 1024)\n", "    scale = 1024\n", "    scaled_gradients = gradients * scale\n", "    scaled_underflow = (scaled_gradients < fp16_min).sum() / len(gradients) * 100\n", "    \n", "    print('üìä LOSS SCALING DEMONSTRATION')\n", "    print('=' * 50)\n", "    print(f'Sample gradient magnitude: ~{np.mean(gradients):.2e}')\n", "    print(f'FP16 minimum normal: ~{fp16_min:.0e}')\n", "    print(f'\\nWithout scaling: {underflow_pct:.1f}% gradients underflow')\n", "    print(f'With scaling (1024x): {scaled_underflow:.1f}% gradients underflow')\n", "    \n", "    # Visualize\n", "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "    \n", "    axes[0].hist(np.log10(gradients + 1e-10), bins=50, color='#ef4444', alpha=0.7)\n", "    axes[0].axvline(x=np.log10(fp16_min), color='black', linestyle='--', \n", "                    linewidth=2, label=f'FP16 min ({fp16_min:.0e})')\n", "    axes[0].set_xlabel('log10(gradient)')\n", "    axes[0].set_ylabel('Count')\n", "    axes[0].set_title('Without Loss Scaling')\n", "    axes[0].legend()\n", "    \n", "    axes[1].hist(np.log10(scaled_gradients + 1e-10), bins=50, color='#22c55e', alpha=0.7)\n", "    axes[1].axvline(x=np.log10(fp16_min), color='black', linestyle='--', \n", "                    linewidth=2, label=f'FP16 min ({fp16_min:.0e})')\n", "    axes[1].set_xlabel('log10(gradient)')\n", "    axes[1].set_ylabel('Count')\n", "    axes[1].set_title('With Loss Scaling (1024x)')\n", "    axes[1].legend()\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "demonstrate_loss_scaling()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Gradient Accumulation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gradient_accumulation_demo():\n", "    \"\"\"\n", "    Demonstrate gradient accumulation for large effective batch sizes.\n", "    \"\"\"\n", "    print('üìä GRADIENT ACCUMULATION')\n", "    print('=' * 60)\n", "    print('\\nProblem: Want large batch (e.g., 1024) but only fit batch=8')\n", "    print('\\nSolution: Accumulate gradients over multiple mini-batches')\n", "    print('\\nPseudocode:')\n", "    print('''    \n", "    accumulation_steps = 128  # 1024 / 8\n", "    for i, (x, y) in enumerate(dataloader):\n", "        loss = model(x, y) / accumulation_steps  # Scale loss\n", "        loss.backward()  # Accumulate gradients\n", "        \n", "        if (i + 1) % accumulation_steps == 0:\n", "            optimizer.step()  # Update weights\n", "            optimizer.zero_grad()  # Reset gradients\n", "    ''')\n", "    \n", "    # Comparison\n", "    scenarios = [\n", "        ('Standard', 8, 1, 8),\n", "        ('4x Accumulation', 8, 4, 32),\n", "        ('16x Accumulation', 8, 16, 128),\n", "        ('128x Accumulation', 8, 128, 1024),\n", "    ]\n", "    \n", "    print('\\nüìä EFFECTIVE BATCH SIZE COMPARISON')\n", "    print(f'{\"Method\":<20} {\"Mini-batch\":<12} {\"Accum Steps\":<12} {\"Effective\":<12}')\n", "    print('-' * 60)\n", "    for name, mini, accum, effective in scenarios:\n", "        print(f'{name:<20} {mini:<12} {accum:<12} {effective:<12}')\n", "\n", "gradient_accumulation_demo()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Verify gradient accumulation produces same result\n", "class SimpleModel(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc = nn.Linear(10, 1)\n", "    \n", "    def forward(self, x):\n", "        return self.fc(x)\n", "\n", "# Create data\n", "torch.manual_seed(42)\n", "X = torch.randn(32, 10)\n", "y = torch.randn(32, 1)\n", "\n", "# Method 1: Full batch\n", "model1 = SimpleModel()\n", "model1.fc.weight.data = torch.randn_like(model1.fc.weight)\n", "model1.fc.bias.data = torch.zeros_like(model1.fc.bias)\n", "initial_weight = model1.fc.weight.data.clone()\n", "\n", "out1 = model1(X)\n", "loss1 = F.mse_loss(out1, y)\n", "loss1.backward()\n", "grad_full = model1.fc.weight.grad.clone()\n", "\n", "# Method 2: Accumulated (4 mini-batches of 8)\n", "model2 = SimpleModel()\n", "model2.fc.weight.data = initial_weight.clone()\n", "model2.fc.bias.data = torch.zeros_like(model2.fc.bias)\n", "\n", "accum_steps = 4\n", "mini_batch = 8\n", "\n", "model2.zero_grad()\n", "for i in range(accum_steps):\n", "    start = i * mini_batch\n", "    end = (i + 1) * mini_batch\n", "    out2 = model2(X[start:end])\n", "    loss2 = F.mse_loss(out2, y[start:end]) / accum_steps  # Scale loss!\n", "    loss2.backward()\n", "\n", "grad_accum = model2.fc.weight.grad.clone()\n", "\n", "print('üìä GRADIENT ACCUMULATION VERIFICATION')\n", "print('=' * 50)\n", "print(f'Full batch gradient norm: {grad_full.norm():.6f}')\n", "print(f'Accumulated gradient norm: {grad_accum.norm():.6f}')\n", "print(f'Difference: {(grad_full - grad_accum).abs().max():.8f}')\n", "print(f'\\n‚úÖ Gradients match!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Memory-Efficient Optimizers"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def optimizer_memory_comparison(params_billions):\n", "    \"\"\"\n", "    Compare memory usage of different optimizers.\n", "    \"\"\"\n", "    params = params_billions * 1e9\n", "    \n", "    optimizers = {\n", "        'SGD': params * 4,  # Just momentum (FP32)\n", "        'SGD + Momentum': params * 4,  # Momentum state\n", "        'Adam': params * 4 * 2,  # m and v states (FP32)\n", "        'AdaFactor': params * 4 * 0.5,  # Factorized states\n", "        '8-bit Adam': params * 1 * 2,  # Quantized states\n", "    }\n", "    \n", "    print('üìä OPTIMIZER MEMORY USAGE')\n", "    print('=' * 50)\n", "    print(f'Model: {params_billions}B parameters')\n", "    print(f'\\n{\"Optimizer\":<20} {\"State Memory (GB)\":<20}')\n", "    print('-' * 40)\n", "    \n", "    for name, mem in optimizers.items():\n", "        mem_gb = mem / 1e9\n", "        print(f'{name:<20} {mem_gb:<20.1f}')\n", "    \n", "    return optimizers\n", "\n", "opt_mems = optimizer_memory_comparison(7)  # LLaMA-7B\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "names = list(opt_mems.keys())\n", "mems = [m / 1e9 for m in opt_mems.values()]\n", "colors = ['#22c55e', '#22c55e', '#ef4444', '#f59e0b', '#3b82f6']\n", "\n", "bars = ax.bar(names, mems, color=colors)\n", "ax.set_ylabel('Memory (GB)')\n", "ax.set_title('üìä Optimizer State Memory for 7B Model')\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "for bar, mem in zip(bars, mems):\n", "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n", "            f'{mem:.0f}GB', ha='center')\n", "\n", "plt.xticks(rotation=15)\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('üéØ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. Training memory: Model + Grads + Optimizer + Activations')\n", "print('\\n2. Gradient Checkpointing: ‚àöN memory, +30% time')\n", "print('\\n3. Mixed Precision: 2x faster, needs loss scaling')\n", "print('\\n4. Gradient Accumulation: Large batch without memory increase')\n", "print('\\n5. 8-bit Optimizers: 4x less optimizer memory')\n", "print('\\n6. Combine all for maximum efficiency!')\n", "print('\\n' + '=' * 60)\n", "print('\\nüìö Next: On-Device Training!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
