{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# âš¡ Lecture 11: Efficient Transformers - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/11_efficient_transformers/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Attention complexity analysis (O(NÂ²) problem)\n", "- Linear attention mechanisms\n", "- FlashAttention and memory efficiency\n", "- Sparse attention patterns"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import time\n", "\n", "torch.manual_seed(42)\n", "print('Ready for Efficient Transformers!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: The O(NÂ²) Problem\n", "\n", "Standard attention: $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$\n", "\n", "The $QK^T$ matrix is $N \\times N$ â†’ O(NÂ²) memory and compute!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def standard_attention(Q, K, V):\n", "    \"\"\"\n", "    Standard scaled dot-product attention.\n", "    Memory: O(NÂ²) for attention matrix\n", "    Compute: O(NÂ² Ã— d)\n", "    \"\"\"\n", "    d_k = Q.shape[-1]\n", "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)  # N Ã— N matrix!\n", "    attn_weights = F.softmax(scores, dim=-1)\n", "    return torch.matmul(attn_weights, V), attn_weights\n", "\n", "# Analyze scaling\n", "seq_lengths = [128, 256, 512, 1024, 2048, 4096]\n", "d_model = 64\n", "\n", "print('ðŸ“Š ATTENTION COMPLEXITY SCALING')\n", "print('=' * 60)\n", "print(f'{\"Seq Length\":<12} {\"Memory (MB)\":<15} {\"Time (ms)\":<15}')\n", "print('-' * 60)\n", "\n", "memory_usage = []\n", "time_usage = []\n", "\n", "for seq_len in seq_lengths:\n", "    Q = torch.randn(1, seq_len, d_model)\n", "    K = torch.randn(1, seq_len, d_model)\n", "    V = torch.randn(1, seq_len, d_model)\n", "    \n", "    # Memory for attention matrix (N Ã— N Ã— 4 bytes for float32)\n", "    attn_memory_mb = seq_len * seq_len * 4 / 1024 / 1024\n", "    memory_usage.append(attn_memory_mb)\n", "    \n", "    # Time measurement\n", "    start = time.time()\n", "    for _ in range(10):\n", "        _, _ = standard_attention(Q, K, V)\n", "    elapsed = (time.time() - start) / 10 * 1000\n", "    time_usage.append(elapsed)\n", "    \n", "    print(f'{seq_len:<12} {attn_memory_mb:<15.2f} {elapsed:<15.2f}')\n", "\n", "print('\\nâš ï¸ Memory grows QUADRATICALLY with sequence length!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize quadratic scaling\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Memory\n", "axes[0].plot(seq_lengths, memory_usage, 'o-', color='#ef4444', linewidth=2, markersize=10)\n", "axes[0].fill_between(seq_lengths, memory_usage, alpha=0.3, color='#ef4444')\n", "axes[0].set_xlabel('Sequence Length')\n", "axes[0].set_ylabel('Memory (MB)')\n", "axes[0].set_title('ðŸ“Š Attention Memory: O(NÂ²)')\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "# Add quadratic reference\n", "x_ref = np.array(seq_lengths)\n", "y_ref = (x_ref / seq_lengths[0]) ** 2 * memory_usage[0]\n", "axes[0].plot(x_ref, y_ref, '--', color='gray', label='O(NÂ²) reference')\n", "axes[0].legend()\n", "\n", "# Time\n", "axes[1].plot(seq_lengths, time_usage, 'o-', color='#3b82f6', linewidth=2, markersize=10)\n", "axes[1].fill_between(seq_lengths, time_usage, alpha=0.3, color='#3b82f6')\n", "axes[1].set_xlabel('Sequence Length')\n", "axes[1].set_ylabel('Time (ms)')\n", "axes[1].set_title('ðŸ“Š Attention Time: O(NÂ² Ã— d)')\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Real-world implications\n", "print('\\nðŸ“Š REAL-WORLD IMPLICATIONS')\n", "print('=' * 50)\n", "contexts = [4096, 32000, 128000, 1000000]\n", "for ctx in contexts:\n", "    mem_gb = ctx * ctx * 4 / 1024**3\n", "    print(f'{ctx:>10} tokens: {mem_gb:>10.2f} GB for attention matrix')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Linear Attention"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def linear_attention(Q, K, V, eps=1e-6):\n", "    \"\"\"\n", "    Linear attention using kernel trick.\n", "    \n", "    Key insight: softmax(QK^T)V â‰ˆ Ï†(Q)(Ï†(K)^T V)\n", "    \n", "    Instead of NÃ—N attention matrix, compute:\n", "    1. Ï†(K)^T V: dÃ—d matrix (small!)\n", "    2. Ï†(Q) Ã— result: NÃ—d\n", "    \n", "    Memory: O(NÃ—d) instead of O(NÂ²)\n", "    \"\"\"\n", "    # Simple kernel: ELU + 1 (ensures positivity)\n", "    Q_prime = F.elu(Q) + 1\n", "    K_prime = F.elu(K) + 1\n", "    \n", "    # Compute K^T V first: (d Ã— N) @ (N Ã— d) = d Ã— d\n", "    KV = torch.matmul(K_prime.transpose(-2, -1), V)  # d Ã— d matrix!\n", "    \n", "    # Normalization factor\n", "    K_sum = K_prime.sum(dim=-2, keepdim=True)  # 1 Ã— d\n", "    \n", "    # Compute output: (N Ã— d) @ (d Ã— d) = N Ã— d\n", "    numerator = torch.matmul(Q_prime, KV)\n", "    denominator = torch.matmul(Q_prime, K_sum.transpose(-2, -1)) + eps\n", "    \n", "    return numerator / denominator\n", "\n", "# Compare standard vs linear attention\n", "print('ðŸ“Š STANDARD vs LINEAR ATTENTION')\n", "print('=' * 60)\n", "print(f'{\"Seq Length\":<12} {\"Standard (ms)\":<15} {\"Linear (ms)\":<15} {\"Speedup\":<10}')\n", "print('-' * 60)\n", "\n", "standard_times = []\n", "linear_times = []\n", "\n", "for seq_len in [256, 512, 1024, 2048, 4096]:\n", "    Q = torch.randn(1, seq_len, d_model)\n", "    K = torch.randn(1, seq_len, d_model)\n", "    V = torch.randn(1, seq_len, d_model)\n", "    \n", "    # Standard\n", "    start = time.time()\n", "    for _ in range(10):\n", "        _ = standard_attention(Q, K, V)\n", "    std_time = (time.time() - start) / 10 * 1000\n", "    standard_times.append(std_time)\n", "    \n", "    # Linear\n", "    start = time.time()\n", "    for _ in range(10):\n", "        _ = linear_attention(Q, K, V)\n", "    lin_time = (time.time() - start) / 10 * 1000\n", "    linear_times.append(lin_time)\n", "    \n", "    speedup = std_time / lin_time\n", "    print(f'{seq_len:<12} {std_time:<15.2f} {lin_time:<15.2f} {speedup:<10.1f}x')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize comparison\n", "seq_lens = [256, 512, 1024, 2048, 4096]\n", "\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Time comparison\n", "x = np.arange(len(seq_lens))\n", "width = 0.35\n", "\n", "axes[0].bar(x - width/2, standard_times, width, label='Standard O(NÂ²)', color='#ef4444')\n", "axes[0].bar(x + width/2, linear_times, width, label='Linear O(N)', color='#22c55e')\n", "axes[0].set_xlabel('Sequence Length')\n", "axes[0].set_ylabel('Time (ms)')\n", "axes[0].set_title('ðŸ“Š Attention Time Comparison')\n", "axes[0].set_xticks(x)\n", "axes[0].set_xticklabels(seq_lens)\n", "axes[0].legend()\n", "axes[0].grid(True, alpha=0.3, axis='y')\n", "\n", "# Scaling comparison (log scale)\n", "axes[1].semilogy(seq_lens, standard_times, 'o-', label='Standard O(NÂ²)', color='#ef4444', linewidth=2)\n", "axes[1].semilogy(seq_lens, linear_times, 's-', label='Linear O(N)', color='#22c55e', linewidth=2)\n", "axes[1].set_xlabel('Sequence Length')\n", "axes[1].set_ylabel('Time (ms, log scale)')\n", "axes[1].set_title('ðŸ“Š Scaling Behavior')\n", "axes[1].legend()\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Sparse Attention Patterns"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_attention_patterns(seq_len):\n", "    \"\"\"\n", "    Create different sparse attention patterns.\n", "    \"\"\"\n", "    patterns = {}\n", "    \n", "    # Full attention\n", "    patterns['Full'] = torch.ones(seq_len, seq_len)\n", "    \n", "    # Local/sliding window (each token attends to neighbors)\n", "    window_size = seq_len // 8\n", "    local = torch.zeros(seq_len, seq_len)\n", "    for i in range(seq_len):\n", "        start = max(0, i - window_size)\n", "        end = min(seq_len, i + window_size + 1)\n", "        local[i, start:end] = 1\n", "    patterns['Local (Window)'] = local\n", "    \n", "    # Strided/dilated\n", "    stride = 4\n", "    strided = torch.zeros(seq_len, seq_len)\n", "    for i in range(seq_len):\n", "        strided[i, ::stride] = 1  # Every stride-th position\n", "        strided[i, max(0, i-2):min(seq_len, i+3)] = 1  # Local context\n", "    patterns['Strided'] = strided\n", "    \n", "    # Global + Local (Longformer style)\n", "    global_local = local.clone()\n", "    num_global = 4\n", "    global_local[:num_global, :] = 1  # First tokens are global\n", "    global_local[:, :num_global] = 1\n", "    patterns['Global+Local'] = global_local\n", "    \n", "    # Block sparse (BigBird style)\n", "    block_size = seq_len // 8\n", "    block_sparse = torch.zeros(seq_len, seq_len)\n", "    for i in range(0, seq_len, block_size):\n", "        block_sparse[i:i+block_size, i:i+block_size] = 1\n", "    # Add some random blocks\n", "    for _ in range(seq_len // block_size):\n", "        ri = np.random.randint(0, seq_len // block_size) * block_size\n", "        rj = np.random.randint(0, seq_len // block_size) * block_size\n", "        block_sparse[ri:ri+block_size, rj:rj+block_size] = 1\n", "    patterns['Block Sparse'] = block_sparse\n", "    \n", "    return patterns\n", "\n", "# Visualize patterns\n", "seq_len = 64\n", "patterns = create_attention_patterns(seq_len)\n", "\n", "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n", "\n", "for ax, (name, pattern) in zip(axes, patterns.items()):\n", "    sparsity = 100 * (1 - pattern.sum() / pattern.numel())\n", "    ax.imshow(pattern, cmap='Blues', aspect='auto')\n", "    ax.set_title(f'{name}\\n{sparsity:.1f}% sparse')\n", "    ax.set_xlabel('Key Position')\n", "    ax.set_ylabel('Query Position')\n", "\n", "plt.suptitle('ðŸ“Š Sparse Attention Patterns', fontsize=14)\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sparse_attention(Q, K, V, mask):\n", "    \"\"\"\n", "    Attention with sparse mask.\n", "    \n", "    In practice, this is implemented more efficiently\n", "    using block-sparse operations or FlashAttention.\n", "    \"\"\"\n", "    d_k = Q.shape[-1]\n", "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n", "    \n", "    # Apply sparse mask\n", "    scores = scores.masked_fill(mask == 0, float('-inf'))\n", "    \n", "    attn_weights = F.softmax(scores, dim=-1)\n", "    attn_weights = attn_weights.masked_fill(mask == 0, 0)  # Clean up NaNs\n", "    \n", "    return torch.matmul(attn_weights, V)\n", "\n", "# Compare accuracy vs efficiency trade-off\n", "print('ðŸ“Š SPARSE ATTENTION EFFICIENCY')\n", "print('=' * 60)\n", "\n", "seq_len = 256\n", "Q = torch.randn(1, seq_len, d_model)\n", "K = torch.randn(1, seq_len, d_model)\n", "V = torch.randn(1, seq_len, d_model)\n", "\n", "# Full attention output (ground truth)\n", "full_out, _ = standard_attention(Q, K, V)\n", "\n", "patterns = create_attention_patterns(seq_len)\n", "\n", "print(f'{\"Pattern\":<20} {\"Sparsity (%)\":<15} {\"MSE vs Full\":<15} {\"Speedup\":<10}')\n", "print('-' * 60)\n", "\n", "for name, mask in patterns.items():\n", "    sparsity = 100 * (1 - mask.sum() / mask.numel())\n", "    \n", "    # Compute sparse attention\n", "    sparse_out = sparse_attention(Q, K, V, mask.unsqueeze(0))\n", "    \n", "    # Measure difference from full attention\n", "    mse = torch.mean((full_out - sparse_out) ** 2).item()\n", "    \n", "    # Theoretical speedup (proportional to non-zero entries)\n", "    speedup = mask.numel() / mask.sum().item()\n", "    \n", "    print(f'{name:<20} {sparsity:<15.1f} {mse:<15.6f} {speedup:<10.1f}x')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: FlashAttention Concept"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def flash_attention_demo(Q, K, V, block_size=64):\n", "    \"\"\"\n", "    Simplified FlashAttention demonstration.\n", "    \n", "    Key ideas:\n", "    1. Compute attention in blocks to fit in fast SRAM\n", "    2. Never materialize full NÃ—N attention matrix\n", "    3. Use online softmax (running max and sum)\n", "    \n", "    This is a simplified version - real FlashAttention uses CUDA kernels.\n", "    \"\"\"\n", "    batch, seq_len, d = Q.shape\n", "    \n", "    # Output and running statistics\n", "    O = torch.zeros_like(V)\n", "    L = torch.zeros(batch, seq_len, 1)  # Log-sum-exp\n", "    M = torch.full((batch, seq_len, 1), float('-inf'))  # Running max\n", "    \n", "    # Process in blocks\n", "    num_blocks = (seq_len + block_size - 1) // block_size\n", "    \n", "    for j in range(num_blocks):\n", "        # Key-Value block\n", "        kv_start = j * block_size\n", "        kv_end = min((j + 1) * block_size, seq_len)\n", "        K_block = K[:, kv_start:kv_end, :]\n", "        V_block = V[:, kv_start:kv_end, :]\n", "        \n", "        for i in range(num_blocks):\n", "            # Query block\n", "            q_start = i * block_size\n", "            q_end = min((i + 1) * block_size, seq_len)\n", "            Q_block = Q[:, q_start:q_end, :]\n", "            \n", "            # Compute block attention scores\n", "            S_block = torch.matmul(Q_block, K_block.transpose(-2, -1)) / np.sqrt(d)\n", "            \n", "            # Online softmax update\n", "            M_block = S_block.max(dim=-1, keepdim=True).values\n", "            P_block = torch.exp(S_block - M_block)\n", "            L_block = P_block.sum(dim=-1, keepdim=True)\n", "            \n", "            # Update running statistics\n", "            M_new = torch.maximum(M[:, q_start:q_end, :], M_block)\n", "            \n", "            exp_diff_old = torch.exp(M[:, q_start:q_end, :] - M_new)\n", "            exp_diff_new = torch.exp(M_block - M_new)\n", "            \n", "            L_new = exp_diff_old * L[:, q_start:q_end, :] + exp_diff_new * L_block\n", "            \n", "            # Update output\n", "            O[:, q_start:q_end, :] = (\n", "                exp_diff_old * L[:, q_start:q_end, :] * O[:, q_start:q_end, :] +\n", "                exp_diff_new * torch.matmul(P_block, V_block)\n", "            ) / (L_new + 1e-8)\n", "            \n", "            # Update statistics\n", "            M[:, q_start:q_end, :] = M_new\n", "            L[:, q_start:q_end, :] = L_new\n", "    \n", "    return O\n", "\n", "# Compare outputs\n", "print('ðŸ“Š FLASHATTENTION DEMONSTRATION')\n", "print('=' * 50)\n", "\n", "Q = torch.randn(1, 256, 64)\n", "K = torch.randn(1, 256, 64)\n", "V = torch.randn(1, 256, 64)\n", "\n", "# Standard attention\n", "standard_out, _ = standard_attention(Q, K, V)\n", "\n", "# Flash attention (simplified)\n", "flash_out = flash_attention_demo(Q, K, V, block_size=32)\n", "\n", "# Compare\n", "mse = torch.mean((standard_out - flash_out) ** 2).item()\n", "print(f'MSE between standard and Flash: {mse:.8f}')\n", "print(f'Outputs match: {mse < 1e-5}')\n", "\n", "print('\\nðŸ’¡ FlashAttention benefits:')\n", "print('   - 2-4x faster than standard attention')\n", "print('   - Memory: O(N) instead of O(NÂ²)')\n", "print('   - Enables much longer contexts')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Efficient Attention Summary"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Summary comparison\n", "methods = {\n", "    'Standard': {'complexity': 'O(NÂ²)', 'memory': 'O(NÂ²)', 'exact': 'Yes', 'contexts': '2-8K'},\n", "    'Linear (Performer)': {'complexity': 'O(N)', 'memory': 'O(N)', 'exact': 'Approx', 'contexts': '64K+'},\n", "    'Sparse (Longformer)': {'complexity': 'O(NâˆšN)', 'memory': 'O(NâˆšN)', 'exact': 'Approx', 'contexts': '16K'},\n", "    'FlashAttention': {'complexity': 'O(NÂ²)', 'memory': 'O(N)', 'exact': 'Yes', 'contexts': '32K+'},\n", "    'Ring Attention': {'complexity': 'O(NÂ²/P)', 'memory': 'O(N/P)', 'exact': 'Yes', 'contexts': '1M+'},\n", "}\n", "\n", "print('ðŸ“Š EFFICIENT ATTENTION METHODS')\n", "print('=' * 80)\n", "print(f'{\"Method\":<20} {\"Time\":<12} {\"Memory\":<12} {\"Exact?\":<10} {\"Context\":<12}')\n", "print('-' * 80)\n", "for name, info in methods.items():\n", "    print(f'{name:<20} {info[\"complexity\"]:<12} {info[\"memory\"]:<12} {info[\"exact\"]:<10} {info[\"contexts\"]:<12}')\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "\n", "# Simulated scaling\n", "seq_lens = np.array([1024, 2048, 4096, 8192, 16384, 32768])\n", "\n", "scaling = {\n", "    'Standard': seq_lens ** 2 / seq_lens[0] ** 2,\n", "    'Linear': seq_lens / seq_lens[0],\n", "    'Sparse': seq_lens * np.sqrt(seq_lens) / (seq_lens[0] * np.sqrt(seq_lens[0])),\n", "    'FlashAttention': seq_lens ** 2 / seq_lens[0] ** 2 / 3,  # Same complexity but faster\n", "}\n", "\n", "colors = ['#ef4444', '#22c55e', '#3b82f6', '#f59e0b']\n", "\n", "for (name, scale), color in zip(scaling.items(), colors):\n", "    ax.semilogy(seq_lens, scale, 'o-', label=name, linewidth=2, markersize=8, color=color)\n", "\n", "ax.set_xlabel('Sequence Length')\n", "ax.set_ylabel('Relative Time (log scale)')\n", "ax.set_title('ðŸ“Š Attention Method Scaling')\n", "ax.legend()\n", "ax.grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('ðŸŽ¯ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. Standard attention is O(NÂ²) - limits context length')\n", "print('\\n2. Linear attention: O(N) via kernel trick, approximate')\n", "print('\\n3. Sparse attention: Skip some pairs, structured patterns')\n", "print('\\n4. FlashAttention: O(N) memory, exact, uses tiling')\n", "print('\\n5. Choose method based on accuracy vs efficiency needs')\n", "print('\\n6. Modern LLMs combine multiple techniques')\n", "print('\\n' + '=' * 60)\n", "print('\\nðŸ“š Next: Efficient Training!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
