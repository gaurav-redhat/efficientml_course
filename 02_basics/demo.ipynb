{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üìä Lecture 2: ML Efficiency Basics - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/02_basics/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- How to count FLOPs for different layer types\n", "- Memory bandwidth and arithmetic intensity\n", "- Roofline model analysis\n", "- Bottleneck identification (compute vs memory bound)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "print('Ready for efficiency analysis!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Counting FLOPs (Floating Point Operations)\n", "\n", "Understanding computational cost is the first step to optimization."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def count_flops_linear(in_features, out_features, batch_size=1):\n", "    \"\"\"\n", "    Linear layer: y = Wx + b\n", "    FLOPs = batch_size √ó (2 √ó in √ó out)  # multiply-add counted as 2\n", "    \"\"\"\n", "    multiply = batch_size * in_features * out_features\n", "    add = batch_size * in_features * out_features  # or (in_features-1)*out_features + out_features for bias\n", "    return multiply + add\n", "\n", "def count_flops_conv2d(in_channels, out_channels, kernel_size, height, width, batch_size=1):\n", "    \"\"\"\n", "    Conv2D: FLOPs = batch √ó out_h √ó out_w √ó out_ch √ó in_ch √ó K √ó K √ó 2\n", "    \"\"\"\n", "    k = kernel_size\n", "    out_h, out_w = height - k + 1, width - k + 1  # no padding\n", "    flops_per_output = 2 * in_channels * k * k\n", "    total = batch_size * out_h * out_w * out_channels * flops_per_output\n", "    return total\n", "\n", "def count_flops_attention(seq_len, d_model, n_heads):\n", "    \"\"\"\n", "    Self-attention FLOPs:\n", "    - QKV projection: 3 √ó (2 √ó seq √ó d √ó d)\n", "    - Attention scores: 2 √ó seq √ó seq √ó d\n", "    - Attention @ V: 2 √ó seq √ó seq √ó d\n", "    - Output projection: 2 √ó seq √ó d √ó d\n", "    \"\"\"\n", "    qkv_proj = 3 * 2 * seq_len * d_model * d_model\n", "    attn_scores = 2 * seq_len * seq_len * d_model\n", "    attn_values = 2 * seq_len * seq_len * d_model\n", "    output_proj = 2 * seq_len * d_model * d_model\n", "    return qkv_proj + attn_scores + attn_values + output_proj\n", "\n", "# Example calculations\n", "print('üìä FLOP CALCULATIONS')\n", "print('=' * 60)\n", "\n", "# Linear layer\n", "linear_flops = count_flops_linear(768, 3072, batch_size=1)\n", "print(f'\\n1Ô∏è‚É£ Linear Layer (768 ‚Üí 3072):')\n", "print(f'   FLOPs = {linear_flops:,} = {linear_flops/1e6:.2f}M')\n", "\n", "# Conv2D\n", "conv_flops = count_flops_conv2d(64, 128, 3, 224, 224)\n", "print(f'\\n2Ô∏è‚É£ Conv2D (64‚Üí128, 3√ó3, 224√ó224):')\n", "print(f'   FLOPs = {conv_flops:,} = {conv_flops/1e9:.2f}G')\n", "\n", "# Self-attention\n", "attn_flops = count_flops_attention(512, 768, 12)\n", "print(f'\\n3Ô∏è‚É£ Self-Attention (seq=512, d=768):')\n", "print(f'   FLOPs = {attn_flops:,} = {attn_flops/1e9:.2f}G')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Memory Bandwidth Analysis"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def memory_bytes_linear(in_features, out_features, batch_size=1, dtype_bytes=4):\n", "    \"\"\"\n", "    Memory accessed for linear layer:\n", "    - Read weights: in √ó out √ó dtype\n", "    - Read input: batch √ó in √ó dtype\n", "    - Write output: batch √ó out √ó dtype\n", "    \"\"\"\n", "    weights = in_features * out_features * dtype_bytes\n", "    input_mem = batch_size * in_features * dtype_bytes\n", "    output_mem = batch_size * out_features * dtype_bytes\n", "    return weights + input_mem + output_mem\n", "\n", "def arithmetic_intensity(flops, bytes_accessed):\n", "    \"\"\"\n", "    Arithmetic Intensity = FLOPs / Bytes\n", "    Higher is better (more compute per memory access)\n", "    \"\"\"\n", "    return flops / bytes_accessed\n", "\n", "# Compare different batch sizes\n", "print('üìà ARITHMETIC INTENSITY vs BATCH SIZE')\n", "print('=' * 60)\n", "print(f'{\"Batch\":<10} {\"FLOPs\":<15} {\"Memory\":<15} {\"AI (FLOPs/Byte)\":<15}')\n", "print('-' * 60)\n", "\n", "batch_sizes = [1, 8, 32, 128, 512]\n", "ais = []\n", "\n", "for bs in batch_sizes:\n", "    flops = count_flops_linear(768, 3072, bs)\n", "    mem = memory_bytes_linear(768, 3072, bs)\n", "    ai = arithmetic_intensity(flops, mem)\n", "    ais.append(ai)\n", "    print(f'{bs:<10} {flops:>12,}   {mem:>12,}   {ai:>12.2f}')\n", "\n", "# Visualize\n", "plt.figure(figsize=(10, 5))\n", "plt.bar([str(b) for b in batch_sizes], ais, color='#3b82f6')\n", "plt.xlabel('Batch Size')\n", "plt.ylabel('Arithmetic Intensity (FLOPs/Byte)')\n", "plt.title('üìà Arithmetic Intensity Increases with Batch Size')\n", "plt.axhline(y=125, color='r', linestyle='--', label='GPU threshold (H100)')\n", "plt.legend()\n", "plt.show()\n", "\n", "print('\\nüí° Insight: Larger batches = better hardware utilization!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: The Roofline Model\n", "\n", "The roofline model helps identify if you're compute-bound or memory-bound."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# GPU specifications\n", "gpus = {\n", "    'A100': {'compute_tflops': 312, 'memory_bw': 2039},  # TF32 peak\n", "    'H100': {'compute_tflops': 989, 'memory_bw': 3350},\n", "    'RTX 4090': {'compute_tflops': 82, 'memory_bw': 1008},\n", "    'M2 Pro': {'compute_tflops': 3.6, 'memory_bw': 200},\n", "}\n", "\n", "# Plot roofline for H100\n", "gpu = 'H100'\n", "compute_peak = gpus[gpu]['compute_tflops'] * 1e12  # Convert to FLOPS\n", "memory_bw = gpus[gpu]['memory_bw'] * 1e9  # Convert to Bytes/s\n", "\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "\n", "# Arithmetic intensity range\n", "ai_range = np.logspace(-2, 4, 1000)\n", "\n", "# Roofline: min(peak_compute, ai √ó memory_bw)\n", "performance = np.minimum(compute_peak, ai_range * memory_bw)\n", "\n", "ax.loglog(ai_range, performance / 1e12, 'b-', linewidth=2, label='Roofline')\n", "\n", "# Ridge point\n", "ridge_point = compute_peak / memory_bw\n", "ax.axvline(x=ridge_point, color='r', linestyle='--', alpha=0.5)\n", "ax.text(ridge_point * 1.2, 100, f'Ridge Point\\nAI = {ridge_point:.1f}', fontsize=10)\n", "\n", "# Plot some operations\n", "operations = {\n", "    'MatMul (small batch)': 0.5,\n", "    'MatMul (large batch)': 200,\n", "    'Attention (short seq)': 10,\n", "    'Attention (long seq)': 150,\n", "    'LayerNorm': 1,\n", "    'Softmax': 2,\n", "}\n", "\n", "for name, ai in operations.items():\n", "    perf = min(compute_peak, ai * memory_bw) / 1e12\n", "    marker = 'o' if ai < ridge_point else 's'\n", "    color = 'red' if ai < ridge_point else 'green'\n", "    ax.scatter(ai, perf * 0.7, marker=marker, s=100, c=color, zorder=5)\n", "    ax.annotate(name, (ai, perf * 0.7), xytext=(5, 10), textcoords='offset points', fontsize=9)\n", "\n", "ax.set_xlabel('Arithmetic Intensity (FLOPs/Byte)', fontsize=12)\n", "ax.set_ylabel('Performance (TFLOPS)', fontsize=12)\n", "ax.set_title(f'üìä Roofline Model for {gpu}', fontsize=14)\n", "ax.set_xlim(0.01, 10000)\n", "ax.set_ylim(0.1, 2000)\n", "ax.grid(True, alpha=0.3)\n", "ax.legend()\n", "\n", "# Add regions\n", "ax.fill_between([0.01, ridge_point], [0.1, 0.1], [2000, 2000], alpha=0.1, color='red', label='Memory Bound')\n", "ax.fill_between([ridge_point, 10000], [0.1, 0.1], [2000, 2000], alpha=0.1, color='green', label='Compute Bound')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüî¥ Red points = Memory-bound (limited by memory bandwidth)')\n", "print('üü¢ Green points = Compute-bound (limited by FLOPS)')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Comparing Different Model Architectures"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Real model analysis\n", "models = {\n", "    'ResNet-50': {'params': 25.6, 'flops': 4.1, 'type': 'CNN'},\n", "    'EfficientNet-B0': {'params': 5.3, 'flops': 0.39, 'type': 'CNN'},\n", "    'ViT-B/16': {'params': 86, 'flops': 17.6, 'type': 'Transformer'},\n", "    'DeiT-S': {'params': 22, 'flops': 4.6, 'type': 'Transformer'},\n", "    'BERT-base': {'params': 110, 'flops': 22, 'type': 'NLP'},\n", "    'DistilBERT': {'params': 66, 'flops': 11, 'type': 'NLP'},\n", "}\n", "\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Plot 1: Parameters vs FLOPs\n", "colors = {'CNN': '#3b82f6', 'Transformer': '#ef4444', 'NLP': '#22c55e'}\n", "for name, data in models.items():\n", "    axes[0].scatter(data['params'], data['flops'], \n", "                    c=colors[data['type']], s=100, label=data['type'])\n", "    axes[0].annotate(name, (data['params'], data['flops']), \n", "                     xytext=(5, 5), textcoords='offset points', fontsize=9)\n", "\n", "axes[0].set_xlabel('Parameters (M)', fontsize=12)\n", "axes[0].set_ylabel('FLOPs (G)', fontsize=12)\n", "axes[0].set_title('Parameters vs FLOPs', fontsize=14)\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "# Plot 2: Efficiency (FLOPs per Parameter)\n", "efficiency = {name: data['flops'] / data['params'] for name, data in models.items()}\n", "names = list(efficiency.keys())\n", "values = list(efficiency.values())\n", "bar_colors = [colors[models[n]['type']] for n in names]\n", "\n", "axes[1].barh(names, values, color=bar_colors)\n", "axes[1].set_xlabel('FLOPs per Parameter (G/M)', fontsize=12)\n", "axes[1].set_title('Compute Efficiency', fontsize=14)\n", "axes[1].grid(True, alpha=0.3, axis='x')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüí° Key Insight: Same accuracy can be achieved with 10x fewer FLOPs!')\n", "print('   EfficientNet-B0 vs ResNet-50: 10x fewer FLOPs, similar accuracy')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Memory Layout and Access Patterns"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Memory layout demonstration\n", "def analyze_memory_layout(tensor):\n", "    \"\"\"Analyze memory layout of a tensor\"\"\"\n", "    print(f'Shape: {tensor.shape}')\n", "    print(f'Stride: {tensor.stride()}')\n", "    print(f'Is contiguous: {tensor.is_contiguous()}')\n", "    print(f'Memory size: {tensor.element_size() * tensor.nelement() / 1024:.2f} KB')\n", "\n", "# Create example tensors\n", "x = torch.randn(32, 768, 512)  # Batch, Features, Sequence\n", "\n", "print('üìä MEMORY LAYOUT ANALYSIS')\n", "print('=' * 50)\n", "\n", "print('\\nOriginal tensor:')\n", "analyze_memory_layout(x)\n", "\n", "print('\\nTransposed tensor:')\n", "x_t = x.transpose(1, 2)\n", "analyze_memory_layout(x_t)\n", "\n", "print('\\nContiguous copy:')\n", "x_t_contig = x_t.contiguous()\n", "analyze_memory_layout(x_t_contig)\n", "\n", "# Benchmark\n", "print('\\n‚è±Ô∏è PERFORMANCE IMPACT')\n", "import time\n", "\n", "# Non-contiguous operation\n", "start = time.time()\n", "for _ in range(100):\n", "    _ = x_t.sum()\n", "non_contig_time = time.time() - start\n", "\n", "# Contiguous operation\n", "start = time.time()\n", "for _ in range(100):\n", "    _ = x_t_contig.sum()\n", "contig_time = time.time() - start\n", "\n", "print(f'Non-contiguous sum: {non_contig_time*1000:.2f}ms')\n", "print(f'Contiguous sum: {contig_time*1000:.2f}ms')\n", "print(f'Speedup: {non_contig_time/contig_time:.2f}x')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('üéØ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. FLOPs: Count multiply-adds, scales with layer dimensions')\n", "print('\\n2. Memory Bandwidth: Often the bottleneck, not compute')\n", "print('\\n3. Arithmetic Intensity: FLOPs/Byte - higher is better')\n", "print('\\n4. Roofline Model: Identifies compute vs memory bottleneck')\n", "print('\\n5. Memory Layout: Contiguous access is 2-10x faster')\n", "print('\\n' + '=' * 60)\n", "print('\\nüìö Next: Learn how to optimize with Pruning!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
