{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî¢ Lecture 5: Quantization - Complete Demo\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/05_quantization_1/demo.ipynb)\n",
        "\n",
        "## What You'll Learn\n",
        "- How quantization reduces model size (FP32 ‚Üí INT8 = 4x smaller)\n",
        "- Implementing quantization from scratch\n",
        "- Understanding scale and zero-point\n",
        "- PyTorch dynamic quantization\n",
        "- Measuring accuracy vs compression trade-off\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "!pip install torch torchvision matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cpu')  # Quantization works on CPU\n",
        "print(f\"üñ•Ô∏è Using device: {device}\")\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Quantization\n",
        "\n",
        "**Quantization** = Converting floating-point numbers to lower-precision integers\n",
        "\n",
        "| Data Type | Bits | Memory per value | Range |\n",
        "|-----------|------|------------------|-------|\n",
        "| FP32 | 32 | 4 bytes | ¬±3.4e38 |\n",
        "| FP16 | 16 | 2 bytes | ¬±65504 |\n",
        "| INT8 | 8 | 1 byte | -128 to 127 |\n",
        "| INT4 | 4 | 0.5 bytes | -8 to 7 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement quantization from scratch\n",
        "def quantize_tensor(x, num_bits=8):\n",
        "    \"\"\"\n",
        "    Quantize a floating-point tensor to integer.\n",
        "    \n",
        "    Quantization formula:\n",
        "        q = round(x / scale) + zero_point\n",
        "    \n",
        "    Dequantization formula:\n",
        "        x_approx = (q - zero_point) * scale\n",
        "    \"\"\"\n",
        "    qmin = 0\n",
        "    qmax = 2**num_bits - 1\n",
        "    \n",
        "    # Calculate scale and zero point\n",
        "    x_min, x_max = x.min().item(), x.max().item()\n",
        "    scale = (x_max - x_min) / (qmax - qmin)\n",
        "    zero_point = qmin - x_min / scale\n",
        "    \n",
        "    # Quantize\n",
        "    q = torch.clamp(torch.round(x / scale + zero_point), qmin, qmax)\n",
        "    \n",
        "    # Dequantize for comparison\n",
        "    x_dequant = (q - zero_point) * scale\n",
        "    \n",
        "    return q.to(torch.uint8), scale, zero_point, x_dequant\n",
        "\n",
        "# Demo with random tensor\n",
        "print(\"üìä Quantization Demo\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "x = torch.randn(3, 3) * 2  # Random values roughly in [-4, 4]\n",
        "print(f\"Original FP32 tensor:\\n{x}\\n\")\n",
        "\n",
        "q, scale, zp, x_recon = quantize_tensor(x, num_bits=8)\n",
        "print(f\"Quantized INT8 tensor:\\n{q}\\n\")\n",
        "print(f\"Scale: {scale:.6f}\")\n",
        "print(f\"Zero point: {zp:.1f}\")\n",
        "\n",
        "# Calculate error\n",
        "error = (x - x_recon).abs()\n",
        "print(f\"\\nReconstruction error:\")\n",
        "print(f\"  Mean absolute error: {error.mean():.6f}\")\n",
        "print(f\"  Max error: {error.max():.6f}\")\n",
        "\n",
        "# Memory comparison\n",
        "print(f\"\\nüíæ Memory savings:\")\n",
        "print(f\"  FP32: {x.numel() * 4} bytes\")\n",
        "print(f\"  INT8: {q.numel() * 1} bytes\")\n",
        "print(f\"  Compression: {x.numel() * 4 / (q.numel() * 1):.0f}x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize quantization at different bit widths\n",
        "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
        "\n",
        "# Original continuous signal\n",
        "x = torch.linspace(-2, 2, 1000)\n",
        "\n",
        "for i, bits in enumerate([8, 4, 2]):\n",
        "    ax = axes[0, i]\n",
        "    \n",
        "    # Quantize\n",
        "    q, scale, zp, x_recon = quantize_tensor(x, num_bits=bits)\n",
        "    \n",
        "    # Plot\n",
        "    ax.plot(x.numpy(), x.numpy(), 'b-', label='Original', alpha=0.5, linewidth=2)\n",
        "    ax.plot(x.numpy(), x_recon.numpy(), 'r-', label='Quantized', linewidth=1)\n",
        "    ax.set_title(f'{bits}-bit Quantization\\n({2**bits} levels)')\n",
        "    ax.set_xlabel('Original Value')\n",
        "    ax.set_ylabel('Quantized Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Error histogram\n",
        "    ax2 = axes[1, i]\n",
        "    error = (x - x_recon).numpy()\n",
        "    ax2.hist(error, bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
        "    ax2.set_title(f'{bits}-bit Error Distribution\\nMean: {np.abs(error).mean():.4f}')\n",
        "    ax2.set_xlabel('Quantization Error')\n",
        "    ax2.set_ylabel('Count')\n",
        "\n",
        "plt.suptitle('üîç Quantization at Different Bit Widths', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Quantizing a Real Neural Network\n",
        "\n",
        "Let's quantize an actual model and measure the impact on accuracy and size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train a simple model\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('./data', train=False, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1000)\n",
        "\n",
        "# Train\n",
        "model = SimpleMLP()\n",
        "\n",
        "def train_model(model, epochs=3):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += (pred == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "print(\"üèãÔ∏è Training model...\")\n",
        "model = train_model(model, epochs=3)\n",
        "original_acc = evaluate(model, test_loader)\n",
        "print(f\"‚úÖ Original FP32 accuracy: {original_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get model size\n",
        "def get_model_size_mb(model):\n",
        "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
        "    return (param_size + buffer_size) / 1024 / 1024\n",
        "\n",
        "# PyTorch Dynamic Quantization (Post-Training)\n",
        "print(\"üî¢ Applying Dynamic Quantization...\")\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,\n",
        "    {nn.Linear},  # Quantize Linear layers\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Evaluate quantized model\n",
        "quantized_acc = evaluate(quantized_model, test_loader)\n",
        "\n",
        "# Compare sizes\n",
        "original_size = get_model_size_mb(model)\n",
        "quantized_size = get_model_size_mb(quantized_model)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(f\"üìä QUANTIZATION RESULTS\")\n",
        "print(f\"=\"*50)\n",
        "print(f\"\\nüìê Model Size:\")\n",
        "print(f\"   Original (FP32):  {original_size:.2f} MB\")\n",
        "print(f\"   Quantized (INT8): {quantized_size:.2f} MB\")\n",
        "print(f\"   Compression:      {original_size/quantized_size:.1f}x smaller\")\n",
        "\n",
        "print(f\"\\nüéØ Accuracy:\")\n",
        "print(f\"   Original:  {original_acc:.2f}%\")\n",
        "print(f\"   Quantized: {quantized_acc:.2f}%\")\n",
        "print(f\"   Drop:      {original_acc - quantized_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Size comparison\n",
        "sizes = [original_size, quantized_size]\n",
        "labels = ['FP32\\n(Original)', 'INT8\\n(Quantized)']\n",
        "colors = ['#3b82f6', '#22c55e']\n",
        "axes[0].bar(labels, sizes, color=colors, edgecolor='black')\n",
        "axes[0].set_ylabel('Size (MB)')\n",
        "axes[0].set_title('üìê Model Size Comparison')\n",
        "for i, (s, l) in enumerate(zip(sizes, labels)):\n",
        "    axes[0].text(i, s + 0.01, f'{s:.2f} MB', ha='center', fontsize=11)\n",
        "\n",
        "# Accuracy comparison\n",
        "accs = [original_acc, quantized_acc]\n",
        "axes[1].bar(labels, accs, color=colors, edgecolor='black')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('üéØ Accuracy Comparison')\n",
        "axes[1].set_ylim([90, 100])\n",
        "for i, (a, l) in enumerate(zip(accs, labels)):\n",
        "    axes[1].text(i, a + 0.2, f'{a:.1f}%', ha='center', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéØ KEY TAKEAWAYS\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚Ä¢ Quantization: FP32 ‚Üí INT8 = 4x smaller model\")\n",
        "print(\"‚Ä¢ Post-Training Quantization (PTQ) is easy: one function call\")\n",
        "print(\"‚Ä¢ Minimal accuracy loss for most models (<1%)\")\n",
        "print(\"‚Ä¢ INT8 inference is 2-4x faster on supported hardware\")\n",
        "print(\"‚Ä¢ For even smaller: INT4 quantization (used in LLMs)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
