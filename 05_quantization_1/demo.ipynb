{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸ”¢ Lecture 5: Quantization Basics - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/05_quantization_1/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Data types: FP32, FP16, INT8, INT4\n", "- Quantization math: scale and zero-point\n", "- Symmetric vs asymmetric quantization\n", "- Post-Training Quantization (PTQ)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "print('Ready for quantization!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Understanding Data Types\n", "\n", "Different precision formats trade off range and accuracy for memory."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Data type comparison\n", "dtypes = {\n", "    'FP32': {'bits': 32, 'range': 'Â±3.4e38', 'precision': '~7 decimal digits'},\n", "    'FP16': {'bits': 16, 'range': 'Â±65504', 'precision': '~3 decimal digits'},\n", "    'BF16': {'bits': 16, 'range': 'Â±3.4e38', 'precision': '~2 decimal digits'},\n", "    'INT8': {'bits': 8, 'range': '-128 to 127', 'precision': 'integer only'},\n", "    'INT4': {'bits': 4, 'range': '-8 to 7', 'precision': 'integer only'},\n", "}\n", "\n", "print('ðŸ“Š DATA TYPE COMPARISON')\n", "print('=' * 70)\n", "print(f'{\"Type\":<10} {\"Bits\":<8} {\"Range\":<20} {\"Precision\":<25}')\n", "print('-' * 70)\n", "for name, info in dtypes.items():\n", "    print(f'{name:<10} {info[\"bits\"]:<8} {info[\"range\"]:<20} {info[\"precision\"]:<25}')\n", "\n", "# Memory savings visualization\n", "fig, ax = plt.subplots(figsize=(10, 5))\n", "types = list(dtypes.keys())\n", "bits = [dtypes[t]['bits'] for t in types]\n", "colors = ['#ef4444', '#f97316', '#eab308', '#22c55e', '#10b981']\n", "\n", "bars = ax.barh(types, bits, color=colors)\n", "ax.set_xlabel('Bits per Value', fontsize=12)\n", "ax.set_title('ðŸ“Š Memory per Value by Data Type', fontsize=14)\n", "\n", "for bar, b in zip(bars, bits):\n", "    compression = 32 / b\n", "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n", "            f'{compression:.1f}x compression vs FP32', va='center')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: The Math of Quantization\n", "\n", "**Quantization Formula:**\n", "$$Q(x) = \\text{round}\\left(\\frac{x}{s}\\right) + z$$\n", "\n", "**Dequantization:**\n", "$$\\hat{x} = s \\cdot (Q(x) - z)$$\n", "\n", "Where:\n", "- $s$ = scale factor\n", "- $z$ = zero-point"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def quantize_tensor(x, num_bits=8, symmetric=True):\n", "    \"\"\"\n", "    Quantize a tensor to specified bit width.\n", "    \n", "    Args:\n", "        x: Input tensor (FP32)\n", "        num_bits: Target bit width\n", "        symmetric: Use symmetric or asymmetric quantization\n", "    \n", "    Returns:\n", "        Quantized tensor, scale, zero_point\n", "    \"\"\"\n", "    if symmetric:\n", "        # Symmetric: zero-point = 0, range = [-max, max]\n", "        qmin = -(2 ** (num_bits - 1))\n", "        qmax = 2 ** (num_bits - 1) - 1\n", "        \n", "        max_val = x.abs().max()\n", "        scale = max_val / qmax\n", "        zero_point = 0\n", "    else:\n", "        # Asymmetric: full range utilization\n", "        qmin = 0\n", "        qmax = 2 ** num_bits - 1\n", "        \n", "        min_val, max_val = x.min(), x.max()\n", "        scale = (max_val - min_val) / (qmax - qmin)\n", "        zero_point = qmin - torch.round(min_val / scale)\n", "    \n", "    # Quantize\n", "    q = torch.clamp(torch.round(x / scale) + zero_point, qmin, qmax)\n", "    \n", "    return q.to(torch.int8 if num_bits == 8 else torch.int32), scale, zero_point\n", "\n", "def dequantize_tensor(q, scale, zero_point):\n", "    \"\"\"Dequantize back to FP32.\"\"\"\n", "    return scale * (q.float() - zero_point)\n", "\n", "# Example\n", "x = torch.randn(1000) * 2 + 0.5  # Non-zero mean tensor\n", "\n", "print('ðŸ“Š QUANTIZATION EXAMPLE')\n", "print('=' * 50)\n", "print(f'Original tensor: min={x.min():.4f}, max={x.max():.4f}')\n", "\n", "# Symmetric quantization\n", "q_sym, scale_sym, zp_sym = quantize_tensor(x, num_bits=8, symmetric=True)\n", "x_deq_sym = dequantize_tensor(q_sym, scale_sym, zp_sym)\n", "\n", "print(f'\\nðŸ”· Symmetric INT8:')\n", "print(f'   Scale: {scale_sym:.6f}')\n", "print(f'   Zero-point: {zp_sym}')\n", "print(f'   Reconstruction error: {torch.mean((x - x_deq_sym) ** 2):.6f}')\n", "\n", "# Asymmetric quantization\n", "q_asym, scale_asym, zp_asym = quantize_tensor(x, num_bits=8, symmetric=False)\n", "x_deq_asym = dequantize_tensor(q_asym, scale_asym, zp_asym)\n", "\n", "print(f'\\nðŸ”¶ Asymmetric INT8:')\n", "print(f'   Scale: {scale_asym:.6f}')\n", "print(f'   Zero-point: {zp_asym:.0f}')\n", "print(f'   Reconstruction error: {torch.mean((x - x_deq_asym) ** 2):.6f}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize quantization\n", "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n", "\n", "# Original distribution\n", "axes[0, 0].hist(x.numpy(), bins=50, color='#3b82f6', alpha=0.7, edgecolor='black')\n", "axes[0, 0].set_title('Original FP32 Values', fontsize=12)\n", "axes[0, 0].set_xlabel('Value')\n", "\n", "# Quantized values (symmetric)\n", "axes[0, 1].hist(q_sym.numpy(), bins=50, color='#22c55e', alpha=0.7, edgecolor='black')\n", "axes[0, 1].set_title('Quantized INT8 Values (Symmetric)', fontsize=12)\n", "axes[0, 1].set_xlabel('Quantized Value')\n", "\n", "# Reconstruction comparison\n", "indices = torch.argsort(x)[:100]\n", "axes[1, 0].plot(x[indices].numpy(), 'b-', label='Original', linewidth=2)\n", "axes[1, 0].plot(x_deq_sym[indices].numpy(), 'g--', label='Symmetric', linewidth=2)\n", "axes[1, 0].plot(x_deq_asym[indices].numpy(), 'r:', label='Asymmetric', linewidth=2)\n", "axes[1, 0].set_title('Original vs Reconstructed', fontsize=12)\n", "axes[1, 0].legend()\n", "axes[1, 0].set_xlabel('Sample Index')\n", "\n", "# Quantization error\n", "error_sym = (x - x_deq_sym).numpy()\n", "error_asym = (x - x_deq_asym).numpy()\n", "axes[1, 1].hist(error_sym, bins=50, alpha=0.5, label=f'Symmetric (MSE={np.mean(error_sym**2):.6f})', color='green')\n", "axes[1, 1].hist(error_asym, bins=50, alpha=0.5, label=f'Asymmetric (MSE={np.mean(error_asym**2):.6f})', color='red')\n", "axes[1, 1].set_title('Quantization Error Distribution', fontsize=12)\n", "axes[1, 1].legend()\n", "axes[1, 1].set_xlabel('Error')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Different Bit Widths"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare different bit widths\n", "bit_widths = [8, 4, 2]\n", "test_tensor = torch.randn(10000)\n", "\n", "print('ðŸ“Š BIT WIDTH COMPARISON')\n", "print('=' * 60)\n", "print(f'{\"Bits\":<8} {\"Levels\":<12} {\"MSE\":<15} {\"Max Error\":<15}')\n", "print('-' * 60)\n", "\n", "results = []\n", "for bits in bit_widths:\n", "    q, s, z = quantize_tensor(test_tensor, num_bits=bits, symmetric=True)\n", "    deq = dequantize_tensor(q, s, z)\n", "    \n", "    mse = torch.mean((test_tensor - deq) ** 2).item()\n", "    max_err = torch.max(torch.abs(test_tensor - deq)).item()\n", "    levels = 2 ** bits\n", "    \n", "    results.append({'bits': bits, 'levels': levels, 'mse': mse, 'max_err': max_err})\n", "    print(f'{bits:<8} {levels:<12} {mse:<15.6f} {max_err:<15.4f}')\n", "\n", "# Visualize\n", "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n", "\n", "for idx, bits in enumerate(bit_widths):\n", "    q, s, z = quantize_tensor(test_tensor[:100], num_bits=bits, symmetric=True)\n", "    deq = dequantize_tensor(q, s, z)\n", "    \n", "    axes[idx].plot(test_tensor[:100].numpy(), 'b-', alpha=0.7, label='Original')\n", "    axes[idx].plot(deq.numpy(), 'r--', alpha=0.7, label='Quantized')\n", "    axes[idx].set_title(f'{bits}-bit Quantization\\n({2**bits} levels)', fontsize=12)\n", "    axes[idx].legend()\n", "    axes[idx].set_xlabel('Sample')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ Lower bits = more error, but much smaller model!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Post-Training Quantization (PTQ)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a simple model\n", "class SimpleClassifier(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = nn.Linear(784, 256)\n", "        self.fc2 = nn.Linear(256, 64)\n", "        self.fc3 = nn.Linear(64, 10)\n", "        self.relu = nn.ReLU()\n", "    \n", "    def forward(self, x):\n", "        x = self.relu(self.fc1(x))\n", "        x = self.relu(self.fc2(x))\n", "        return self.fc3(x)\n", "\n", "# Simulate trained model\n", "model = SimpleClassifier()\n", "\n", "# Create test data\n", "X_test = torch.randn(1000, 784)\n", "y_test = torch.randint(0, 10, (1000,))\n", "\n", "def get_model_size(model):\n", "    \"\"\"Get model size in MB.\"\"\"\n", "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n", "    return param_size / 1024 / 1024\n", "\n", "def evaluate_model(model, X, y):\n", "    \"\"\"Evaluate model accuracy.\"\"\"\n", "    model.eval()\n", "    with torch.no_grad():\n", "        outputs = model(X)\n", "        _, predicted = outputs.max(1)\n", "        accuracy = (predicted == y).float().mean().item() * 100\n", "    return accuracy\n", "\n", "print('ðŸ“Š ORIGINAL MODEL')\n", "print('=' * 40)\n", "print(f'Size: {get_model_size(model):.2f} MB')\n", "print(f'Accuracy: {evaluate_model(model, X_test, y_test):.1f}%')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# PyTorch Dynamic Quantization\n", "print('\\nðŸ”§ PYTORCH DYNAMIC QUANTIZATION')\n", "print('=' * 50)\n", "\n", "# Apply dynamic quantization\n", "quantized_model = torch.quantization.quantize_dynamic(\n", "    model,\n", "    {nn.Linear},  # Layers to quantize\n", "    dtype=torch.qint8\n", ")\n", "\n", "print('\\nQuantized model structure:')\n", "print(quantized_model)\n", "\n", "# Compare\n", "orig_size = get_model_size(model)\n", "quant_acc = evaluate_model(quantized_model, X_test, y_test)\n", "\n", "print(f'\\nðŸ“Š COMPARISON')\n", "print(f'Original accuracy: {evaluate_model(model, X_test, y_test):.1f}%')\n", "print(f'Quantized accuracy: {quant_acc:.1f}%')\n", "print(f'\\nðŸ’¾ Model size comparison requires saving to disk...')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Calibration for Static Quantization"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def collect_activation_stats(model, calibration_data):\n", "    \"\"\"\n", "    Collect activation statistics for calibration.\n", "    This helps determine optimal scale and zero-point for activations.\n", "    \"\"\"\n", "    stats = {}\n", "    hooks = []\n", "    \n", "    def hook_fn(name):\n", "        def hook(module, input, output):\n", "            if name not in stats:\n", "                stats[name] = {'min': float('inf'), 'max': float('-inf'), 'values': []}\n", "            \n", "            out = output.detach()\n", "            stats[name]['min'] = min(stats[name]['min'], out.min().item())\n", "            stats[name]['max'] = max(stats[name]['max'], out.max().item())\n", "            stats[name]['values'].append(out.flatten()[:1000])  # Sample\n", "        return hook\n", "    \n", "    # Register hooks\n", "    for name, module in model.named_modules():\n", "        if isinstance(module, (nn.Linear, nn.ReLU)):\n", "            hooks.append(module.register_forward_hook(hook_fn(name)))\n", "    \n", "    # Run calibration data\n", "    model.eval()\n", "    with torch.no_grad():\n", "        for batch in calibration_data:\n", "            _ = model(batch)\n", "    \n", "    # Remove hooks\n", "    for hook in hooks:\n", "        hook.remove()\n", "    \n", "    return stats\n", "\n", "# Run calibration\n", "calibration_data = [torch.randn(32, 784) for _ in range(10)]\n", "activation_stats = collect_activation_stats(model, calibration_data)\n", "\n", "print('ðŸ“Š ACTIVATION STATISTICS FOR CALIBRATION')\n", "print('=' * 60)\n", "print(f'{\"Layer\":<20} {\"Min\":<15} {\"Max\":<15} {\"Range\":<15}')\n", "print('-' * 60)\n", "\n", "for name, stat in activation_stats.items():\n", "    range_val = stat['max'] - stat['min']\n", "    print(f'{name:<20} {stat[\"min\"]:<15.4f} {stat[\"max\"]:<15.4f} {range_val:<15.4f}')\n", "\n", "# Visualize activation distributions\n", "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n", "axes = axes.flatten()\n", "\n", "for idx, (name, stat) in enumerate(list(activation_stats.items())[:4]):\n", "    values = torch.cat(stat['values']).numpy()\n", "    axes[idx].hist(values, bins=50, color='#3b82f6', alpha=0.7, edgecolor='black')\n", "    axes[idx].axvline(x=0, color='red', linestyle='--')\n", "    axes[idx].set_title(f'{name}\\nRange: [{stat[\"min\"]:.2f}, {stat[\"max\"]:.2f}]')\n", "\n", "plt.suptitle('ðŸ“Š Activation Distributions (for Calibration)', fontsize=14)\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ Calibration data helps find optimal quantization parameters!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 6: Quantization Error Analysis"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def analyze_quantization_error(model, num_bits=8):\n", "    \"\"\"\n", "    Analyze quantization error per layer.\n", "    \"\"\"\n", "    errors = {}\n", "    \n", "    for name, param in model.named_parameters():\n", "        if 'weight' in name:\n", "            # Quantize and dequantize\n", "            q, s, z = quantize_tensor(param.data, num_bits=num_bits)\n", "            deq = dequantize_tensor(q, s, z)\n", "            \n", "            # Calculate errors\n", "            mse = torch.mean((param.data - deq) ** 2).item()\n", "            relative_error = (torch.abs(param.data - deq) / (torch.abs(param.data) + 1e-10)).mean().item()\n", "            \n", "            errors[name] = {\n", "                'mse': mse,\n", "                'relative_error': relative_error * 100,\n", "                'weight_range': (param.min().item(), param.max().item())\n", "            }\n", "    \n", "    return errors\n", "\n", "# Analyze errors at different bit widths\n", "print('ðŸ“Š PER-LAYER QUANTIZATION ERROR')\n", "print('=' * 70)\n", "\n", "for bits in [8, 4]:\n", "    print(f'\\n{bits}-bit Quantization:')\n", "    print(f'{\"Layer\":<15} {\"MSE\":<15} {\"Rel. Error (%)\":<15} {\"Weight Range\":<25}')\n", "    print('-' * 70)\n", "    \n", "    errors = analyze_quantization_error(model, num_bits=bits)\n", "    for name, err in errors.items():\n", "        short_name = name.split('.')[0]\n", "        w_range = f'[{err[\"weight_range\"][0]:.3f}, {err[\"weight_range\"][1]:.3f}]'\n", "        print(f'{short_name:<15} {err[\"mse\"]:<15.6f} {err[\"relative_error\"]:<15.2f} {w_range:<25}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('ðŸŽ¯ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. Quantization: FP32 â†’ INT8/INT4 (4-8x memory reduction)')\n", "print('\\n2. Scale and zero-point map floating point to integer range')\n", "print('\\n3. Symmetric: simpler, works well for weights')\n", "print('\\n4. Asymmetric: better range utilization for activations')\n", "print('\\n5. PTQ: Quick, no retraining, slight accuracy loss')\n", "print('\\n6. Calibration: Run sample data to find optimal parameters')\n", "print('\\n7. Lower bits = more compression but more error')\n", "print('\\n' + '=' * 60)\n", "print('\\nðŸ“š Next: Quantization-Aware Training for better accuracy!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
