{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üåê Lecture 14: Distributed Training - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/14_distributed_training/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Data parallelism vs model parallelism\n", "- ZeRO optimization stages\n", "- Pipeline parallelism\n", "- Communication overhead analysis"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "print('Ready for Distributed Training!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Why Distributed Training?\n", "\n", "Modern models are too large for single GPUs!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def model_vs_gpu_memory():\n", "    \"\"\"\n", "    Compare model training memory requirements vs GPU capacity.\n", "    \"\"\"\n", "    models = {\n", "        'GPT-2': 1.5,\n", "        'GPT-3': 175,\n", "        'LLaMA-7B': 7,\n", "        'LLaMA-70B': 70,\n", "        'GPT-4 (est.)': 1800,\n", "    }\n", "    \n", "    gpus = {\n", "        'RTX 4090': 24,\n", "        'A100-40GB': 40,\n", "        'A100-80GB': 80,\n", "        'H100-80GB': 80,\n", "    }\n", "    \n", "    print('üìä MODEL TRAINING MEMORY (FP16 + Adam)')\n", "    print('=' * 60)\n", "    print('\\nRule of thumb: Training memory ‚âà 18 √ó model size')\n", "    print('  (Weights + Gradients + Optimizer: 2 + 2 + 12 = 16 bytes/param)')\n", "    print('  (+ Activations: ~2 bytes/param)')\n", "    \n", "    print(f'\\n{\"Model\":<15} {\"Params (B)\":<12} {\"Training Mem (GB)\":<20} {\"GPUs Needed\":<15}')\n", "    print('-' * 60)\n", "    \n", "    for name, params_b in models.items():\n", "        train_mem = params_b * 18  # GB\n", "        gpus_needed = np.ceil(train_mem / 80)  # A100-80GB\n", "        print(f'{name:<15} {params_b:<12.1f} {train_mem:<20.0f} {gpus_needed:<15.0f}')\n", "    \n", "    return models, gpus\n", "\n", "models, gpus = model_vs_gpu_memory()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "\n", "model_names = list(models.keys())\n", "train_mems = [p * 18 for p in models.values()]\n", "\n", "bars = ax.bar(model_names, train_mems, color='#3b82f6')\n", "\n", "# GPU lines\n", "ax.axhline(y=24, color='#22c55e', linestyle='--', linewidth=2, label='RTX 4090 (24GB)')\n", "ax.axhline(y=80, color='#f59e0b', linestyle='--', linewidth=2, label='A100-80GB')\n", "ax.axhline(y=80*8, color='#ef4444', linestyle='--', linewidth=2, label='8√ó A100-80GB')\n", "\n", "ax.set_ylabel('Training Memory (GB)', fontsize=12)\n", "ax.set_title('üìä Model Training Memory vs GPU Capacity', fontsize=14)\n", "ax.set_yscale('log')\n", "ax.legend()\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "for bar, mem in zip(bars, train_mems):\n", "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.2,\n", "            f'{mem:.0f}GB', ha='center', fontsize=10)\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Parallelism Strategies"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def explain_parallelism():\n", "    \"\"\"\n", "    Explain different parallelism strategies.\n", "    \"\"\"\n", "    print('üìä PARALLELISM STRATEGIES')\n", "    print('=' * 70)\n", "    \n", "    strategies = {\n", "        'Data Parallelism (DDP)': {\n", "            'description': 'Replicate model on each GPU, split batch',\n", "            'memory': 'Full model per GPU',\n", "            'comm': 'All-reduce gradients',\n", "            'best_for': 'Models that fit in single GPU'\n", "        },\n", "        'Model Parallelism (MP)': {\n", "            'description': 'Split model layers across GPUs',\n", "            'memory': 'Fraction of model per GPU',\n", "            'comm': 'Activations between GPUs',\n", "            'best_for': 'Very large models'\n", "        },\n", "        'Pipeline Parallelism (PP)': {\n", "            'description': 'Split model stages, micro-batch pipeline',\n", "            'memory': 'Fraction of model + pipeline buffers',\n", "            'comm': 'Activations between stages',\n", "            'best_for': 'Deep models'\n", "        },\n", "        'Tensor Parallelism (TP)': {\n", "            'description': 'Split individual tensors across GPUs',\n", "            'memory': 'Fraction of each layer per GPU',\n", "            'comm': 'High - within each layer',\n", "            'best_for': 'Large layers (attention, FFN)'\n", "        },\n", "    }\n", "    \n", "    for name, info in strategies.items():\n", "        print(f'\\nüîπ {name}')\n", "        print(f'   Description: {info[\"description\"]}')\n", "        print(f'   Memory: {info[\"memory\"]}')\n", "        print(f'   Communication: {info[\"comm\"]}')\n", "        print(f'   Best for: {info[\"best_for\"]}')\n", "\n", "explain_parallelism()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize Data Parallelism\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Data Parallelism diagram\n", "ax = axes[0]\n", "ax.set_xlim(0, 10)\n", "ax.set_ylim(0, 10)\n", "ax.set_aspect('equal')\n", "\n", "# GPUs\n", "gpu_positions = [(1, 7), (4, 7), (7, 7)]\n", "for i, (x, y) in enumerate(gpu_positions):\n", "    rect = plt.Rectangle((x, y), 2, 2, fill=True, color='#3b82f6', alpha=0.8)\n", "    ax.add_patch(rect)\n", "    ax.text(x+1, y+1, f'GPU {i}\\nFull Model', ha='center', va='center', color='white', fontsize=9)\n", "\n", "# Data batches\n", "batch_positions = [(1, 3), (4, 3), (7, 3)]\n", "for i, (x, y) in enumerate(batch_positions):\n", "    rect = plt.Rectangle((x, y), 2, 1.5, fill=True, color='#22c55e', alpha=0.8)\n", "    ax.add_patch(rect)\n", "    ax.text(x+1, y+0.75, f'Batch {i}', ha='center', va='center', color='white', fontsize=9)\n", "\n", "# Arrows\n", "for (gx, gy), (bx, by) in zip(gpu_positions, batch_positions):\n", "    ax.annotate('', xy=(gx+1, gy), xytext=(bx+1, by+1.5),\n", "                arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n", "\n", "ax.set_title('Data Parallelism\\n(Each GPU has full model)', fontsize=12)\n", "ax.axis('off')\n", "\n", "# Model Parallelism diagram\n", "ax = axes[1]\n", "ax.set_xlim(0, 10)\n", "ax.set_ylim(0, 10)\n", "ax.set_aspect('equal')\n", "\n", "# GPUs with model parts\n", "gpu_positions = [(1, 7), (4, 7), (7, 7)]\n", "parts = ['Layers 1-4', 'Layers 5-8', 'Layers 9-12']\n", "for i, ((x, y), part) in enumerate(zip(gpu_positions, parts)):\n", "    rect = plt.Rectangle((x, y), 2, 2, fill=True, color='#ef4444', alpha=0.8)\n", "    ax.add_patch(rect)\n", "    ax.text(x+1, y+1, f'GPU {i}\\n{part}', ha='center', va='center', color='white', fontsize=9)\n", "\n", "# Arrows between GPUs\n", "for i in range(len(gpu_positions)-1):\n", "    x1, y1 = gpu_positions[i]\n", "    x2, y2 = gpu_positions[i+1]\n", "    ax.annotate('', xy=(x2, y1+1), xytext=(x1+2, y1+1),\n", "                arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n", "\n", "ax.set_title('Model Parallelism\\n(Model split across GPUs)', fontsize=12)\n", "ax.axis('off')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: ZeRO Optimization"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def zero_memory_analysis(model_params_b, num_gpus):\n", "    \"\"\"\n", "    Analyze memory per GPU for different ZeRO stages.\n", "    \n", "    ZeRO-1: Partition optimizer states\n", "    ZeRO-2: + Partition gradients\n", "    ZeRO-3: + Partition parameters\n", "    \"\"\"\n", "    # Memory breakdown (bytes per parameter)\n", "    # FP16 training: params=2, grads=2, optimizer=12 (Adam: m+v+master)\n", "    \n", "    param_bytes = 2  # FP16\n", "    grad_bytes = 2   # FP16\n", "    opt_bytes = 12   # FP32 master + momentum + variance\n", "    \n", "    params = model_params_b * 1e9\n", "    \n", "    results = {}\n", "    \n", "    # DDP (no partitioning)\n", "    ddp_mem = params * (param_bytes + grad_bytes + opt_bytes) / 1e9\n", "    results['DDP'] = ddp_mem\n", "    \n", "    # ZeRO-1: Partition optimizer\n", "    zero1_mem = params * (param_bytes + grad_bytes + opt_bytes / num_gpus) / 1e9\n", "    results['ZeRO-1'] = zero1_mem\n", "    \n", "    # ZeRO-2: + Partition gradients\n", "    zero2_mem = params * (param_bytes + (grad_bytes + opt_bytes) / num_gpus) / 1e9\n", "    results['ZeRO-2'] = zero2_mem\n", "    \n", "    # ZeRO-3: + Partition parameters\n", "    zero3_mem = params * (param_bytes + grad_bytes + opt_bytes) / num_gpus / 1e9\n", "    results['ZeRO-3'] = zero3_mem\n", "    \n", "    return results\n", "\n", "# Analyze for 7B model\n", "print('üìä ZERO OPTIMIZATION MEMORY (7B Model, 8 GPUs)')\n", "print('=' * 60)\n", "\n", "mems = zero_memory_analysis(model_params_b=7, num_gpus=8)\n", "\n", "print(f'{\"Stage\":<15} {\"Memory/GPU (GB)\":<20} {\"vs DDP\":<15}')\n", "print('-' * 50)\n", "\n", "ddp_mem = mems['DDP']\n", "for stage, mem in mems.items():\n", "    savings = f'{ddp_mem/mem:.1f}x' if stage != 'DDP' else '-'\n", "    print(f'{stage:<15} {mem:<20.1f} {savings:<15}')\n", "\n", "print('\\nüí° ZeRO-3 enables training models 8x larger than GPU memory!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize ZeRO stages\n", "fig, axes = plt.subplots(1, 4, figsize=(16, 5))\n", "\n", "stages = ['DDP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3']\n", "mems_7b = zero_memory_analysis(7, 8)\n", "\n", "# What's stored on each GPU\n", "components = ['Parameters', 'Gradients', 'Optimizer']\n", "ddp_vals = [14, 14, 84]  # 7B √ó 2, 2, 12 bytes\n", "\n", "stage_vals = {\n", "    'DDP': [14, 14, 84],\n", "    'ZeRO-1': [14, 14, 84/8],\n", "    'ZeRO-2': [14, 14/8, 84/8],\n", "    'ZeRO-3': [14/8, 14/8, 84/8],\n", "}\n", "\n", "colors = ['#3b82f6', '#22c55e', '#f59e0b']\n", "\n", "for ax, stage in zip(axes, stages):\n", "    vals = stage_vals[stage]\n", "    bars = ax.bar(components, vals, color=colors)\n", "    ax.set_ylabel('Memory (GB)')\n", "    ax.set_title(f'{stage}\\nTotal: {sum(vals):.0f} GB/GPU')\n", "    ax.set_ylim(0, 100)\n", "\n", "plt.suptitle('üìä Memory per GPU for 7B Model on 8 GPUs', fontsize=14)\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Communication Analysis"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def communication_analysis(model_params_b, num_gpus, bandwidth_gbps=400):\n", "    \"\"\"\n", "    Analyze communication overhead for different strategies.\n", "    \"\"\"\n", "    params = model_params_b * 1e9\n", "    param_bytes = 2  # FP16\n", "    \n", "    # Bytes to communicate per iteration\n", "    comm = {}\n", "    \n", "    # DDP: All-reduce gradients (2√ó params for ring all-reduce)\n", "    comm['DDP'] = 2 * params * param_bytes\n", "    \n", "    # ZeRO-1: Same as DDP (gradients)\n", "    comm['ZeRO-1'] = 2 * params * param_bytes\n", "    \n", "    # ZeRO-2: Reduce-scatter gradients\n", "    comm['ZeRO-2'] = params * param_bytes\n", "    \n", "    # ZeRO-3: + All-gather params (twice: forward + backward)\n", "    comm['ZeRO-3'] = params * param_bytes + 2 * params * param_bytes\n", "    \n", "    print('üìä COMMUNICATION OVERHEAD')\n", "    print('=' * 60)\n", "    print(f'Model: {model_params_b}B params, {num_gpus} GPUs, {bandwidth_gbps} Gbps')\n", "    print(f'\\n{\"Strategy\":<15} {\"Comm (GB)\":<15} {\"Time (ms)\":<15}')\n", "    print('-' * 45)\n", "    \n", "    for strategy, bytes_comm in comm.items():\n", "        gb = bytes_comm / 1e9\n", "        time_ms = (bytes_comm * 8 / bandwidth_gbps / 1e9) * 1000\n", "        print(f'{strategy:<15} {gb:<15.1f} {time_ms:<15.1f}')\n", "    \n", "    return comm\n", "\n", "comm = communication_analysis(7, 8)\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "strategies = list(comm.keys())\n", "comm_gb = [c / 1e9 for c in comm.values()]\n", "\n", "bars = ax.bar(strategies, comm_gb, color=['#3b82f6', '#22c55e', '#f59e0b', '#ef4444'])\n", "ax.set_ylabel('Communication per Step (GB)')\n", "ax.set_title('üìä Communication Overhead by Strategy')\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "for bar, gb in zip(bars, comm_gb):\n", "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n", "            f'{gb:.1f}GB', ha='center')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Choosing the Right Strategy"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def strategy_recommendation(model_params_b, gpu_memory_gb, num_gpus):\n", "    \"\"\"\n", "    Recommend parallelism strategy based on constraints.\n", "    \"\"\"\n", "    train_mem_per_model = model_params_b * 18  # GB\n", "    \n", "    print('üìä STRATEGY RECOMMENDATION')\n", "    print('=' * 60)\n", "    print(f'Model: {model_params_b}B params')\n", "    print(f'Training memory needed: {train_mem_per_model:.0f} GB')\n", "    print(f'Available: {num_gpus} √ó {gpu_memory_gb}GB = {num_gpus * gpu_memory_gb}GB')\n", "    \n", "    print('\\nüéØ RECOMMENDATIONS:')\n", "    \n", "    if train_mem_per_model <= gpu_memory_gb:\n", "        print('\\n‚úÖ Model fits in single GPU')\n", "        print('   ‚Üí Use DDP for fastest training')\n", "        \n", "    elif train_mem_per_model <= gpu_memory_gb * 1.5:\n", "        print('\\n‚ö†Ô∏è Model barely fits - tight on memory')\n", "        print('   ‚Üí Use ZeRO-1 or ZeRO-2')\n", "        \n", "    elif train_mem_per_model <= num_gpus * gpu_memory_gb:\n", "        print('\\n‚ö†Ô∏è Model needs memory sharding')\n", "        print('   ‚Üí Use ZeRO-3 (DeepSpeed/FSDP)')\n", "        \n", "    else:\n", "        print('\\n‚ùå Model too large even with ZeRO-3')\n", "        print('   ‚Üí Need more GPUs or model parallelism')\n", "        print(f'   ‚Üí Minimum GPUs needed: {int(np.ceil(train_mem_per_model / gpu_memory_gb))}')\n", "\n", "# Test different scenarios\n", "scenarios = [\n", "    (1.5, 24, 4),   # GPT-2 on 4√ó RTX 4090\n", "    (7, 80, 8),     # LLaMA-7B on 8√ó A100\n", "    (70, 80, 8),    # LLaMA-70B on 8√ó A100\n", "]\n", "\n", "for params, gpu_mem, n_gpus in scenarios:\n", "    print('\\n' + '='*60)\n", "    strategy_recommendation(params, gpu_mem, n_gpus)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('üéØ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. Large models need distributed training across GPUs')\n", "print('\\n2. Data Parallelism: Replicate model, split data')\n", "print('\\n3. ZeRO partitions memory across GPUs (1‚Üí2‚Üí3)')\n", "print('\\n4. ZeRO-3 enables models 8x larger than GPU memory')\n", "print('\\n5. More partitioning = more communication')\n", "print('\\n6. Use DDP when possible, ZeRO when needed')\n", "print('\\n' + '=' * 60)\n", "print('\\nüìö Next: Efficient Vision Models!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
