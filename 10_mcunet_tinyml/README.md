# Lecture 10: MCUNet & TinyML

[‚Üê Back to Course](../README.md) | [‚Üê Previous](../09_knowledge_distillation/README.md) | [Next: Efficient Transformers ‚Üí](../11_efficient_transformers/README.md)

üì∫ [Watch Lecture 10 on YouTube](https://www.youtube.com/watch?v=FVhz7kQ0xWU&list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB&index=10)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/10_mcunet_tinyml/demo.ipynb) ‚Üê **Try the code!**

---

## What is TinyML?

Running machine learning on **microcontrollers** (MCUs):

| Device | RAM | Flash | Compute |
|--------|-----|-------|---------|
| Server GPU | 80GB | TB | 312 TFLOPS |
| Smartphone | 6GB | 128GB | 10 TFLOPS |
| Raspberry Pi | 4GB | 32GB | 13 GFLOPS |
| **MCU (STM32)** | **320KB** | **1MB** | **0.1 GFLOPS** |

**MCUs have 1000x less memory than phones!**

---

## Why TinyML?

| Benefit | Why It Matters |
|---------|---------------|
| Privacy | Data never leaves device |
| Latency | No network round-trip |
| Cost | MCUs cost $1-5 |
| Power | Battery lasts months |
| Availability | Works offline |

---

## TinyML Challenges

1. **Memory** ‚Äî Model + activations must fit in KB
2. **No OS** ‚Äî Direct hardware access
3. **No floating point** ‚Äî Many MCUs only support INT
4. **Limited compute** ‚Äî 100MHz vs 3GHz

---

## MCUNet

Co-design network architecture AND inference engine for MCUs.

### Two-Stage Design

```
Stage 1: TinyNAS ‚Äî Find optimal architecture for target MCU
Stage 2: TinyEngine ‚Äî Efficient inference engine
```

---

## TinyNAS

Search for architectures that fit in MCU memory:

```python
# Constraint: peak memory < 320KB
for architecture in search_space:
    if peak_memory(architecture) > 320_000:
        skip  # Won't fit!
    else:
        evaluate(architecture)
```

### Memory-Optimized Search Space
- Depthwise separable convolutions
- Inverted bottleneck blocks
- Squeeze-and-excite (optional)
- Variable resolution (96-176 pixels)

---

## Peak Memory Optimization

Standard inference:
```
Layer1 ‚Üí [Activation1] ‚Üí Layer2 ‚Üí [Activation2] ‚Üí ...
Peak memory = max(Activation1, Activation2, ...)
```

TinyEngine optimizes layer order:
```
Reorder layers to minimize peak memory
Result: 4x memory reduction!
```

---

## Inference Scheduling

### Patch-Based Inference
Don't process entire feature map at once:

```
Instead of:
[Full 28x28 feature map] ‚Üí Conv ‚Üí [Full 28x28]

Do:
[7x7 patch] ‚Üí Conv ‚Üí [7x7] ‚Üí [7x7 patch] ‚Üí Conv ‚Üí [7x7] ‚Üí ...
```

Memory: 28√ó28 = 784 ‚Üí 7√ó7 = 49 (16x reduction!)

---

## MCUNet Results

| Model | Flash | SRAM | ImageNet Acc |
|-------|-------|------|--------------|
| MobileNetV2 (0.35x) | 1.8MB | 1.2MB | 49.7% |
| ProxylessNAS | 1.5MB | 0.7MB | 57.0% |
| **MCUNet** | **0.9MB** | **0.3MB** | **62.2%** |

**First time ImageNet on MCU!**

---

## TinyEngine Optimizations

| Optimization | Memory Saved |
|--------------|-------------|
| In-place depthwise | 2x |
| Loop tiling | 2-4x |
| Im2col-free conv | 2x |
| SIMD vectorization | - (speed) |

---

## Code: TinyML Workflow

```python
# 1. Design/search for tiny model
model = TinyNAS.search(
    target_device="STM32F746",
    sram_constraint=320_000,  # 320KB
    flash_constraint=1_000_000  # 1MB
)

# 2. Quantize to INT8
model_int8 = quantize(model, calibration_data)

# 3. Export to TinyEngine format
export_tinyengine(model_int8, "model.tflite")

# 4. Flash to MCU
flash_to_device("model.tflite", device="STM32F746")
```

---

## TinyML Applications

| Application | MCU | Power |
|-------------|-----|-------|
| Wake word detection | $1 MCU | 1mW |
| Gesture recognition | Arduino | 5mW |
| Anomaly detection | ESP32 | 10mW |
| Visual wake words | STM32 | 100mW |

---

## Memory Hierarchy

```
Registers: ~1KB, fastest
    ‚Üì
SRAM: 320KB, fast
    ‚Üì
Flash: 1MB, slow (code + weights)
    ‚Üì
External: SD card, very slow
```

**Goal: Keep activations in SRAM, weights in Flash**

---

## Key Papers

- üìÑ [MCUNet](https://arxiv.org/abs/2007.10319)
- üìÑ [MCUNetV2](https://arxiv.org/abs/2110.15352) - Patch-based inference
- üìÑ [TinyML Book](https://www.oreilly.com/library/view/tinyml/9781492052036/)

---

## Resources

- [TinyML Foundation](https://www.tinyml.org/)
- [TensorFlow Lite Micro](https://www.tensorflow.org/lite/microcontrollers)
- [Edge Impulse](https://www.edgeimpulse.com/)

