{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üî¨ Lecture 10: MCUNet & TinyML - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/10_mcunet_tinyml/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- TinyML constraints and challenges\n", "- Memory-efficient inference\n", "- MCUNet architecture design\n", "- Peak memory optimization"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "print('Ready for TinyML!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: TinyML Constraints\n", "\n", "Microcontrollers have extreme constraints:\n", "- **Flash**: 256KB - 2MB (model weights)\n", "- **SRAM**: 64KB - 512KB (activations)\n", "- **Compute**: No GPU, limited CPU"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# MCU specifications\n", "mcus = {\n", "    'STM32F4': {'flash_kb': 512, 'sram_kb': 128, 'mhz': 180, 'use': 'Wearables'},\n", "    'STM32H7': {'flash_kb': 2048, 'sram_kb': 1024, 'mhz': 480, 'use': 'Industrial'},\n", "    'nRF52840': {'flash_kb': 1024, 'sram_kb': 256, 'mhz': 64, 'use': 'IoT'},\n", "    'ESP32': {'flash_kb': 4096, 'sram_kb': 520, 'mhz': 240, 'use': 'Smart Home'},\n", "    'Cortex-M0': {'flash_kb': 256, 'sram_kb': 32, 'mhz': 48, 'use': 'Sensors'},\n", "}\n", "\n", "# Compare with typical ML models\n", "models = {\n", "    'MobileNetV2': {'params_mb': 14, 'peak_act_mb': 40},\n", "    'ResNet-18': {'params_mb': 46, 'peak_act_mb': 100},\n", "    'BERT-tiny': {'params_mb': 17, 'peak_act_mb': 50},\n", "    'MCUNet': {'params_mb': 0.74, 'peak_act_mb': 0.39},\n", "}\n", "\n", "print('üìä MCU MEMORY CONSTRAINTS')\n", "print('=' * 70)\n", "print(f'{\"MCU\":<15} {\"Flash\":<12} {\"SRAM\":<12} {\"MHz\":<10} {\"Use Case\":<20}')\n", "print('-' * 70)\n", "for name, spec in mcus.items():\n", "    print(f'{name:<15} {spec[\"flash_kb\"]:>8} KB  {spec[\"sram_kb\"]:>8} KB  {spec[\"mhz\"]:>5}  {spec[\"use\"]:<20}')\n", "\n", "print(f'\\nüìä TYPICAL MODEL REQUIREMENTS')\n", "print('=' * 50)\n", "print(f'{\"Model\":<15} {\"Params (MB)\":<15} {\"Peak Memory (MB)\":<20}')\n", "print('-' * 50)\n", "for name, req in models.items():\n", "    print(f'{name:<15} {req[\"params_mb\"]:>10.2f}    {req[\"peak_act_mb\"]:>15.2f}')\n", "\n", "print('\\n‚ö†Ô∏è Most models are 100x too big for MCUs!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize the gap\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Model sizes vs MCU Flash\n", "model_names = list(models.keys())\n", "model_params = [models[m]['params_mb'] * 1024 for m in model_names]  # Convert to KB\n", "mcu_flash = [mcus[m]['flash_kb'] for m in mcus.keys()]\n", "\n", "axes[0].bar(model_names, model_params, color='#ef4444', alpha=0.8, label='Model Size')\n", "axes[0].axhline(y=np.mean(mcu_flash), color='#22c55e', linestyle='--', \n", "               linewidth=2, label=f'Avg MCU Flash ({np.mean(mcu_flash):.0f} KB)')\n", "axes[0].set_ylabel('Size (KB)')\n", "axes[0].set_title('Model Size vs MCU Flash')\n", "axes[0].legend()\n", "axes[0].set_yscale('log')\n", "\n", "# Peak memory vs MCU SRAM\n", "model_peak = [models[m]['peak_act_mb'] * 1024 for m in model_names]  # Convert to KB\n", "mcu_sram = [mcus[m]['sram_kb'] for m in mcus.keys()]\n", "\n", "axes[1].bar(model_names, model_peak, color='#ef4444', alpha=0.8, label='Peak Memory')\n", "axes[1].axhline(y=np.mean(mcu_sram), color='#22c55e', linestyle='--', \n", "               linewidth=2, label=f'Avg MCU SRAM ({np.mean(mcu_sram):.0f} KB)')\n", "axes[1].set_ylabel('Memory (KB)')\n", "axes[1].set_title('Peak Activation Memory vs MCU SRAM')\n", "axes[1].legend()\n", "axes[1].set_yscale('log')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüí° MCUNet bridges this gap!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Understanding Peak Memory"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calculate_layer_memory(layer, input_size, dtype_bytes=1):\n", "    \"\"\"\n", "    Calculate memory for a single layer.\n", "    \n", "    Returns: (input_mem, output_mem, weight_mem)\n", "    \"\"\"\n", "    batch, in_ch, h, w = input_size\n", "    \n", "    if isinstance(layer, nn.Conv2d):\n", "        out_ch = layer.out_channels\n", "        # Output size (assuming padding maintains size)\n", "        out_h, out_w = h, w\n", "        if layer.stride[0] > 1:\n", "            out_h, out_w = h // layer.stride[0], w // layer.stride[1]\n", "        \n", "        input_mem = batch * in_ch * h * w * dtype_bytes\n", "        output_mem = batch * out_ch * out_h * out_w * dtype_bytes\n", "        weight_mem = layer.weight.numel() * dtype_bytes\n", "        \n", "        return input_mem, output_mem, weight_mem, (batch, out_ch, out_h, out_w)\n", "    \n", "    return 0, 0, 0, input_size\n", "\n", "class TinyNet(nn.Module):\n", "    \"\"\"Example tiny network.\"\"\"\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n", "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n", "        self.conv3 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n", "        self.conv4 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n", "        self.gap = nn.AdaptiveAvgPool2d(1)\n", "        self.fc = nn.Linear(128, 10)\n", "    \n", "    def forward(self, x):\n", "        x = F.relu(self.conv1(x))\n", "        x = F.relu(self.conv2(x))\n", "        x = F.relu(self.conv3(x))\n", "        x = F.relu(self.conv4(x))\n", "        x = self.gap(x).flatten(1)\n", "        return self.fc(x)\n", "\n", "# Analyze memory usage\n", "model = TinyNet()\n", "input_size = (1, 3, 128, 128)\n", "\n", "print('üìä LAYER-BY-LAYER MEMORY ANALYSIS')\n", "print('=' * 70)\n", "print(f'{\"Layer\":<15} {\"Input (KB)\":<15} {\"Output (KB)\":<15} {\"Weights (KB)\":<15}')\n", "print('-' * 70)\n", "\n", "current_size = input_size\n", "peak_memory = 0\n", "memory_timeline = []\n", "\n", "for name, layer in model.named_modules():\n", "    if isinstance(layer, nn.Conv2d):\n", "        in_mem, out_mem, w_mem, new_size = calculate_layer_memory(layer, current_size)\n", "        \n", "        # Peak memory = input + output (need both during computation)\n", "        layer_peak = (in_mem + out_mem) / 1024  # KB\n", "        peak_memory = max(peak_memory, layer_peak)\n", "        memory_timeline.append({'name': name, 'peak': layer_peak, 'in': in_mem/1024, 'out': out_mem/1024})\n", "        \n", "        print(f'{name:<15} {in_mem/1024:>12.2f}   {out_mem/1024:>12.2f}   {w_mem/1024:>12.2f}')\n", "        current_size = new_size\n", "\n", "print(f'\\nüî∫ Peak activation memory: {peak_memory:.2f} KB')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize memory timeline\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "\n", "layers = [m['name'] for m in memory_timeline]\n", "peaks = [m['peak'] for m in memory_timeline]\n", "inputs = [m['in'] for m in memory_timeline]\n", "outputs = [m['out'] for m in memory_timeline]\n", "\n", "x = np.arange(len(layers))\n", "width = 0.35\n", "\n", "ax.bar(x - width/2, inputs, width, label='Input', color='#3b82f6')\n", "ax.bar(x + width/2, outputs, width, label='Output', color='#22c55e')\n", "ax.plot(x, peaks, 'ro-', linewidth=2, markersize=10, label='Peak (In+Out)')\n", "\n", "ax.axhline(y=128, color='red', linestyle='--', label='STM32F4 SRAM (128KB)')\n", "\n", "ax.set_xlabel('Layer')\n", "ax.set_ylabel('Memory (KB)')\n", "ax.set_title('üìä Memory Usage Through Network')\n", "ax.set_xticks(x)\n", "ax.set_xticklabels(layers)\n", "ax.legend()\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüí° Peak memory occurs at early layers (large feature maps)!')\n", "print('   MCUNet optimizes network width at each stage.')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: MCUNet Design Principles"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MCUNetBlock(nn.Module):\n", "    \"\"\"\n", "    MCUNet uses inverted residual blocks with optimized expansion ratios.\n", "    Key: Different expansion ratios at different resolutions.\n", "    \"\"\"\n", "    def __init__(self, in_ch, out_ch, stride=1, expand_ratio=3):\n", "        super().__init__()\n", "        hidden_ch = in_ch * expand_ratio\n", "        \n", "        self.use_residual = (stride == 1 and in_ch == out_ch)\n", "        \n", "        layers = []\n", "        # Expand\n", "        if expand_ratio != 1:\n", "            layers.extend([\n", "                nn.Conv2d(in_ch, hidden_ch, 1, bias=False),\n", "                nn.BatchNorm2d(hidden_ch),\n", "                nn.ReLU6()\n", "            ])\n", "        \n", "        # Depthwise\n", "        layers.extend([\n", "            nn.Conv2d(hidden_ch, hidden_ch, 3, stride, 1, groups=hidden_ch, bias=False),\n", "            nn.BatchNorm2d(hidden_ch),\n", "            nn.ReLU6()\n", "        ])\n", "        \n", "        # Project\n", "        layers.extend([\n", "            nn.Conv2d(hidden_ch, out_ch, 1, bias=False),\n", "            nn.BatchNorm2d(out_ch)\n", "        ])\n", "        \n", "        self.conv = nn.Sequential(*layers)\n", "    \n", "    def forward(self, x):\n", "        if self.use_residual:\n", "            return x + self.conv(x)\n", "        return self.conv(x)\n", "\n", "class MCUNet(nn.Module):\n", "    \"\"\"\n", "    MCUNet-style architecture optimized for MCU deployment.\n", "    \n", "    Key optimizations:\n", "    1. Lower expansion ratios at high resolutions (save memory)\n", "    2. Higher expansion ratios at low resolutions (more capacity)\n", "    3. Aggressive downsampling early\n", "    \"\"\"\n", "    def __init__(self, num_classes=10):\n", "        super().__init__()\n", "        \n", "        # Stem - reduce resolution quickly\n", "        self.stem = nn.Sequential(\n", "            nn.Conv2d(3, 16, 3, stride=2, padding=1, bias=False),\n", "            nn.BatchNorm2d(16),\n", "            nn.ReLU6()\n", "        )\n", "        \n", "        # Stage 1: High resolution, low expansion\n", "        self.stage1 = nn.Sequential(\n", "            MCUNetBlock(16, 16, stride=1, expand_ratio=1),\n", "            MCUNetBlock(16, 24, stride=2, expand_ratio=2),\n", "        )\n", "        \n", "        # Stage 2: Medium resolution, medium expansion\n", "        self.stage2 = nn.Sequential(\n", "            MCUNetBlock(24, 24, stride=1, expand_ratio=3),\n", "            MCUNetBlock(24, 40, stride=2, expand_ratio=3),\n", "        )\n", "        \n", "        # Stage 3: Low resolution, high expansion\n", "        self.stage3 = nn.Sequential(\n", "            MCUNetBlock(40, 40, stride=1, expand_ratio=4),\n", "            MCUNetBlock(40, 80, stride=2, expand_ratio=4),\n", "        )\n", "        \n", "        # Head\n", "        self.head = nn.Sequential(\n", "            nn.Conv2d(80, 160, 1, bias=False),\n", "            nn.BatchNorm2d(160),\n", "            nn.ReLU6(),\n", "            nn.AdaptiveAvgPool2d(1)\n", "        )\n", "        \n", "        self.fc = nn.Linear(160, num_classes)\n", "    \n", "    def forward(self, x):\n", "        x = self.stem(x)\n", "        x = self.stage1(x)\n", "        x = self.stage2(x)\n", "        x = self.stage3(x)\n", "        x = self.head(x).flatten(1)\n", "        return self.fc(x)\n", "\n", "# Analyze MCUNet\n", "mcunet = MCUNet()\n", "\n", "# Count parameters\n", "params = sum(p.numel() for p in mcunet.parameters())\n", "params_kb = params * 1 / 1024  # INT8\n", "\n", "print('üìä MCUNET ARCHITECTURE')\n", "print('=' * 50)\n", "print(f'Parameters: {params:,}')\n", "print(f'Model size (INT8): {params_kb:.2f} KB')\n", "print(f'\\nFits in {params_kb:.0f} KB Flash! ‚úÖ')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare memory usage: Regular vs MCUNet design\n", "def estimate_peak_memory(model, input_size, dtype_bytes=1):\n", "    \"\"\"Estimate peak memory through the network.\"\"\"\n", "    x = torch.randn(input_size)\n", "    peak = 0\n", "    \n", "    # Hook to track activations\n", "    activations = []\n", "    \n", "    def hook(module, input, output):\n", "        activations.append(output.numel() * dtype_bytes / 1024)  # KB\n", "    \n", "    hooks = []\n", "    for module in model.modules():\n", "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n", "            hooks.append(module.register_forward_hook(hook))\n", "    \n", "    with torch.no_grad():\n", "        _ = model(x)\n", "    \n", "    # Remove hooks\n", "    for h in hooks:\n", "        h.remove()\n", "    \n", "    # Peak is max of consecutive sums\n", "    for i in range(len(activations) - 1):\n", "        peak = max(peak, activations[i] + activations[i+1])\n", "    \n", "    return peak, activations\n", "\n", "# Compare\n", "print('üìä PEAK MEMORY COMPARISON')\n", "print('=' * 50)\n", "\n", "input_size = (1, 3, 128, 128)\n", "\n", "peak_tiny, acts_tiny = estimate_peak_memory(TinyNet(), input_size)\n", "peak_mcu, acts_mcu = estimate_peak_memory(mcunet, input_size)\n", "\n", "print(f'TinyNet (naive):     {peak_tiny:.2f} KB')\n", "print(f'MCUNet (optimized):  {peak_mcu:.2f} KB')\n", "print(f'Reduction:           {peak_tiny/peak_mcu:.1f}x')\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(12, 5))\n", "\n", "ax.plot(acts_tiny, 'o-', label='TinyNet', color='#ef4444', linewidth=2)\n", "ax.plot(acts_mcu, 's-', label='MCUNet', color='#22c55e', linewidth=2)\n", "ax.axhline(y=128, color='gray', linestyle='--', label='128KB SRAM limit')\n", "\n", "ax.set_xlabel('Layer')\n", "ax.set_ylabel('Activation Memory (KB)')\n", "ax.set_title('üìä Activation Memory Through Network')\n", "ax.legend()\n", "ax.grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: TinyNAS - Joint Network-Memory Optimization"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tinynas_search(target_flash_kb, target_sram_kb, num_trials=20):\n", "    \"\"\"\n", "    Simplified TinyNAS: Search for architecture that fits MCU constraints.\n", "    \n", "    Search over:\n", "    - Network width (channels)\n", "    - Expansion ratios\n", "    - Number of blocks\n", "    \"\"\"\n", "    best_config = None\n", "    best_acc = 0\n", "    \n", "    results = []\n", "    \n", "    for trial in range(num_trials):\n", "        # Sample architecture config\n", "        base_width = np.random.choice([8, 12, 16, 20, 24])\n", "        expand_ratios = [np.random.choice([1, 2, 3, 4]) for _ in range(3)]\n", "        num_blocks = [np.random.choice([1, 2, 3]) for _ in range(3)]\n", "        \n", "        # Estimate size\n", "        params = base_width * 100 + sum(expand_ratios) * 50  # Simplified estimate\n", "        peak_mem = base_width * 10 + sum(expand_ratios) * 5  # Simplified estimate\n", "        \n", "        # Check constraints\n", "        if params <= target_flash_kb and peak_mem <= target_sram_kb:\n", "            # Simulate accuracy (in real NAS, train and evaluate)\n", "            acc = 50 + params / 10 + np.random.randn() * 2\n", "            \n", "            results.append({\n", "                'config': {'width': base_width, 'expand': expand_ratios, 'blocks': num_blocks},\n", "                'params': params,\n", "                'peak_mem': peak_mem,\n", "                'acc': acc\n", "            })\n", "            \n", "            if acc > best_acc:\n", "                best_acc = acc\n", "                best_config = results[-1]\n", "    \n", "    return best_config, results\n", "\n", "print('üîç TINYNAS SEARCH')\n", "print('=' * 50)\n", "print('Target: Flash ‚â§ 256KB, SRAM ‚â§ 128KB')\n", "\n", "best, all_results = tinynas_search(target_flash_kb=256, target_sram_kb=128, num_trials=50)\n", "\n", "print(f'\\nüèÜ Best Architecture Found:')\n", "print(f'   Config: {best[\"config\"]}')\n", "print(f'   Model size: {best[\"params\"]:.0f} KB')\n", "print(f'   Peak memory: {best[\"peak_mem\"]:.0f} KB')\n", "print(f'   Accuracy: {best[\"acc\"]:.1f}%')\n", "\n", "# Visualize search results\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "params = [r['params'] for r in all_results]\n", "peak_mems = [r['peak_mem'] for r in all_results]\n", "accs = [r['acc'] for r in all_results]\n", "\n", "scatter = ax.scatter(params, peak_mems, c=accs, cmap='RdYlGn', s=100, alpha=0.7)\n", "ax.scatter(best['params'], best['peak_mem'], c='red', s=300, marker='*', label='Best')\n", "\n", "# Constraint box\n", "ax.axvline(x=256, color='red', linestyle='--', alpha=0.5)\n", "ax.axhline(y=128, color='red', linestyle='--', alpha=0.5)\n", "ax.fill_between([0, 256], [0, 0], [128, 128], alpha=0.1, color='green', label='Feasible')\n", "\n", "ax.set_xlabel('Model Size (KB)')\n", "ax.set_ylabel('Peak Memory (KB)')\n", "ax.set_title('üìä TinyNAS Search Space')\n", "plt.colorbar(scatter, label='Accuracy (%)')\n", "ax.legend()\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('üéØ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. MCUs have KB-level memory (1000x less than GPUs)')\n", "print('\\n2. Peak memory = max(input + output) across layers')\n", "print('\\n3. Early layers (high resolution) need low expansion')\n", "print('\\n4. MCUNet optimizes width/expansion per resolution')\n", "print('\\n5. TinyNAS jointly optimizes network and memory')\n", "print('\\n6. Result: ImageNet models in 1MB Flash + 256KB SRAM!')\n", "print('\\n' + '=' * 60)\n", "print('\\nüìö Next: Efficient Transformers!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
