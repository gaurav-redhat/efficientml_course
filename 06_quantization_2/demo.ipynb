{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üéØ Lecture 6: Quantization-Aware Training (QAT) - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/06_quantization_2/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Why QAT beats Post-Training Quantization\n", "- Straight-Through Estimator (STE) for gradients\n", "- Fake quantization during training\n", "- Mixed-precision quantization"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "print('Ready for QAT!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: The Problem with PTQ\n", "\n", "Post-Training Quantization has limitations, especially at low bit-widths."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Demonstrate PTQ accuracy degradation\n", "class SimpleNet(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = nn.Linear(784, 256)\n", "        self.fc2 = nn.Linear(256, 64)\n", "        self.fc3 = nn.Linear(64, 10)\n", "    \n", "    def forward(self, x):\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        return self.fc3(x)\n", "\n", "def quantize_weight(w, num_bits):\n", "    \"\"\"Simple symmetric quantization.\"\"\"\n", "    qmax = 2 ** (num_bits - 1) - 1\n", "    scale = w.abs().max() / qmax\n", "    q = torch.round(w / scale).clamp(-qmax-1, qmax)\n", "    return q * scale  # Dequantized\n", "\n", "def apply_ptq(model, num_bits):\n", "    \"\"\"Apply PTQ to all weights.\"\"\"\n", "    model_ptq = type(model)()\n", "    model_ptq.load_state_dict(model.state_dict())\n", "    \n", "    with torch.no_grad():\n", "        for name, param in model_ptq.named_parameters():\n", "            if 'weight' in name:\n", "                param.data = quantize_weight(param.data, num_bits)\n", "    \n", "    return model_ptq\n", "\n", "# Create and \"train\" model\n", "model = SimpleNet()\n", "X_test = torch.randn(1000, 784)\n", "y_test = torch.randint(0, 10, (1000,))\n", "\n", "# Quick training\n", "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n", "criterion = nn.CrossEntropyLoss()\n", "for _ in range(20):\n", "    optimizer.zero_grad()\n", "    loss = criterion(model(X_test), y_test)\n", "    loss.backward()\n", "    optimizer.step()\n", "\n", "def evaluate(model, X, y):\n", "    model.eval()\n", "    with torch.no_grad():\n", "        return (model(X).argmax(1) == y).float().mean().item() * 100\n", "\n", "# Test PTQ at different bit widths\n", "print('üìä PTQ ACCURACY DEGRADATION')\n", "print('=' * 50)\n", "baseline = evaluate(model, X_test, y_test)\n", "print(f'FP32 Baseline: {baseline:.1f}%')\n", "\n", "ptq_results = []\n", "for bits in [8, 6, 4, 3, 2]:\n", "    model_ptq = apply_ptq(model, bits)\n", "    acc = evaluate(model_ptq, X_test, y_test)\n", "    ptq_results.append((bits, acc))\n", "    print(f'{bits}-bit PTQ: {acc:.1f}% (drop: {baseline-acc:.1f}%)')\n", "\n", "print('\\n‚ö†Ô∏è Notice: Accuracy drops significantly at low bit widths!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Straight-Through Estimator (STE)\n", "\n", "The key to QAT: Approximate gradients through quantization.\n", "\n", "**Problem**: `round()` has zero gradient almost everywhere\n", "\n", "**Solution**: STE - In forward pass: quantize. In backward pass: pass gradient through"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class StraightThroughEstimator(torch.autograd.Function):\n", "    \"\"\"\n", "    Straight-Through Estimator for quantization.\n", "    \n", "    Forward: Quantize (round)\n", "    Backward: Pass gradient through (identity)\n", "    \"\"\"\n", "    @staticmethod\n", "    def forward(ctx, x, num_bits):\n", "        qmax = 2 ** (num_bits - 1) - 1\n", "        scale = x.abs().max() / qmax + 1e-8\n", "        \n", "        # Quantize\n", "        q = torch.round(x / scale).clamp(-qmax-1, qmax)\n", "        x_q = q * scale\n", "        \n", "        ctx.save_for_backward(x, torch.tensor([qmax], dtype=torch.float32))\n", "        return x_q\n", "    \n", "    @staticmethod\n", "    def backward(ctx, grad_output):\n", "        x, qmax_tensor = ctx.saved_tensors\n", "        qmax = qmax_tensor.item()\n", "        scale = x.abs().max() / qmax + 1e-8\n", "        \n", "        # Gradient clipping (optional): zero gradient outside quantization range\n", "        mask = (x.abs() / scale <= qmax).float()\n", "        \n", "        # STE: pass gradient through\n", "        return grad_output * mask, None\n", "\n", "fake_quantize = StraightThroughEstimator.apply\n", "\n", "# Demonstrate STE\n", "x = torch.randn(100, requires_grad=True)\n", "\n", "# Forward: quantized\n", "x_q = fake_quantize(x, 4)\n", "\n", "# Backward: gradient flows!\n", "loss = x_q.sum()\n", "loss.backward()\n", "\n", "print('üìä STRAIGHT-THROUGH ESTIMATOR DEMO')\n", "print('=' * 50)\n", "print(f'Input range: [{x.min():.3f}, {x.max():.3f}]')\n", "print(f'Quantized output has {len(torch.unique(x_q))} unique values')\n", "print(f'Gradient is non-zero: {(x.grad != 0).sum().item()} / {x.numel()}')\n", "\n", "# Visualize\n", "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n", "\n", "# Original vs Quantized\n", "axes[0].scatter(x.detach().numpy(), x_q.detach().numpy(), alpha=0.5)\n", "axes[0].plot([-3, 3], [-3, 3], 'r--', label='y=x')\n", "axes[0].set_xlabel('Original')\n", "axes[0].set_ylabel('Quantized')\n", "axes[0].set_title('Forward: Quantization')\n", "axes[0].legend()\n", "\n", "# Gradient\n", "axes[1].scatter(x.detach().numpy(), x.grad.numpy(), alpha=0.5, c='green')\n", "axes[1].axhline(y=1, color='r', linestyle='--', label='STE: grad=1')\n", "axes[1].set_xlabel('Input Value')\n", "axes[1].set_ylabel('Gradient')\n", "axes[1].set_title('Backward: STE Gradient')\n", "axes[1].legend()\n", "\n", "# Compare with true gradient (which would be 0)\n", "axes[2].bar(['Round (true)', 'STE'], [0, 1], color=['red', 'green'])\n", "axes[2].set_ylabel('Gradient Flow')\n", "axes[2].set_title('Why STE Works')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Implementing QAT from Scratch"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class QATLinear(nn.Module):\n", "    \"\"\"\n", "    Linear layer with Quantization-Aware Training.\n", "    Uses fake quantization during training.\n", "    \"\"\"\n", "    def __init__(self, in_features, out_features, weight_bits=8, act_bits=8):\n", "        super().__init__()\n", "        self.in_features = in_features\n", "        self.out_features = out_features\n", "        self.weight_bits = weight_bits\n", "        self.act_bits = act_bits\n", "        \n", "        # Full precision weights\n", "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01)\n", "        self.bias = nn.Parameter(torch.zeros(out_features))\n", "    \n", "    def forward(self, x):\n", "        # Fake quantize weights\n", "        w_q = fake_quantize(self.weight, self.weight_bits)\n", "        \n", "        # Fake quantize activations\n", "        x_q = fake_quantize(x, self.act_bits)\n", "        \n", "        # Compute with quantized values\n", "        return F.linear(x_q, w_q, self.bias)\n", "\n", "class QATNet(nn.Module):\n", "    \"\"\"Network with QAT layers.\"\"\"\n", "    def __init__(self, weight_bits=8, act_bits=8):\n", "        super().__init__()\n", "        self.fc1 = QATLinear(784, 256, weight_bits, act_bits)\n", "        self.fc2 = QATLinear(256, 64, weight_bits, act_bits)\n", "        self.fc3 = QATLinear(64, 10, weight_bits, act_bits)\n", "    \n", "    def forward(self, x):\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        return self.fc3(x)\n", "\n", "print('‚úÖ QAT Network defined!')\n", "print('\\nüìä QAT Layer Details:')\n", "print('- Forward: Apply fake quantization to weights and activations')\n", "print('- Backward: Use STE to pass gradients through quantization')\n", "print('- Result: Network learns to be robust to quantization noise')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train QAT model vs PTQ model\n", "def train_model(model, X, y, epochs=50, lr=0.001):\n", "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n", "    criterion = nn.CrossEntropyLoss()\n", "    losses = []\n", "    \n", "    for epoch in range(epochs):\n", "        model.train()\n", "        optimizer.zero_grad()\n", "        output = model(X)\n", "        loss = criterion(output, y)\n", "        loss.backward()\n", "        optimizer.step()\n", "        losses.append(loss.item())\n", "    \n", "    return losses\n", "\n", "# Create training data\n", "X_train = torch.randn(2000, 784)\n", "y_train = torch.randint(0, 10, (2000,))\n", "\n", "print('üìä QAT vs PTQ COMPARISON')\n", "print('=' * 60)\n", "\n", "results = {'bits': [], 'ptq': [], 'qat': []}\n", "\n", "for bits in [8, 6, 4]:\n", "    print(f'\\n{bits}-bit Quantization:')\n", "    \n", "    # Train FP32 model then apply PTQ\n", "    fp32_model = SimpleNet()\n", "    train_model(fp32_model, X_train, y_train, epochs=30)\n", "    ptq_model = apply_ptq(fp32_model, bits)\n", "    ptq_acc = evaluate(ptq_model, X_test, y_test)\n", "    print(f'  PTQ accuracy: {ptq_acc:.1f}%')\n", "    \n", "    # Train with QAT from scratch\n", "    qat_model = QATNet(weight_bits=bits, act_bits=bits)\n", "    train_model(qat_model, X_train, y_train, epochs=30)\n", "    qat_acc = evaluate(qat_model, X_test, y_test)\n", "    print(f'  QAT accuracy: {qat_acc:.1f}%')\n", "    print(f'  Improvement: +{qat_acc - ptq_acc:.1f}%')\n", "    \n", "    results['bits'].append(bits)\n", "    results['ptq'].append(ptq_acc)\n", "    results['qat'].append(qat_acc)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize results\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "x = np.arange(len(results['bits']))\n", "width = 0.35\n", "\n", "bars1 = ax.bar(x - width/2, results['ptq'], width, label='PTQ', color='#ef4444')\n", "bars2 = ax.bar(x + width/2, results['qat'], width, label='QAT', color='#22c55e')\n", "\n", "ax.set_xlabel('Bit Width', fontsize=12)\n", "ax.set_ylabel('Accuracy (%)', fontsize=12)\n", "ax.set_title('üìä QAT vs PTQ Accuracy', fontsize=14)\n", "ax.set_xticks(x)\n", "ax.set_xticklabels([f'{b}-bit' for b in results['bits']])\n", "ax.legend()\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "# Add value labels\n", "for bars in [bars1, bars2]:\n", "    for bar in bars:\n", "        height = bar.get_height()\n", "        ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n", "                f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüí° QAT significantly outperforms PTQ at low bit widths!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Mixed-Precision Quantization"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MixedPrecisionNet(nn.Module):\n", "    \"\"\"\n", "    Different layers use different bit widths.\n", "    First/last layers often need more precision.\n", "    \"\"\"\n", "    def __init__(self):\n", "        super().__init__()\n", "        # First layer: 8-bit (sensitive to quantization)\n", "        self.fc1 = QATLinear(784, 256, weight_bits=8, act_bits=8)\n", "        # Middle layer: 4-bit (can handle more compression)\n", "        self.fc2 = QATLinear(256, 64, weight_bits=4, act_bits=4)\n", "        # Last layer: 8-bit (sensitive to quantization)\n", "        self.fc3 = QATLinear(64, 10, weight_bits=8, act_bits=8)\n", "    \n", "    def forward(self, x):\n", "        x = F.relu(self.fc1(x))\n", "        x = F.relu(self.fc2(x))\n", "        return self.fc3(x)\n", "\n", "print('üìä MIXED-PRECISION QUANTIZATION')\n", "print('=' * 50)\n", "\n", "# Train uniform 4-bit model\n", "uniform_4bit = QATNet(weight_bits=4, act_bits=4)\n", "train_model(uniform_4bit, X_train, y_train, epochs=30)\n", "uniform_acc = evaluate(uniform_4bit, X_test, y_test)\n", "\n", "# Train mixed precision model\n", "mixed_model = MixedPrecisionNet()\n", "train_model(mixed_model, X_train, y_train, epochs=30)\n", "mixed_acc = evaluate(mixed_model, X_test, y_test)\n", "\n", "# Calculate average bits\n", "def calc_avg_bits(model):\n", "    total_params = 0\n", "    total_bits = 0\n", "    for name, module in model.named_modules():\n", "        if isinstance(module, QATLinear):\n", "            params = module.weight.numel()\n", "            total_params += params\n", "            total_bits += params * module.weight_bits\n", "    return total_bits / total_params if total_params > 0 else 0\n", "\n", "print(f'\\nUniform 4-bit:')\n", "print(f'  Accuracy: {uniform_acc:.1f}%')\n", "print(f'  Avg bits: 4.0')\n", "\n", "print(f'\\nMixed Precision (8-4-8):')\n", "print(f'  Accuracy: {mixed_acc:.1f}%')\n", "print(f'  Avg bits: ~{calc_avg_bits(mixed_model):.1f}')\n", "\n", "print(f'\\nüí° Mixed precision: Better accuracy with similar compression!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Sensitivity Analysis"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def layer_sensitivity_analysis(model, X, y, bits_range=[8, 6, 4, 2]):\n", "    \"\"\"\n", "    Analyze each layer's sensitivity to quantization.\n", "    Quantize one layer at a time and measure accuracy drop.\n", "    \"\"\"\n", "    baseline = evaluate(model, X, y)\n", "    \n", "    results = {}\n", "    \n", "    for name, param in model.named_parameters():\n", "        if 'weight' not in name:\n", "            continue\n", "        \n", "        results[name] = []\n", "        \n", "        for bits in bits_range:\n", "            # Create copy and quantize only this layer\n", "            model_copy = type(model)()\n", "            model_copy.load_state_dict(model.state_dict())\n", "            \n", "            with torch.no_grad():\n", "                for n, p in model_copy.named_parameters():\n", "                    if n == name:\n", "                        p.data = quantize_weight(p.data, bits)\n", "            \n", "            acc = evaluate(model_copy, X, y)\n", "            results[name].append({\n", "                'bits': bits,\n", "                'accuracy': acc,\n", "                'drop': baseline - acc\n", "            })\n", "    \n", "    return results, baseline\n", "\n", "# Run sensitivity analysis\n", "sensitivity, baseline = layer_sensitivity_analysis(model, X_test, y_test)\n", "\n", "print('üìä LAYER SENSITIVITY ANALYSIS')\n", "print('=' * 60)\n", "print(f'Baseline (FP32): {baseline:.1f}%')\n", "print(f'\\n{\"Layer\":<20} {\"8-bit\":<12} {\"4-bit\":<12} {\"2-bit\":<12}')\n", "print('-' * 60)\n", "\n", "for name, results in sensitivity.items():\n", "    short_name = name.split('.')[0]\n", "    row = f'{short_name:<20}'\n", "    for r in results:\n", "        if r['bits'] in [8, 4, 2]:\n", "            row += f'{r[\"drop\"]:+.1f}%{\" \":>8}'\n", "    print(row)\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "layers = list(sensitivity.keys())\n", "x = np.arange(len(layers))\n", "width = 0.25\n", "\n", "for i, bits in enumerate([8, 4, 2]):\n", "    drops = []\n", "    for layer in layers:\n", "        for r in sensitivity[layer]:\n", "            if r['bits'] == bits:\n", "                drops.append(r['drop'])\n", "                break\n", "    \n", "    ax.bar(x + i*width, drops, width, label=f'{bits}-bit')\n", "\n", "ax.set_xlabel('Layer')\n", "ax.set_ylabel('Accuracy Drop (%)')\n", "ax.set_title('üìä Layer Sensitivity to Quantization')\n", "ax.set_xticks(x + width)\n", "ax.set_xticklabels([l.split('.')[0] for l in layers])\n", "ax.legend()\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüí° Use this analysis to decide per-layer bit widths!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('üéØ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. QAT > PTQ, especially at low bit widths (4-bit, 2-bit)')\n", "print('\\n2. STE: Forward=quantize, Backward=pass gradient through')\n", "print('\\n3. Fake quantization: Quantize‚ÜíDequantize during training')\n", "print('\\n4. Network learns to be robust to quantization noise')\n", "print('\\n5. Mixed precision: Different bits for different layers')\n", "print('\\n6. First/last layers are most sensitive')\n", "print('\\n7. Sensitivity analysis guides bit-width allocation')\n", "print('\\n' + '=' * 60)\n", "print('\\nüìö Next: Neural Architecture Search!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
