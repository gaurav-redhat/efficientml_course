{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸŽ“ Lecture 9: Knowledge Distillation - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/09_knowledge_distillation/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Teacher-student framework\n", "- Soft labels and temperature scaling\n", "- Different distillation methods\n", "- Feature-based vs output-based distillation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "print('Ready for Knowledge Distillation!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: The Idea of Knowledge Distillation\n", "\n", "**Goal**: Transfer knowledge from a large \"teacher\" to a small \"student\".\n", "\n", "**Key Insight**: Soft labels (probabilities) contain more information than hard labels.\n", "\n", "Example: For image of \"3\", teacher outputs:\n", "- \"3\": 0.7, \"8\": 0.2, \"5\": 0.05, ... (soft label - rich information!)\n", "- vs hard label: just \"3\" (less information)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize soft vs hard labels\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n", "\n", "# Hard label (one-hot)\n", "hard_label = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n", "axes[0].bar(classes, hard_label, color='#ef4444')\n", "axes[0].set_xlabel('Class')\n", "axes[0].set_ylabel('Probability')\n", "axes[0].set_title('Hard Label (Ground Truth)\\n\"This is definitely a 3\"', fontsize=12)\n", "axes[0].set_ylim(0, 1.1)\n", "\n", "# Soft label (teacher output)\n", "soft_label = [0.01, 0.01, 0.02, 0.70, 0.01, 0.05, 0.01, 0.02, 0.15, 0.02]\n", "axes[1].bar(classes, soft_label, color='#22c55e')\n", "axes[1].set_xlabel('Class')\n", "axes[1].set_ylabel('Probability')\n", "axes[1].set_title('Soft Label (Teacher Output)\\n\"Mostly 3, but similar to 8 and 5\"', fontsize=12)\n", "axes[1].set_ylim(0, 1.1)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('ðŸ’¡ Soft labels tell the student:')\n", "print('   - 3 looks similar to 8 (both have curves)')\n", "print('   - 3 looks somewhat like 5 (upper curve)')\n", "print('   - This is DARK KNOWLEDGE that hard labels miss!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Temperature Scaling\n", "\n", "Temperature softens the probability distribution:\n", "\n", "$$p_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n", "\n", "- T=1: Normal softmax\n", "- T>1: Softer distribution (more knowledge transfer)\n", "- T<1: Harder distribution"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def softmax_with_temperature(logits, temperature):\n", "    \"\"\"Apply softmax with temperature scaling.\"\"\"\n", "    return F.softmax(logits / temperature, dim=-1)\n", "\n", "# Example logits\n", "logits = torch.tensor([5.0, 2.0, 1.0, 0.5, 0.1, -0.5, -1.0, -1.5, 1.5, -2.0])\n", "\n", "temperatures = [0.5, 1.0, 2.0, 5.0, 10.0]\n", "\n", "fig, axes = plt.subplots(1, len(temperatures), figsize=(20, 4))\n", "\n", "for ax, T in zip(axes, temperatures):\n", "    probs = softmax_with_temperature(logits, T).numpy()\n", "    ax.bar(classes, probs, color=plt.cm.coolwarm(T/10))\n", "    ax.set_xlabel('Class')\n", "    ax.set_ylabel('Probability')\n", "    ax.set_title(f'T = {T}', fontsize=12)\n", "    ax.set_ylim(0, 1)\n", "    \n", "    # Show entropy\n", "    entropy = -np.sum(probs * np.log(probs + 1e-8))\n", "    ax.text(0.5, 0.95, f'Entropy: {entropy:.2f}', transform=ax.transAxes, \n", "            ha='center', fontsize=10)\n", "\n", "plt.suptitle('ðŸ“Š Effect of Temperature on Softmax', fontsize=14)\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ Higher temperature:')\n", "print('   - More uniform distribution')\n", "print('   - More information about class similarities')\n", "print('   - Better for knowledge transfer')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Knowledge Distillation Loss"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def distillation_loss(student_logits, teacher_logits, labels, T=4.0, alpha=0.7):\n", "    \"\"\"\n", "    Knowledge Distillation Loss:\n", "    \n", "    L = Î± * KL(soft_teacher || soft_student) * TÂ² + (1-Î±) * CE(student, labels)\n", "    \n", "    Args:\n", "        student_logits: Raw outputs from student\n", "        teacher_logits: Raw outputs from teacher\n", "        labels: Ground truth labels\n", "        T: Temperature for soft labels\n", "        alpha: Weight for distillation loss (vs CE loss)\n", "    \"\"\"\n", "    # Soft targets from teacher\n", "    soft_teacher = F.softmax(teacher_logits / T, dim=1)\n", "    soft_student = F.log_softmax(student_logits / T, dim=1)\n", "    \n", "    # KL divergence (distillation loss)\n", "    distill_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T ** 2)\n", "    \n", "    # Cross-entropy with hard labels\n", "    ce_loss = F.cross_entropy(student_logits, labels)\n", "    \n", "    # Combined loss\n", "    total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n", "    \n", "    return total_loss, distill_loss, ce_loss\n", "\n", "# Demo\n", "batch_size = 32\n", "num_classes = 10\n", "\n", "teacher_logits = torch.randn(batch_size, num_classes) * 3  # Teacher outputs\n", "student_logits = torch.randn(batch_size, num_classes)      # Student outputs\n", "labels = torch.randint(0, num_classes, (batch_size,))\n", "\n", "total, distill, ce = distillation_loss(student_logits, teacher_logits, labels)\n", "\n", "print('ðŸ“Š DISTILLATION LOSS COMPONENTS')\n", "print('=' * 50)\n", "print(f'Distillation Loss (KL): {distill.item():.4f}')\n", "print(f'Cross-Entropy Loss: {ce.item():.4f}')\n", "print(f'Total Loss (Î±=0.7): {total.item():.4f}')\n", "print(f'\\nFormula: L = 0.7 Ã— KL + 0.3 Ã— CE')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Complete Distillation Training"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define Teacher (large) and Student (small) models\n", "class TeacherNet(nn.Module):\n", "    \"\"\"Large teacher model.\"\"\"\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.features = nn.Sequential(\n", "            nn.Linear(784, 1200), nn.ReLU(), nn.Dropout(0.3),\n", "            nn.Linear(1200, 1200), nn.ReLU(), nn.Dropout(0.3),\n", "            nn.Linear(1200, 800), nn.ReLU(), nn.Dropout(0.3),\n", "        )\n", "        self.classifier = nn.Linear(800, 10)\n", "    \n", "    def forward(self, x):\n", "        x = self.features(x)\n", "        return self.classifier(x)\n", "\n", "class StudentNet(nn.Module):\n", "    \"\"\"Small student model.\"\"\"\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.features = nn.Sequential(\n", "            nn.Linear(784, 400), nn.ReLU(),\n", "            nn.Linear(400, 100), nn.ReLU(),\n", "        )\n", "        self.classifier = nn.Linear(100, 10)\n", "    \n", "    def forward(self, x):\n", "        x = self.features(x)\n", "        return self.classifier(x)\n", "\n", "# Count parameters\n", "teacher = TeacherNet()\n", "student = StudentNet()\n", "\n", "teacher_params = sum(p.numel() for p in teacher.parameters())\n", "student_params = sum(p.numel() for p in student.parameters())\n", "\n", "print('ðŸ“Š MODEL SIZES')\n", "print('=' * 40)\n", "print(f'Teacher: {teacher_params:,} parameters')\n", "print(f'Student: {student_params:,} parameters')\n", "print(f'Compression: {teacher_params/student_params:.1f}x smaller')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create synthetic data\n", "X_train = torch.randn(2000, 784)\n", "y_train = torch.randint(0, 10, (2000,))\n", "X_test = torch.randn(500, 784)\n", "y_test = torch.randint(0, 10, (500,))\n", "\n", "def evaluate(model, X, y):\n", "    model.eval()\n", "    with torch.no_grad():\n", "        return (model(X).argmax(1) == y).float().mean().item() * 100\n", "\n", "# Train Teacher\n", "print('ðŸ“š TRAINING TEACHER')\n", "print('=' * 50)\n", "\n", "teacher = TeacherNet()\n", "optimizer = torch.optim.Adam(teacher.parameters(), lr=0.001)\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "for epoch in range(30):\n", "    teacher.train()\n", "    optimizer.zero_grad()\n", "    loss = criterion(teacher(X_train), y_train)\n", "    loss.backward()\n", "    optimizer.step()\n", "    \n", "    if (epoch + 1) % 10 == 0:\n", "        acc = evaluate(teacher, X_test, y_test)\n", "        print(f'Epoch {epoch+1}: Loss={loss.item():.3f}, Acc={acc:.1f}%')\n", "\n", "teacher_acc = evaluate(teacher, X_test, y_test)\n", "print(f'\\nFinal Teacher Accuracy: {teacher_acc:.1f}%')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train Student with different methods\n", "def train_student(student, method='scratch', teacher=None, epochs=30, T=4.0, alpha=0.7):\n", "    \"\"\"\n", "    Train student with different methods:\n", "    - scratch: Normal training with CE loss\n", "    - distill: Knowledge distillation from teacher\n", "    \"\"\"\n", "    optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n", "    \n", "    history = {'loss': [], 'acc': []}\n", "    \n", "    for epoch in range(epochs):\n", "        student.train()\n", "        optimizer.zero_grad()\n", "        \n", "        student_logits = student(X_train)\n", "        \n", "        if method == 'scratch':\n", "            loss = F.cross_entropy(student_logits, y_train)\n", "        else:  # distillation\n", "            teacher.eval()\n", "            with torch.no_grad():\n", "                teacher_logits = teacher(X_train)\n", "            loss, _, _ = distillation_loss(student_logits, teacher_logits, y_train, T, alpha)\n", "        \n", "        loss.backward()\n", "        optimizer.step()\n", "        \n", "        acc = evaluate(student, X_test, y_test)\n", "        history['loss'].append(loss.item())\n", "        history['acc'].append(acc)\n", "    \n", "    return history\n", "\n", "print('ðŸ“š TRAINING STUDENTS')\n", "print('=' * 50)\n", "\n", "# Student trained from scratch\n", "print('\\n1ï¸âƒ£ Student trained from scratch...')\n", "student_scratch = StudentNet()\n", "history_scratch = train_student(student_scratch, method='scratch', epochs=30)\n", "\n", "# Student with distillation\n", "print('\\n2ï¸âƒ£ Student with knowledge distillation...')\n", "student_distill = StudentNet()\n", "history_distill = train_student(student_distill, method='distill', teacher=teacher, epochs=30)\n", "\n", "print(f'\\nðŸ“Š FINAL RESULTS')\n", "print('=' * 50)\n", "print(f'Teacher:              {teacher_acc:.1f}% ({teacher_params:,} params)')\n", "print(f'Student (scratch):    {history_scratch[\"acc\"][-1]:.1f}% ({student_params:,} params)')\n", "print(f'Student (distilled):  {history_distill[\"acc\"][-1]:.1f}% ({student_params:,} params)')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize results\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Accuracy curves\n", "axes[0].plot(history_scratch['acc'], label='From Scratch', color='#ef4444', linewidth=2)\n", "axes[0].plot(history_distill['acc'], label='Distilled', color='#22c55e', linewidth=2)\n", "axes[0].axhline(y=teacher_acc, color='#3b82f6', linestyle='--', label=f'Teacher ({teacher_acc:.1f}%)')\n", "axes[0].set_xlabel('Epoch')\n", "axes[0].set_ylabel('Accuracy (%)')\n", "axes[0].set_title('Student Training Comparison')\n", "axes[0].legend()\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "# Final comparison\n", "methods = ['Teacher', 'Student\\n(scratch)', 'Student\\n(distilled)']\n", "accuracies = [teacher_acc, history_scratch['acc'][-1], history_distill['acc'][-1]]\n", "params = [teacher_params/1000, student_params/1000, student_params/1000]\n", "colors = ['#3b82f6', '#ef4444', '#22c55e']\n", "\n", "ax2 = axes[1]\n", "bars = ax2.bar(methods, accuracies, color=colors)\n", "ax2.set_ylabel('Accuracy (%)')\n", "ax2.set_title('Final Accuracy Comparison')\n", "\n", "for bar, p, acc in zip(bars, params, accuracies):\n", "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n", "             f'{acc:.1f}%\\n({p:.0f}K params)', ha='center', fontsize=10)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ Distillation helps small model learn better!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Temperature and Alpha Sensitivity"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Test different hyperparameters\n", "print('ðŸ“Š HYPERPARAMETER SENSITIVITY')\n", "print('=' * 50)\n", "\n", "# Temperature sensitivity\n", "print('\\nTemperature sensitivity (Î±=0.7):')\n", "temp_results = []\n", "for T in [1, 2, 4, 8, 16]:\n", "    student = StudentNet()\n", "    history = train_student(student, method='distill', teacher=teacher, T=T, alpha=0.7, epochs=20)\n", "    temp_results.append((T, history['acc'][-1]))\n", "    print(f'  T={T:2d}: {history[\"acc\"][-1]:.1f}%')\n", "\n", "# Alpha sensitivity\n", "print('\\nAlpha sensitivity (T=4):')\n", "alpha_results = []\n", "for alpha in [0.0, 0.3, 0.5, 0.7, 0.9, 1.0]:\n", "    student = StudentNet()\n", "    history = train_student(student, method='distill', teacher=teacher, T=4, alpha=alpha, epochs=20)\n", "    alpha_results.append((alpha, history['acc'][-1]))\n", "    print(f'  Î±={alpha:.1f}: {history[\"acc\"][-1]:.1f}%')\n", "\n", "# Visualize\n", "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n", "\n", "temps, temp_accs = zip(*temp_results)\n", "axes[0].plot(temps, temp_accs, 'o-', color='#3b82f6', linewidth=2, markersize=8)\n", "axes[0].set_xlabel('Temperature (T)')\n", "axes[0].set_ylabel('Accuracy (%)')\n", "axes[0].set_title('Temperature Sensitivity')\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "alphas, alpha_accs = zip(*alpha_results)\n", "axes[1].plot(alphas, alpha_accs, 'o-', color='#22c55e', linewidth=2, markersize=8)\n", "axes[1].set_xlabel('Alpha (Î±)')\n", "axes[1].set_ylabel('Accuracy (%)')\n", "axes[1].set_title('Alpha Sensitivity (KD vs CE weight)')\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ Typical good values: T=4-8, Î±=0.5-0.9')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('ðŸŽ¯ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. Soft labels contain \"dark knowledge\" about class similarities')\n", "print('\\n2. Temperature (T) controls how soft the labels are')\n", "print('\\n3. KD Loss = Î± Ã— KL_div + (1-Î±) Ã— CE_loss')\n", "print('\\n4. Distilled students often outperform scratch-trained')\n", "print('\\n5. Student can be 10x smaller with similar accuracy')\n", "print('\\n6. T=4-8 and Î±=0.5-0.9 usually work well')\n", "print('\\n' + '=' * 60)\n", "print('\\nðŸ“š Next: MCUNet and TinyML!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
