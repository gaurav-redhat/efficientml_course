{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üîç Lecture 7: Neural Architecture Search (Part 1) - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/07_neural_architecture_search_1/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- NAS fundamentals: Search space, strategy, and evaluation\n", "- DARTS: Differentiable architecture search\n", "- Supernet training with architecture weights\n", "- Deriving final architecture from trained supernet"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "print('Ready for Neural Architecture Search!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: The NAS Problem\n", "\n", "**Goal**: Automatically find the best neural network architecture.\n", "\n", "**Three components**:\n", "1. **Search Space**: What architectures can we explore?\n", "2. **Search Strategy**: How do we explore efficiently?\n", "3. **Evaluation Strategy**: How do we measure architecture quality?"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize the search space\n", "operations = {\n", "    'conv3x3': 'Standard 3√ó3 convolution',\n", "    'conv5x5': 'Standard 5√ó5 convolution',\n", "    'sep_conv3x3': 'Depthwise separable 3√ó3',\n", "    'dil_conv3x3': 'Dilated 3√ó3 convolution',\n", "    'max_pool': 'Max pooling 3√ó3',\n", "    'avg_pool': 'Average pooling 3√ó3',\n", "    'skip': 'Skip connection (identity)',\n", "    'zero': 'No connection',\n", "}\n", "\n", "print('üîç TYPICAL NAS SEARCH SPACE')\n", "print('=' * 60)\n", "print('\\nOperations available at each edge:')\n", "for op, desc in operations.items():\n", "    print(f'  ‚Ä¢ {op:15} - {desc}')\n", "\n", "# Calculate search space size\n", "num_ops = len(operations)\n", "num_edges = 14  # Typical DARTS cell has 14 edges\n", "search_space_size = num_ops ** num_edges\n", "\n", "print(f'\\nüìä Search Space Size:')\n", "print(f'   Operations: {num_ops}')\n", "print(f'   Edges per cell: {num_edges}')\n", "print(f'   Total architectures: {num_ops}^{num_edges} = {search_space_size:,}')\n", "print(f'\\n‚ö†Ô∏è Exhaustive search is impossible!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: DARTS - Differentiable Architecture Search\n", "\n", "**Key Idea**: Make architecture choice differentiable!\n", "\n", "Instead of discrete choice, use weighted sum of all operations:\n", "\n", "$$\\bar{o}(x) = \\sum_i \\frac{\\exp(\\alpha_i)}{\\sum_j \\exp(\\alpha_j)} \\cdot o_i(x)$$\n", "\n", "Where $\\alpha_i$ are learnable architecture parameters."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MixedOperation(nn.Module):\n", "    \"\"\"\n", "    Mixed operation: Weighted sum of candidate operations.\n", "    Architecture weights (alpha) are learned jointly with model weights.\n", "    \"\"\"\n", "    def __init__(self, in_channels, out_channels):\n", "        super().__init__()\n", "        \n", "        # Candidate operations\n", "        self.ops = nn.ModuleList([\n", "            nn.Sequential(  # conv3x3\n", "                nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n", "                nn.BatchNorm2d(out_channels),\n", "                nn.ReLU()\n", "            ),\n", "            nn.Sequential(  # conv5x5\n", "                nn.Conv2d(in_channels, out_channels, 5, padding=2, bias=False),\n", "                nn.BatchNorm2d(out_channels),\n", "                nn.ReLU()\n", "            ),\n", "            nn.Sequential(  # sep_conv3x3\n", "                nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False),\n", "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n", "                nn.BatchNorm2d(out_channels),\n", "                nn.ReLU()\n", "            ),\n", "            nn.MaxPool2d(3, stride=1, padding=1),  # max_pool\n", "            nn.Identity(),  # skip connection\n", "        ])\n", "        \n", "        self.op_names = ['conv3x3', 'conv5x5', 'sep_conv3x3', 'max_pool', 'skip']\n", "        \n", "        # Architecture weights (learnable)\n", "        self.alpha = nn.Parameter(torch.zeros(len(self.ops)))\n", "    \n", "    def forward(self, x):\n", "        # Softmax over architecture weights\n", "        weights = F.softmax(self.alpha, dim=0)\n", "        \n", "        # Weighted sum of all operations\n", "        out = sum(w * op(x) for w, op in zip(weights, self.ops))\n", "        return out\n", "    \n", "    def get_selected_op(self):\n", "        \"\"\"Return the operation with highest weight.\"\"\"\n", "        idx = self.alpha.argmax().item()\n", "        return self.op_names[idx], F.softmax(self.alpha, dim=0)[idx].item()\n", "\n", "# Demo\n", "mixed_op = MixedOperation(16, 16)\n", "x = torch.randn(1, 16, 32, 32)\n", "out = mixed_op(x)\n", "\n", "print('üìä MIXED OPERATION')\n", "print('=' * 50)\n", "print(f'Input shape: {x.shape}')\n", "print(f'Output shape: {out.shape}')\n", "print(f'\\nArchitecture weights (before training):')\n", "weights = F.softmax(mixed_op.alpha, dim=0)\n", "for name, w in zip(mixed_op.op_names, weights):\n", "    print(f'  {name:15}: {w.item():.3f}')\n", "print(f'\\nüí° Initially uniform - will change during training!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Building a DARTS Supernet"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DARTSCell(nn.Module):\n", "    \"\"\"A cell with multiple mixed operations.\"\"\"\n", "    def __init__(self, channels):\n", "        super().__init__()\n", "        \n", "        # Each cell has multiple edges, each with a mixed operation\n", "        self.edges = nn.ModuleList([\n", "            MixedOperation(channels, channels),\n", "            MixedOperation(channels, channels),\n", "            MixedOperation(channels, channels),\n", "        ])\n", "    \n", "    def forward(self, x):\n", "        # Simple sequential for demo (real DARTS has DAG structure)\n", "        for edge in self.edges:\n", "            x = x + edge(x)  # Residual connection\n", "        return x\n", "\n", "class DARTSSupernet(nn.Module):\n", "    \"\"\"Supernet that contains all possible architectures.\"\"\"\n", "    def __init__(self, num_classes=10):\n", "        super().__init__()\n", "        \n", "        # Stem\n", "        self.stem = nn.Sequential(\n", "            nn.Conv2d(3, 16, 3, padding=1, bias=False),\n", "            nn.BatchNorm2d(16),\n", "            nn.ReLU()\n", "        )\n", "        \n", "        # Searchable cells\n", "        self.cells = nn.ModuleList([\n", "            DARTSCell(16),\n", "            DARTSCell(16),\n", "        ])\n", "        \n", "        # Classifier\n", "        self.gap = nn.AdaptiveAvgPool2d(1)\n", "        self.fc = nn.Linear(16, num_classes)\n", "    \n", "    def forward(self, x):\n", "        x = self.stem(x)\n", "        for cell in self.cells:\n", "            x = cell(x)\n", "        x = self.gap(x).flatten(1)\n", "        return self.fc(x)\n", "    \n", "    def get_architecture_params(self):\n", "        \"\"\"Return all architecture parameters (alphas).\"\"\"\n", "        arch_params = []\n", "        for cell in self.cells:\n", "            for edge in cell.edges:\n", "                arch_params.append(edge.alpha)\n", "        return arch_params\n", "    \n", "    def get_weight_params(self):\n", "        \"\"\"Return all weight parameters (excluding alphas).\"\"\"\n", "        weight_params = []\n", "        for name, param in self.named_parameters():\n", "            if 'alpha' not in name:\n", "                weight_params.append(param)\n", "        return weight_params\n", "    \n", "    def print_architecture(self):\n", "        \"\"\"Print the current architecture.\"\"\"\n", "        print('\\nüèóÔ∏è Current Architecture:')\n", "        for i, cell in enumerate(self.cells):\n", "            print(f'\\nCell {i}:')\n", "            for j, edge in enumerate(cell.edges):\n", "                op, prob = edge.get_selected_op()\n", "                print(f'  Edge {j}: {op} ({prob:.2%})')\n", "\n", "# Create supernet\n", "supernet = DARTSSupernet()\n", "print('üìä DARTS SUPERNET')\n", "print('=' * 50)\n", "\n", "# Count parameters\n", "arch_params = sum(p.numel() for p in supernet.get_architecture_params())\n", "weight_params = sum(p.numel() for p in supernet.get_weight_params())\n", "\n", "print(f'Architecture parameters: {arch_params}')\n", "print(f'Weight parameters: {weight_params:,}')\n", "\n", "supernet.print_architecture()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Bi-level Optimization"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create synthetic data\n", "def create_data(n_samples=500, img_size=32):\n", "    X = torch.randn(n_samples, 3, img_size, img_size)\n", "    y = torch.randint(0, 10, (n_samples,))\n", "    return X, y\n", "\n", "X_train, y_train = create_data(500)\n", "X_val, y_val = create_data(200)\n", "\n", "def train_darts(supernet, X_train, y_train, X_val, y_val, epochs=20):\n", "    \"\"\"\n", "    DARTS bi-level optimization:\n", "    1. Update weights w on training data\n", "    2. Update architecture Œ± on validation data\n", "    \"\"\"\n", "    # Two optimizers\n", "    weight_optimizer = torch.optim.SGD(supernet.get_weight_params(), lr=0.01, momentum=0.9)\n", "    arch_optimizer = torch.optim.Adam(supernet.get_architecture_params(), lr=0.001)\n", "    \n", "    criterion = nn.CrossEntropyLoss()\n", "    \n", "    history = {'train_loss': [], 'val_loss': [], 'arch_entropy': []}\n", "    \n", "    for epoch in range(epochs):\n", "        supernet.train()\n", "        \n", "        # Step 1: Update weights on training data\n", "        weight_optimizer.zero_grad()\n", "        train_output = supernet(X_train)\n", "        train_loss = criterion(train_output, y_train)\n", "        train_loss.backward()\n", "        weight_optimizer.step()\n", "        \n", "        # Step 2: Update architecture on validation data\n", "        arch_optimizer.zero_grad()\n", "        val_output = supernet(X_val)\n", "        val_loss = criterion(val_output, y_val)\n", "        val_loss.backward()\n", "        arch_optimizer.step()\n", "        \n", "        # Track architecture entropy (how decisive the choices are)\n", "        entropy = 0\n", "        for cell in supernet.cells:\n", "            for edge in cell.edges:\n", "                probs = F.softmax(edge.alpha, dim=0)\n", "                entropy -= (probs * (probs + 1e-8).log()).sum().item()\n", "        \n", "        history['train_loss'].append(train_loss.item())\n", "        history['val_loss'].append(val_loss.item())\n", "        history['arch_entropy'].append(entropy)\n", "        \n", "        if (epoch + 1) % 5 == 0:\n", "            print(f'Epoch {epoch+1}: Train Loss={train_loss.item():.3f}, '\n", "                  f'Val Loss={val_loss.item():.3f}, Entropy={entropy:.3f}')\n", "    \n", "    return history\n", "\n", "print('üîÑ TRAINING DARTS SUPERNET')\n", "print('=' * 50)\n", "history = train_darts(supernet, X_train, y_train, X_val, y_val, epochs=30)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize training\n", "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n", "\n", "axes[0].plot(history['train_loss'], label='Train', color='#3b82f6')\n", "axes[0].plot(history['val_loss'], label='Val', color='#ef4444')\n", "axes[0].set_xlabel('Epoch')\n", "axes[0].set_ylabel('Loss')\n", "axes[0].set_title('Loss During Search')\n", "axes[0].legend()\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "axes[1].plot(history['arch_entropy'], color='#22c55e')\n", "axes[1].set_xlabel('Epoch')\n", "axes[1].set_ylabel('Architecture Entropy')\n", "axes[1].set_title('Architecture Becoming More Decisive')\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "# Show final architecture weights\n", "all_weights = []\n", "labels = []\n", "for i, cell in enumerate(supernet.cells):\n", "    for j, edge in enumerate(cell.edges):\n", "        probs = F.softmax(edge.alpha, dim=0).detach().numpy()\n", "        all_weights.append(probs)\n", "        labels.append(f'C{i}E{j}')\n", "\n", "all_weights = np.array(all_weights)\n", "im = axes[2].imshow(all_weights.T, cmap='YlOrRd', aspect='auto')\n", "axes[2].set_xlabel('Edge')\n", "axes[2].set_ylabel('Operation')\n", "axes[2].set_xticks(range(len(labels)))\n", "axes[2].set_xticklabels(labels)\n", "axes[2].set_yticks(range(5))\n", "axes[2].set_yticklabels(supernet.cells[0].edges[0].op_names)\n", "axes[2].set_title('Architecture Weights (Brighter = Higher)')\n", "plt.colorbar(im, ax=axes[2])\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Print final architecture\n", "supernet.print_architecture()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Deriving Final Architecture"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def derive_architecture(supernet):\n", "    \"\"\"\n", "    Derive discrete architecture from trained supernet.\n", "    Select the operation with highest weight at each edge.\n", "    \"\"\"\n", "    architecture = []\n", "    \n", "    for i, cell in enumerate(supernet.cells):\n", "        cell_arch = []\n", "        for j, edge in enumerate(cell.edges):\n", "            op, prob = edge.get_selected_op()\n", "            cell_arch.append({\n", "                'operation': op,\n", "                'probability': prob\n", "            })\n", "        architecture.append(cell_arch)\n", "    \n", "    return architecture\n", "\n", "# Derive architecture\n", "final_arch = derive_architecture(supernet)\n", "\n", "print('üèÜ FINAL DISCOVERED ARCHITECTURE')\n", "print('=' * 50)\n", "\n", "for i, cell in enumerate(final_arch):\n", "    print(f'\\nCell {i}:')\n", "    for j, edge in enumerate(cell):\n", "        print(f'  Edge {j}: {edge[\"operation\"]} (confidence: {edge[\"probability\"]:.1%})')\n", "\n", "# Create discrete model from discovered architecture\n", "class DiscoveredNet(nn.Module):\n", "    \"\"\"Discrete model based on discovered architecture.\"\"\"\n", "    def __init__(self, architecture, channels=16, num_classes=10):\n", "        super().__init__()\n", "        \n", "        self.stem = nn.Sequential(\n", "            nn.Conv2d(3, channels, 3, padding=1, bias=False),\n", "            nn.BatchNorm2d(channels),\n", "            nn.ReLU()\n", "        )\n", "        \n", "        # Build cells based on discovered architecture\n", "        self.cells = nn.ModuleList()\n", "        for cell_arch in architecture:\n", "            cell_ops = []\n", "            for edge in cell_arch:\n", "                op = self._make_op(edge['operation'], channels)\n", "                cell_ops.append(op)\n", "            self.cells.append(nn.ModuleList(cell_ops))\n", "        \n", "        self.gap = nn.AdaptiveAvgPool2d(1)\n", "        self.fc = nn.Linear(channels, num_classes)\n", "    \n", "    def _make_op(self, name, channels):\n", "        if name == 'conv3x3':\n", "            return nn.Sequential(\n", "                nn.Conv2d(channels, channels, 3, padding=1, bias=False),\n", "                nn.BatchNorm2d(channels), nn.ReLU())\n", "        elif name == 'conv5x5':\n", "            return nn.Sequential(\n", "                nn.Conv2d(channels, channels, 5, padding=2, bias=False),\n", "                nn.BatchNorm2d(channels), nn.ReLU())\n", "        elif name == 'sep_conv3x3':\n", "            return nn.Sequential(\n", "                nn.Conv2d(channels, channels, 3, padding=1, groups=channels, bias=False),\n", "                nn.Conv2d(channels, channels, 1, bias=False),\n", "                nn.BatchNorm2d(channels), nn.ReLU())\n", "        elif name == 'max_pool':\n", "            return nn.MaxPool2d(3, stride=1, padding=1)\n", "        else:  # skip\n", "            return nn.Identity()\n", "    \n", "    def forward(self, x):\n", "        x = self.stem(x)\n", "        for cell_ops in self.cells:\n", "            for op in cell_ops:\n", "                x = x + op(x)\n", "        x = self.gap(x).flatten(1)\n", "        return self.fc(x)\n", "\n", "# Create and compare\n", "discovered_net = DiscoveredNet(final_arch)\n", "\n", "supernet_params = sum(p.numel() for p in supernet.parameters())\n", "discovered_params = sum(p.numel() for p in discovered_net.parameters())\n", "\n", "print(f'\\nüìä MODEL COMPARISON')\n", "print(f'Supernet parameters: {supernet_params:,}')\n", "print(f'Discovered net parameters: {discovered_params:,}')\n", "print(f'Reduction: {supernet_params/discovered_params:.1f}x smaller!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('üéØ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. NAS automates architecture design')\n", "print('\\n2. Search space can have billions of architectures')\n", "print('\\n3. DARTS makes search differentiable using softmax')\n", "print('\\n4. Bi-level optimization: weights on train, arch on val')\n", "print('\\n5. Architecture weights converge to discrete choices')\n", "print('\\n6. Final model is much smaller than supernet')\n", "print('\\n7. Discovered architectures often beat hand-designed ones!')\n", "print('\\n' + '=' * 60)\n", "print('\\nüìö Next: Hardware-Aware NAS!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
