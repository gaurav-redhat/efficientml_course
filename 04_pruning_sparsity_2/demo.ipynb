{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸŽ° Lecture 4: Lottery Ticket Hypothesis - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/04_pruning_sparsity_2/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- The Lottery Ticket Hypothesis (LTH)\n", "- Finding winning tickets through iterative pruning\n", "- Why initialization matters for sparse networks\n", "- Practical implementation of LTH"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch torchvision matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.utils.prune as prune\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import copy\n", "\n", "torch.manual_seed(42)\n", "print('Ready for Lottery Ticket discovery!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: The Lottery Ticket Hypothesis Explained\n", "\n", "> **\"A randomly-initialized, dense neural network contains a subnetwork that, \n", "> when trained in isolation, can match the test accuracy of the original network \n", "> after training for at most the same number of iterations.\"**\n", "> \n", "> â€” Frankle & Carlin, 2019\n", "\n", "ðŸŽ° Think of it like a lottery:\n", "- **Dense network** = Buying ALL lottery tickets\n", "- **Winning ticket** = The sparse subnetwork that can win (achieve good accuracy)\n", "- **Key insight** = The winning ticket was there from the START (initialization)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualization of the hypothesis\n", "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n", "\n", "# Dense network\n", "dense = np.random.randn(20, 20) * 0.1\n", "axes[0].imshow(dense, cmap='RdBu', vmin=-0.3, vmax=0.3)\n", "axes[0].set_title('1. Dense Network\\n(Random Init)', fontsize=12)\n", "axes[0].axis('off')\n", "\n", "# Training\n", "trained = dense + np.random.randn(20, 20) * 0.3\n", "trained[np.abs(trained) < 0.15] *= 0.1  # Many weights become small\n", "axes[1].imshow(trained, cmap='RdBu', vmin=-0.5, vmax=0.5)\n", "axes[1].set_title('2. After Training\\n(Many small weights)', fontsize=12)\n", "axes[1].axis('off')\n", "\n", "# Pruning\n", "mask = np.abs(trained) > 0.2\n", "pruned = trained * mask\n", "axes[2].imshow(pruned, cmap='RdBu', vmin=-0.5, vmax=0.5)\n", "axes[2].set_title('3. After Pruning\\n(Sparse network)', fontsize=12)\n", "axes[2].axis('off')\n", "\n", "# Winning ticket (original init with mask)\n", "winning_ticket = dense * mask\n", "axes[3].imshow(winning_ticket, cmap='RdBu', vmin=-0.3, vmax=0.3)\n", "axes[3].set_title('4. WINNING TICKET\\n(Original init + mask)', fontsize=12)\n", "axes[3].axis('off')\n", "\n", "plt.suptitle('ðŸŽ° The Lottery Ticket Hypothesis', fontsize=14)\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('ðŸ’¡ The winning ticket uses the ORIGINAL initialization, not trained weights!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Implementing Lottery Ticket Discovery"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LotteryNet(nn.Module):\n", "    \"\"\"Simple network for lottery ticket experiments.\"\"\"\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = nn.Linear(784, 300)\n", "        self.fc2 = nn.Linear(300, 100)\n", "        self.fc3 = nn.Linear(100, 10)\n", "        self.relu = nn.ReLU()\n", "    \n", "    def forward(self, x):\n", "        x = self.relu(self.fc1(x))\n", "        x = self.relu(self.fc2(x))\n", "        return self.fc3(x)\n", "\n", "def save_initial_weights(model):\n", "    \"\"\"Save initial weights for later rewinding.\"\"\"\n", "    return {name: param.data.clone() for name, param in model.named_parameters()}\n", "\n", "def rewind_weights(model, initial_weights, masks):\n", "    \"\"\"Rewind to initial weights with pruning mask applied.\"\"\"\n", "    for name, param in model.named_parameters():\n", "        if name in masks:\n", "            param.data = initial_weights[name] * masks[name]\n", "        else:\n", "            param.data = initial_weights[name].clone()\n", "\n", "def create_masks(model, sparsity):\n", "    \"\"\"Create pruning masks based on trained weight magnitudes.\"\"\"\n", "    masks = {}\n", "    for name, param in model.named_parameters():\n", "        if 'weight' in name:\n", "            threshold = torch.quantile(param.data.abs().flatten(), sparsity)\n", "            masks[name] = (param.data.abs() > threshold).float()\n", "    return masks\n", "\n", "def apply_masks(model, masks):\n", "    \"\"\"Apply masks to model weights.\"\"\"\n", "    for name, param in model.named_parameters():\n", "        if name in masks:\n", "            param.data *= masks[name]\n", "\n", "def count_nonzero(model):\n", "    \"\"\"Count non-zero parameters.\"\"\"\n", "    total = sum(p.numel() for p in model.parameters())\n", "    nonzero = sum((p != 0).sum().item() for p in model.parameters())\n", "    return total, nonzero\n", "\n", "# Create synthetic data\n", "X_train = torch.randn(2000, 784)\n", "y_train = torch.randint(0, 10, (2000,))\n", "X_test = torch.randn(500, 784)\n", "y_test = torch.randint(0, 10, (500,))\n", "\n", "print('âœ… Lottery Ticket utilities ready!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Finding the Winning Ticket"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_model(model, X, y, masks=None, epochs=10, lr=0.01):\n", "    \"\"\"Train model with optional mask application.\"\"\"\n", "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n", "    criterion = nn.CrossEntropyLoss()\n", "    \n", "    losses = []\n", "    for epoch in range(epochs):\n", "        model.train()\n", "        optimizer.zero_grad()\n", "        \n", "        # Forward pass\n", "        outputs = model(X)\n", "        loss = criterion(outputs, y)\n", "        \n", "        # Backward pass\n", "        loss.backward()\n", "        \n", "        # Zero gradients of pruned weights\n", "        if masks:\n", "            for name, param in model.named_parameters():\n", "                if name in masks and param.grad is not None:\n", "                    param.grad.data *= masks[name]\n", "        \n", "        optimizer.step()\n", "        \n", "        # Re-apply masks\n", "        if masks:\n", "            apply_masks(model, masks)\n", "        \n", "        losses.append(loss.item())\n", "    \n", "    return losses\n", "\n", "def evaluate(model, X, y):\n", "    \"\"\"Evaluate model accuracy.\"\"\"\n", "    model.eval()\n", "    with torch.no_grad():\n", "        outputs = model(X)\n", "        _, predicted = outputs.max(1)\n", "        accuracy = (predicted == y).float().mean().item() * 100\n", "    return accuracy\n", "\n", "# Initialize model and save initial weights\n", "model = LotteryNet()\n", "initial_weights = save_initial_weights(model)\n", "\n", "print('ðŸŽ° LOTTERY TICKET EXPERIMENT')\n", "print('=' * 60)\n", "\n", "# Step 1: Train original dense network\n", "print('\\n1ï¸âƒ£ Training dense network...')\n", "dense_losses = train_model(model, X_train, y_train, epochs=20)\n", "dense_acc = evaluate(model, X_test, y_test)\n", "print(f'   Dense accuracy: {dense_acc:.1f}%')\n", "\n", "# Step 2: Create masks from trained network\n", "print('\\n2ï¸âƒ£ Creating pruning masks (80% sparsity)...')\n", "masks = create_masks(model, sparsity=0.8)\n", "\n", "# Step 3: Random reinitialization (baseline)\n", "print('\\n3ï¸âƒ£ Training with random reinitialization...')\n", "random_model = LotteryNet()  # New random init\n", "apply_masks(random_model, masks)  # Apply same mask\n", "random_losses = train_model(random_model, X_train, y_train, masks=masks, epochs=20)\n", "random_acc = evaluate(random_model, X_test, y_test)\n", "print(f'   Random sparse accuracy: {random_acc:.1f}%')\n", "\n", "# Step 4: Winning ticket (original init + mask)\n", "print('\\n4ï¸âƒ£ Training WINNING TICKET (original init + mask)...')\n", "winning_model = LotteryNet()\n", "rewind_weights(winning_model, initial_weights, masks)\n", "winning_losses = train_model(winning_model, X_train, y_train, masks=masks, epochs=20)\n", "winning_acc = evaluate(winning_model, X_test, y_test)\n", "print(f'   Winning ticket accuracy: {winning_acc:.1f}%')\n", "\n", "# Count parameters\n", "total, nonzero = count_nonzero(winning_model)\n", "print(f'\\nðŸ“Š Parameters: {nonzero:,} / {total:,} ({100*nonzero/total:.1f}% remaining)')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize results\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Training curves\n", "axes[0].plot(dense_losses, label=f'Dense ({dense_acc:.1f}%)', linewidth=2, color='#3b82f6')\n", "axes[0].plot(random_losses, label=f'Random Sparse ({random_acc:.1f}%)', linewidth=2, color='#ef4444')\n", "axes[0].plot(winning_losses, label=f'Winning Ticket ({winning_acc:.1f}%)', linewidth=2, color='#22c55e')\n", "axes[0].set_xlabel('Epoch')\n", "axes[0].set_ylabel('Loss')\n", "axes[0].set_title('ðŸ“ˆ Training Curves')\n", "axes[0].legend()\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "# Accuracy comparison\n", "categories = ['Dense\\n(100%)', 'Random Sparse\\n(20%)', 'Winning Ticket\\n(20%)']\n", "accuracies = [dense_acc, random_acc, winning_acc]\n", "colors = ['#3b82f6', '#ef4444', '#22c55e']\n", "\n", "bars = axes[1].bar(categories, accuracies, color=colors)\n", "axes[1].set_ylabel('Accuracy (%)')\n", "axes[1].set_title('ðŸ“Š Accuracy Comparison')\n", "for bar, acc in zip(bars, accuracies):\n", "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n", "                 f'{acc:.1f}%', ha='center', fontsize=12)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸŽ¯ KEY RESULT: Winning ticket (20% weights) matches dense network!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Iterative Magnitude Pruning (IMP)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def iterative_magnitude_pruning(prune_rate=0.2, iterations=10):\n", "    \"\"\"\n", "    Iterative Magnitude Pruning:\n", "    1. Train network\n", "    2. Prune p% of smallest weights\n", "    3. Reset to initial weights (with mask)\n", "    4. Repeat\n", "    \"\"\"\n", "    model = LotteryNet()\n", "    initial_weights = save_initial_weights(model)\n", "    \n", "    # Track cumulative mask\n", "    cumulative_mask = {name: torch.ones_like(param) \n", "                       for name, param in model.named_parameters() \n", "                       if 'weight' in name}\n", "    \n", "    results = []\n", "    \n", "    for iteration in range(iterations):\n", "        # Reset to initial weights with current mask\n", "        rewind_weights(model, initial_weights, cumulative_mask)\n", "        \n", "        # Train\n", "        train_model(model, X_train, y_train, masks=cumulative_mask, epochs=15)\n", "        \n", "        # Evaluate\n", "        acc = evaluate(model, X_test, y_test)\n", "        total, nonzero = count_nonzero(model)\n", "        sparsity = 100 * (1 - nonzero/total)\n", "        \n", "        results.append({'iteration': iteration, 'accuracy': acc, 'sparsity': sparsity})\n", "        print(f'Iteration {iteration}: Sparsity={sparsity:.1f}%, Accuracy={acc:.1f}%')\n", "        \n", "        # Prune for next iteration\n", "        for name, param in model.named_parameters():\n", "            if name in cumulative_mask:\n", "                # Only consider non-zero weights for pruning\n", "                nonzero_weights = param.data * cumulative_mask[name]\n", "                if nonzero_weights.abs().sum() > 0:\n", "                    # Find threshold for this layer\n", "                    nonzero_abs = nonzero_weights.abs()[cumulative_mask[name] == 1]\n", "                    if len(nonzero_abs) > 0:\n", "                        threshold = torch.quantile(nonzero_abs.flatten(), prune_rate)\n", "                        # Update cumulative mask\n", "                        cumulative_mask[name] *= (nonzero_weights.abs() > threshold).float()\n", "    \n", "    return results\n", "\n", "print('ðŸ”„ ITERATIVE MAGNITUDE PRUNING')\n", "print('=' * 60)\n", "imp_results = iterative_magnitude_pruning(prune_rate=0.2, iterations=8)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize IMP results\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "sparsities = [r['sparsity'] for r in imp_results]\n", "accuracies = [r['accuracy'] for r in imp_results]\n", "\n", "ax.plot(sparsities, accuracies, 'o-', linewidth=2, markersize=10, color='#22c55e')\n", "ax.axhline(y=accuracies[0], color='#3b82f6', linestyle='--', alpha=0.7, label='Dense baseline')\n", "\n", "# Mark the \"knee\" point\n", "for i, (s, a) in enumerate(zip(sparsities, accuracies)):\n", "    ax.annotate(f'Iter {i}', (s, a), xytext=(5, 5), textcoords='offset points', fontsize=9)\n", "\n", "ax.set_xlabel('Sparsity (%)', fontsize=12)\n", "ax.set_ylabel('Accuracy (%)', fontsize=12)\n", "ax.set_title('ðŸ“ˆ Iterative Magnitude Pruning Results', fontsize=14)\n", "ax.grid(True, alpha=0.3)\n", "ax.legend()\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ IMP finds winning tickets at higher sparsity levels!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Late Rewinding (Practical Improvement)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def late_rewinding_experiment():\n", "    \"\"\"\n", "    Late Rewinding: Instead of rewinding to iteration 0,\n", "    rewind to iteration k (e.g., after 10% of training).\n", "    \n", "    This often works better for larger models!\n", "    \"\"\"\n", "    model = LotteryNet()\n", "    \n", "    # Save checkpoints at different iterations\n", "    checkpoints = {}\n", "    checkpoint_epochs = [0, 2, 5, 10]\n", "    \n", "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n", "    criterion = nn.CrossEntropyLoss()\n", "    \n", "    # Train and save checkpoints\n", "    for epoch in range(20):\n", "        if epoch in checkpoint_epochs:\n", "            checkpoints[epoch] = {name: p.data.clone() for name, p in model.named_parameters()}\n", "        \n", "        model.train()\n", "        optimizer.zero_grad()\n", "        loss = criterion(model(X_train), y_train)\n", "        loss.backward()\n", "        optimizer.step()\n", "    \n", "    # Create pruning mask from final trained model\n", "    masks = create_masks(model, sparsity=0.8)\n", "    \n", "    # Test different rewind points\n", "    print('ðŸ“Š LATE REWINDING EXPERIMENT')\n", "    print('=' * 50)\n", "    print(f'{\"Rewind to Epoch\":<20} {\"Accuracy\":<15}')\n", "    print('-' * 35)\n", "    \n", "    results = []\n", "    for rewind_epoch, weights in checkpoints.items():\n", "        test_model = LotteryNet()\n", "        rewind_weights(test_model, weights, masks)\n", "        train_model(test_model, X_train, y_train, masks=masks, epochs=20)\n", "        acc = evaluate(test_model, X_test, y_test)\n", "        results.append((rewind_epoch, acc))\n", "        print(f'{rewind_epoch:<20} {acc:.1f}%')\n", "    \n", "    return results\n", "\n", "late_rewind_results = late_rewinding_experiment()\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(8, 5))\n", "epochs = [r[0] for r in late_rewind_results]\n", "accs = [r[1] for r in late_rewind_results]\n", "\n", "ax.bar([str(e) for e in epochs], accs, color='#8b5cf6')\n", "ax.set_xlabel('Rewind Epoch')\n", "ax.set_ylabel('Final Accuracy (%)')\n", "ax.set_title('ðŸ“Š Late Rewinding: Different Rewind Points')\n", "\n", "for i, (e, a) in enumerate(zip(epochs, accs)):\n", "    ax.text(i, a + 0.5, f'{a:.1f}%', ha='center')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ Late rewinding often works better than iteration 0!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('ðŸŽ¯ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. Lottery Ticket Hypothesis: Sparse winning tickets exist at init')\n", "print('\\n2. Winning tickets: Original init + pruning mask from trained net')\n", "print('\\n3. Random sparse != Winning ticket (init matters!)')\n", "print('\\n4. Iterative Magnitude Pruning finds tickets at higher sparsity')\n", "print('\\n5. Late Rewinding: Rewind to early training, not init')\n", "print('\\n6. LTH inspired efficient training and sparse architecture search')\n", "print('\\n' + '=' * 60)\n", "print('\\nðŸ“š Next: Quantization for further compression!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
