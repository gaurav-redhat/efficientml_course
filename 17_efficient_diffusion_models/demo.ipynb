{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸŽ¨ Lecture 17: Efficient Diffusion Models - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/17_efficient_diffusion_models/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Diffusion model basics and why they're slow\n", "- Fast samplers (DDIM, DPM++)\n", "- Model distillation (LCM, SDXL Turbo)\n", "- Architecture optimizations"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "print('Ready for Efficient Diffusion!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: The Diffusion Bottleneck"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def diffusion_analysis():\n", "    \"\"\"\n", "    Analyze diffusion model inference characteristics.\n", "    \"\"\"\n", "    print('ðŸ“Š DIFFUSION MODEL INFERENCE')\n", "    print('=' * 60)\n", "    \n", "    print('\\nðŸ”¹ Why diffusion is slow:')\n", "    print('   1. Sequential denoising: T steps, each needs full forward pass')\n", "    print('   2. Large UNet: ~860M parameters (SD 1.5)')\n", "    print('   3. High resolution: 512Ã—512 or 1024Ã—1024')\n", "    \n", "    # Comparison\n", "    methods = {\n", "        'GAN (StyleGAN)': {'steps': 1, 'time_per_step': 50, 'total': 50},\n", "        'DDPM (original)': {'steps': 1000, 'time_per_step': 50, 'total': 50000},\n", "        'DDIM': {'steps': 50, 'time_per_step': 50, 'total': 2500},\n", "        'DPM++ 2M': {'steps': 20, 'time_per_step': 50, 'total': 1000},\n", "        'LCM': {'steps': 4, 'time_per_step': 50, 'total': 200},\n", "        'SDXL Turbo': {'steps': 1, 'time_per_step': 100, 'total': 100},\n", "    }\n", "    \n", "    print(f'\\n{\"Method\":<20} {\"Steps\":<10} {\"Time/Step\":<12} {\"Total (ms)\":<12}')\n", "    print('-' * 55)\n", "    for name, info in methods.items():\n", "        print(f'{name:<20} {info[\"steps\"]:<10} {info[\"time_per_step\"]:>8}ms {info[\"total\"]:>10}ms')\n", "    \n", "    return methods\n", "\n", "methods = diffusion_analysis()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize speedup\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Steps comparison\n", "names = list(methods.keys())\n", "steps = [methods[n]['steps'] for n in names]\n", "times = [methods[n]['total'] for n in names]\n", "\n", "colors = plt.cm.RdYlGn(np.linspace(0.9, 0.2, len(names)))\n", "\n", "axes[0].bar(names, steps, color=colors)\n", "axes[0].set_ylabel('Number of Steps')\n", "axes[0].set_title('Denoising Steps by Method')\n", "axes[0].set_yscale('log')\n", "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=20, ha='right')\n", "\n", "for i, (n, s) in enumerate(zip(names, steps)):\n", "    axes[0].text(i, s * 1.2, str(s), ha='center', fontsize=10)\n", "\n", "# Total time comparison\n", "axes[1].bar(names, times, color=colors)\n", "axes[1].set_ylabel('Total Time (ms)')\n", "axes[1].set_title('Generation Time by Method')\n", "axes[1].set_yscale('log')\n", "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=20, ha='right')\n", "\n", "for i, (n, t) in enumerate(zip(names, times)):\n", "    label = f'{t/1000:.1f}s' if t >= 1000 else f'{t}ms'\n", "    axes[1].text(i, t * 1.2, label, ha='center', fontsize=10)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print(f'\\nðŸ’¡ SDXL Turbo is {methods[\"DDPM (original)\"][\"total\"] / methods[\"SDXL Turbo\"][\"total\"]:.0f}x faster than DDPM!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Fast Samplers (DDIM, DPM++)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def explain_samplers():\n", "    \"\"\"\n", "    Explain different sampling strategies.\n", "    \"\"\"\n", "    print('ðŸ“Š SAMPLING STRATEGIES')\n", "    print('=' * 70)\n", "    \n", "    samplers = {\n", "        'DDPM': {\n", "            'formula': 'x_{t-1} = Î¼(x_t, t) + Ïƒ_t Ã— Îµ',\n", "            'key_idea': 'Stochastic, follows Markov chain',\n", "            'min_steps': 1000,\n", "        },\n", "        'DDIM': {\n", "            'formula': 'x_{t-1} = âˆš(Î±_{t-1}) Ã— pred_x0 + âˆš(1-Î±_{t-1}) Ã— pred_noise',\n", "            'key_idea': 'Deterministic, can skip steps',\n", "            'min_steps': 20,\n", "        },\n", "        'DPM++ 2M': {\n", "            'formula': 'Uses ODE solver with 2nd order multistep',\n", "            'key_idea': 'Higher order = fewer steps needed',\n", "            'min_steps': 15,\n", "        },\n", "        'Euler Ancestral': {\n", "            'formula': 'First-order Euler method + noise',\n", "            'key_idea': 'Simple, good for creative outputs',\n", "            'min_steps': 25,\n", "        },\n", "    }\n", "    \n", "    for name, info in samplers.items():\n", "        print(f'\\nðŸ”¹ {name}')\n", "        print(f'   Key idea: {info[\"key_idea\"]}')\n", "        print(f'   Minimum steps: ~{info[\"min_steps\"]}')\n", "\n", "explain_samplers()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simulate denoising process\n", "def simulate_denoising(n_steps, method='linear'):\n", "    \"\"\"\n", "    Simulate the denoising trajectory.\n", "    \"\"\"\n", "    t = np.linspace(1, 0, n_steps + 1)\n", "    \n", "    if method == 'linear':\n", "        noise_levels = t\n", "    elif method == 'cosine':\n", "        noise_levels = np.cos(t * np.pi / 2)\n", "    elif method == 'quadratic':\n", "        noise_levels = t ** 2\n", "    \n", "    return noise_levels\n", "\n", "# Visualize denoising schedules\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Different step counts\n", "for steps in [10, 20, 50, 100]:\n", "    trajectory = simulate_denoising(steps, 'cosine')\n", "    axes[0].plot(np.linspace(0, 1, len(trajectory)), trajectory, \n", "                 'o-', label=f'{steps} steps', markersize=3)\n", "\n", "axes[0].set_xlabel('Progress')\n", "axes[0].set_ylabel('Noise Level')\n", "axes[0].set_title('Denoising Trajectory at Different Step Counts')\n", "axes[0].legend()\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "# Different schedules\n", "schedules = ['linear', 'cosine', 'quadratic']\n", "colors = ['#3b82f6', '#22c55e', '#ef4444']\n", "\n", "for schedule, color in zip(schedules, colors):\n", "    trajectory = simulate_denoising(50, schedule)\n", "    axes[1].plot(np.linspace(0, 1, len(trajectory)), trajectory, \n", "                 '-', label=schedule, color=color, linewidth=2)\n", "\n", "axes[1].set_xlabel('Progress')\n", "axes[1].set_ylabel('Noise Level')\n", "axes[1].set_title('Different Noise Schedules (50 steps)')\n", "axes[1].legend()\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Model Distillation (LCM, SDXL Turbo)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def explain_distillation():\n", "    \"\"\"\n", "    Explain diffusion model distillation techniques.\n", "    \"\"\"\n", "    print('ðŸ“Š DIFFUSION MODEL DISTILLATION')\n", "    print('=' * 70)\n", "    \n", "    print('\\nðŸ”¹ Progressive Distillation:')\n", "    print('   Teacher: 1000 steps â†’ Student: 500 steps')\n", "    print('   Student learns to match 2 teacher steps in 1 step')\n", "    print('   Repeat: 500 â†’ 250 â†’ 125 â†’ ... â†’ 1 step')\n", "    \n", "    print('\\nðŸ”¹ Latent Consistency Models (LCM):')\n", "    print('   Train model to directly predict clean image')\n", "    print('   Uses consistency loss: f(x_t) â‰ˆ f(x_{t-k})')\n", "    print('   Result: 4 steps with good quality')\n", "    \n", "    print('\\nðŸ”¹ Adversarial Distillation (SDXL Turbo):')\n", "    print('   Add discriminator to guide single-step generation')\n", "    print('   Student learns to fool discriminator in 1 step')\n", "    print('   Result: 1 step with remarkable quality')\n", "    \n", "    # Quality comparison\n", "    print('\\nðŸ“Š QUALITY COMPARISON (FID on COCO)')\n", "    print('-' * 50)\n", "    \n", "    comparisons = [\n", "        ('SD 1.5 (50 steps)', 50, 8.5),\n", "        ('SD 1.5 (20 steps)', 20, 9.5),\n", "        ('LCM-LoRA (4 steps)', 4, 10.2),\n", "        ('SDXL Turbo (1 step)', 1, 11.5),\n", "    ]\n", "    \n", "    print(f'{\"Method\":<25} {\"Steps\":<10} {\"FID â†“\":<10}')\n", "    for name, steps, fid in comparisons:\n", "        print(f'{name:<25} {steps:<10} {fid:<10.1f}')\n", "\n", "explain_distillation()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize quality vs speed trade-off\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "methods_data = [\n", "    ('DDPM 1000', 1000, 8.0),\n", "    ('DDIM 50', 50, 8.5),\n", "    ('DPM++ 20', 20, 9.0),\n", "    ('LCM 8', 8, 9.5),\n", "    ('LCM 4', 4, 10.2),\n", "    ('Turbo 1', 1, 11.5),\n", "]\n", "\n", "steps = [d[1] for d in methods_data]\n", "fids = [d[2] for d in methods_data]\n", "names = [d[0] for d in methods_data]\n", "\n", "ax.scatter(steps, fids, s=200, c='#3b82f6', zorder=5)\n", "ax.plot(steps, fids, '--', color='gray', alpha=0.5)\n", "\n", "for name, s, f in methods_data:\n", "    ax.annotate(name, (s, f), xytext=(10, 5), textcoords='offset points', fontsize=10)\n", "\n", "ax.set_xlabel('Number of Steps (log scale)', fontsize=12)\n", "ax.set_ylabel('FID Score (lower is better)', fontsize=12)\n", "ax.set_title('ðŸ“Š Quality vs Speed Trade-off in Diffusion', fontsize=14)\n", "ax.set_xscale('log')\n", "ax.grid(True, alpha=0.3)\n", "ax.invert_xaxis()\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸ’¡ 1-step generation achieves ~90% of 1000-step quality!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Architecture Optimizations"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def architecture_optimizations():\n", "    \"\"\"\n", "    Discuss architecture-level optimizations.\n", "    \"\"\"\n", "    print('ðŸ“Š ARCHITECTURE OPTIMIZATIONS')\n", "    print('=' * 70)\n", "    \n", "    optimizations = {\n", "        'FlashAttention': {\n", "            'speedup': '2-4x',\n", "            'memory': '5-20x less',\n", "            'description': 'Fused attention kernel, no NÂ² memory'\n", "        },\n", "        'xFormers': {\n", "            'speedup': '2x',\n", "            'memory': '2x less',\n", "            'description': 'Memory-efficient attention implementations'\n", "        },\n", "        'VAE Tiling': {\n", "            'speedup': '1x',\n", "            'memory': '4x less',\n", "            'description': 'Process large images in tiles'\n", "        },\n", "        'FP16/BF16': {\n", "            'speedup': '2x',\n", "            'memory': '2x less',\n", "            'description': 'Half precision computation'\n", "        },\n", "        'Torch Compile': {\n", "            'speedup': '1.5-2x',\n", "            'memory': '1x',\n", "            'description': 'Graph optimization and fusion'\n", "        },\n", "    }\n", "    \n", "    print(f'{\"Optimization\":<20} {\"Speedup\":<12} {\"Memory\":<12} {\"Description\":<30}')\n", "    print('-' * 75)\n", "    \n", "    for name, info in optimizations.items():\n", "        print(f'{name:<20} {info[\"speedup\"]:<12} {info[\"memory\"]:<12} {info[\"description\"]:<30}')\n", "    \n", "    # Total impact\n", "    print('\\nðŸ“Š COMBINED IMPACT')\n", "    print('=' * 50)\n", "    print('Baseline (SD 1.5, FP32, naive): 15 seconds/image')\n", "    print('Optimized (all above):          1.5 seconds/image')\n", "    print('+ LCM (4 steps):                0.3 seconds/image')\n", "    print('\\nðŸ’¡ Total: 50x faster end-to-end!')\n", "\n", "architecture_optimizations()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('ðŸŽ¯ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. Diffusion is slow: 1000 sequential denoising steps')\n", "print('\\n2. Fast samplers: DDIM, DPM++ reduce to 20-50 steps')\n", "print('\\n3. Distillation: LCM/Turbo achieve 1-4 step generation')\n", "print('\\n4. Architecture: FlashAttention, xFormers save memory')\n", "print('\\n5. Combined: 50x faster than naive implementation')\n", "print('\\n6. Quality trade-off: 1-step â‰ˆ 90% of 1000-step quality')\n", "print('\\n' + '=' * 60)\n", "print('\\nðŸ“š Next: Quantum Machine Learning!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
