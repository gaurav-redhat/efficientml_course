{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸ“± Lecture 13: On-Device Training - Complete Demo\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/efficientml_course/blob/main/13_on_device_training/demo.ipynb)\n", "\n", "## What You'll Learn\n", "- Why train on edge devices\n", "- TinyTL: Lite training\n", "- Sparse updates and bias-only training\n", "- Memory-efficient backpropagation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "print('Ready for On-Device Training!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Why On-Device Training?\n", "\n", "- **Privacy**: Data never leaves device\n", "- **Personalization**: Adapt to user's data\n", "- **Latency**: No network round-trip\n", "- **Offline**: Works without connectivity"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Constraints comparison\n", "platforms = {\n", "    'Cloud GPU (A100)': {'memory_gb': 80, 'compute_tflops': 312},\n", "    'Laptop GPU (4060)': {'memory_gb': 8, 'compute_tflops': 15},\n", "    'Smartphone (A16)': {'memory_gb': 6, 'compute_tflops': 2},\n", "    'MCU (STM32H7)': {'memory_gb': 0.001, 'compute_tflops': 0.0001},\n", "}\n", "\n", "print('ðŸ“Š PLATFORM COMPARISON')\n", "print('=' * 60)\n", "print(f'{\"Platform\":<25} {\"Memory\":<15} {\"Compute (TFLOPS)\":<15}')\n", "print('-' * 60)\n", "for name, specs in platforms.items():\n", "    mem = f'{specs[\"memory_gb\"]:.3f} GB' if specs['memory_gb'] < 1 else f'{specs[\"memory_gb\"]:.0f} GB'\n", "    print(f'{name:<25} {mem:<15} {specs[\"compute_tflops\"]:<15.4f}')\n", "\n", "print('\\nâš ï¸ On-device training needs 1000x less memory!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Full Fine-tuning vs Lite Training"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MobileNetBlock(nn.Module):\n", "    \"\"\"Simplified MobileNet block.\"\"\"\n", "    def __init__(self, in_ch, out_ch, stride=1):\n", "        super().__init__()\n", "        self.conv1 = nn.Conv2d(in_ch, in_ch, 3, stride, 1, groups=in_ch, bias=False)\n", "        self.bn1 = nn.BatchNorm2d(in_ch)\n", "        self.conv2 = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n", "        self.bn2 = nn.BatchNorm2d(out_ch)\n", "    \n", "    def forward(self, x):\n", "        x = F.relu(self.bn1(self.conv1(x)))\n", "        x = F.relu(self.bn2(self.conv2(x)))\n", "        return x\n", "\n", "class TinyTLModel(nn.Module):\n", "    \"\"\"\n", "    Model supporting different training modes:\n", "    1. Full: Train all parameters\n", "    2. Last layer: Only train classifier\n", "    3. Bias-only (TinyTL): Only train biases\n", "    4. Lite residual: Train small adapter modules\n", "    \"\"\"\n", "    def __init__(self, num_classes=10):\n", "        super().__init__()\n", "        \n", "        self.features = nn.Sequential(\n", "            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n", "            nn.BatchNorm2d(32),\n", "            nn.ReLU(),\n", "            MobileNetBlock(32, 64, stride=2),\n", "            MobileNetBlock(64, 128, stride=2),\n", "            MobileNetBlock(128, 256, stride=2),\n", "        )\n", "        \n", "        # Lite residual adapters (small, trainable)\n", "        self.adapters = nn.ModuleList([\n", "            nn.Conv2d(64, 64, 1, bias=True),\n", "            nn.Conv2d(128, 128, 1, bias=True),\n", "            nn.Conv2d(256, 256, 1, bias=True),\n", "        ])\n", "        \n", "        self.gap = nn.AdaptiveAvgPool2d(1)\n", "        self.classifier = nn.Linear(256, num_classes)\n", "    \n", "    def forward(self, x):\n", "        x = self.features(x)\n", "        x = self.gap(x).flatten(1)\n", "        return self.classifier(x)\n", "    \n", "    def set_training_mode(self, mode='full'):\n", "        \"\"\"\n", "        Set which parameters to train.\n", "        \n", "        Modes:\n", "        - full: All parameters\n", "        - last_layer: Only classifier\n", "        - bias_only: Only bias terms\n", "        - lite_residual: Only adapters + classifier\n", "        \"\"\"\n", "        # First freeze everything\n", "        for param in self.parameters():\n", "            param.requires_grad = False\n", "        \n", "        if mode == 'full':\n", "            for param in self.parameters():\n", "                param.requires_grad = True\n", "        \n", "        elif mode == 'last_layer':\n", "            for param in self.classifier.parameters():\n", "                param.requires_grad = True\n", "        \n", "        elif mode == 'bias_only':\n", "            for name, param in self.named_parameters():\n", "                if 'bias' in name:\n", "                    param.requires_grad = True\n", "            # Also train classifier\n", "            for param in self.classifier.parameters():\n", "                param.requires_grad = True\n", "        \n", "        elif mode == 'lite_residual':\n", "            for param in self.adapters.parameters():\n", "                param.requires_grad = True\n", "            for param in self.classifier.parameters():\n", "                param.requires_grad = True\n", "        \n", "        return self.count_trainable_params()\n", "    \n", "    def count_trainable_params(self):\n", "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n", "\n", "# Create model and compare modes\n", "model = TinyTLModel()\n", "total_params = sum(p.numel() for p in model.parameters())\n", "\n", "print('ðŸ“Š TRAINING MODE COMPARISON')\n", "print('=' * 60)\n", "print(f'{\"Mode\":<20} {\"Trainable Params\":<20} {\"% of Total\":<15}')\n", "print('-' * 60)\n", "\n", "modes = ['full', 'last_layer', 'bias_only', 'lite_residual']\n", "results = []\n", "\n", "for mode in modes:\n", "    trainable = model.set_training_mode(mode)\n", "    pct = trainable / total_params * 100\n", "    results.append((mode, trainable, pct))\n", "    print(f'{mode:<20} {trainable:>15,}   {pct:>12.2f}%')\n", "\n", "print(f'\\nTotal parameters: {total_params:,}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Trainable parameters\n", "modes = [r[0] for r in results]\n", "trainable = [r[1] for r in results]\n", "pcts = [r[2] for r in results]\n", "\n", "colors = ['#ef4444', '#f59e0b', '#22c55e', '#3b82f6']\n", "axes[0].bar(modes, trainable, color=colors)\n", "axes[0].set_ylabel('Trainable Parameters')\n", "axes[0].set_title('Trainable Parameters by Mode')\n", "axes[0].set_yscale('log')\n", "\n", "for i, (t, p) in enumerate(zip(trainable, pcts)):\n", "    axes[0].text(i, t * 1.5, f'{p:.1f}%', ha='center')\n", "\n", "# Memory savings (simplified estimate)\n", "# Training memory â‰ˆ params Ã— 4 (gradients) + activations\n", "base_mem = 100  # Arbitrary baseline\n", "mem_savings = [base_mem, base_mem * 0.3, base_mem * 0.1, base_mem * 0.15]\n", "\n", "axes[1].bar(modes, mem_savings, color=colors)\n", "axes[1].set_ylabel('Training Memory (relative)')\n", "axes[1].set_title('Training Memory by Mode')\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Memory-Efficient Backpropagation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def backprop_memory_analysis():\n", "    \"\"\"\n", "    Analyze memory requirements for different training strategies.\n", "    \"\"\"\n", "    print('ðŸ“Š BACKPROP MEMORY ANALYSIS')\n", "    print('=' * 60)\n", "    print('\\nFull backprop needs to store:')\n", "    print('  1. All intermediate activations')\n", "    print('  2. All gradients')\n", "    print('  3. Optimizer states')\n", "    \n", "    print('\\nBias-only training:')\n", "    print('  1. Only bias-related activations (much smaller)')\n", "    print('  2. Only bias gradients')\n", "    print('  3. Minimal optimizer states')\n", "    \n", "    # Simplified memory calculation\n", "    # Assume: 10-layer network, each layer has 1M params, 10KB bias\n", "    n_layers = 10\n", "    params_per_layer = 1_000_000\n", "    bias_per_layer = 10_000\n", "    \n", "    # Full training\n", "    full_grad_mem = n_layers * params_per_layer * 4  # FP32 gradients\n", "    full_act_mem = n_layers * params_per_layer * 4   # Activations\n", "    \n", "    # Bias-only\n", "    bias_grad_mem = n_layers * bias_per_layer * 4\n", "    bias_act_mem = n_layers * bias_per_layer * 4\n", "    \n", "    print(f'\\nðŸ“Š MEMORY COMPARISON')\n", "    print(f'{\"Component\":<20} {\"Full (MB)\":<15} {\"Bias-only (MB)\":<15} {\"Savings\":<10}')\n", "    print('-' * 60)\n", "    print(f'{\"Gradients\":<20} {full_grad_mem/1e6:<15.1f} {bias_grad_mem/1e6:<15.3f} {full_grad_mem/bias_grad_mem:<10.0f}x')\n", "    print(f'{\"Activations\":<20} {full_act_mem/1e6:<15.1f} {bias_act_mem/1e6:<15.3f} {full_act_mem/bias_act_mem:<10.0f}x')\n", "\n", "backprop_memory_analysis()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Training Comparison Experiment"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simulate training with different modes\n", "def train_model(model, mode, X_train, y_train, X_test, y_test, epochs=20):\n", "    model.set_training_mode(mode)\n", "    \n", "    optimizer = torch.optim.Adam(\n", "        filter(lambda p: p.requires_grad, model.parameters()),\n", "        lr=0.001\n", "    )\n", "    criterion = nn.CrossEntropyLoss()\n", "    \n", "    history = {'train_loss': [], 'test_acc': []}\n", "    \n", "    for epoch in range(epochs):\n", "        # Train\n", "        model.train()\n", "        optimizer.zero_grad()\n", "        out = model(X_train)\n", "        loss = criterion(out, y_train)\n", "        loss.backward()\n", "        optimizer.step()\n", "        \n", "        # Evaluate\n", "        model.eval()\n", "        with torch.no_grad():\n", "            test_out = model(X_test)\n", "            acc = (test_out.argmax(1) == y_test).float().mean().item() * 100\n", "        \n", "        history['train_loss'].append(loss.item())\n", "        history['test_acc'].append(acc)\n", "    \n", "    return history\n", "\n", "# Create data\n", "X_train = torch.randn(200, 3, 64, 64)\n", "y_train = torch.randint(0, 10, (200,))\n", "X_test = torch.randn(50, 3, 64, 64)\n", "y_test = torch.randint(0, 10, (50,))\n", "\n", "print('ðŸ“š TRAINING EXPERIMENT')\n", "print('=' * 50)\n", "\n", "all_histories = {}\n", "\n", "for mode in ['full', 'last_layer', 'bias_only', 'lite_residual']:\n", "    print(f'\\nTraining with {mode} mode...')\n", "    model = TinyTLModel()\n", "    history = train_model(model, mode, X_train, y_train, X_test, y_test, epochs=30)\n", "    all_histories[mode] = history\n", "    print(f'  Final accuracy: {history[\"test_acc\"][-1]:.1f}%')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize training results\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "colors = {'full': '#ef4444', 'last_layer': '#f59e0b', 'bias_only': '#22c55e', 'lite_residual': '#3b82f6'}\n", "\n", "for mode, history in all_histories.items():\n", "    axes[0].plot(history['train_loss'], label=mode, color=colors[mode], linewidth=2)\n", "    axes[1].plot(history['test_acc'], label=mode, color=colors[mode], linewidth=2)\n", "\n", "axes[0].set_xlabel('Epoch')\n", "axes[0].set_ylabel('Training Loss')\n", "axes[0].set_title('Training Loss by Mode')\n", "axes[0].legend()\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "axes[1].set_xlabel('Epoch')\n", "axes[1].set_ylabel('Test Accuracy (%)')\n", "axes[1].set_title('Test Accuracy by Mode')\n", "axes[1].legend()\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Summary\n", "print('\\nðŸ“Š FINAL SUMMARY')\n", "print('=' * 60)\n", "print(f'{\"Mode\":<15} {\"Trainable %\":<15} {\"Final Acc\":<15} {\"Memory\":<15}')\n", "print('-' * 60)\n", "\n", "mem_estimate = {'full': '100%', 'last_layer': '30%', 'bias_only': '10%', 'lite_residual': '15%'}\n", "\n", "for mode, history in all_histories.items():\n", "    trainable_pct = [r[2] for r in results if r[0] == mode][0]\n", "    print(f'{mode:<15} {trainable_pct:>12.1f}%  {history[\"test_acc\"][-1]:>12.1f}%  {mem_estimate[mode]:>12}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('ðŸŽ¯ KEY TAKEAWAYS')\n", "print('=' * 60)\n", "print('\\n1. On-device training enables privacy and personalization')\n", "print('\\n2. Full fine-tuning is too expensive for edge devices')\n", "print('\\n3. Bias-only (TinyTL): 100x less memory, good accuracy')\n", "print('\\n4. Lite residual: Small adapters for better adaptation')\n", "print('\\n5. Last-layer only: Simplest but limited')\n", "print('\\n6. Choose based on memory budget and accuracy needs')\n", "print('\\n' + '=' * 60)\n", "print('\\nðŸ“š Next: Distributed Training!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
